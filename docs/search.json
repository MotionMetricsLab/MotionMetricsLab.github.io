[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Motion Metrics",
    "section": "",
    "text": "Home\n    Resources"
  },
  {
    "objectID": "resources.html#datasets",
    "href": "resources.html#datasets",
    "title": "Motion Metrics",
    "section": "Datasets",
    "text": "Datasets\n\nGFT: Group Formation Task Spontaneous Facial Expression Database (2017) A 2D video database of spontaneous behavior during unscripted social interactions Complete with frame-level FACS annotations of facial behavior (n = 96) Primary Article/Citation: https://jmgirard.com/pubs/girard2017d/ Website/Request Access: http://osf.io/7wcyz\nMMSE/BP4D+: Multimodal Spontaneous Emotion Database (2016) An extension of BP4D that includes thermal and physiological data Complete with frame-level FACS annotations of facial behavior (n = 140) Primary Article/Citation: https://jmgirard.com/pubs/zhang2016/ Website/Request Access: http://bit.ly/2yg39Cn\nBP4D: Binghamton-Pittsburgh 4D Spontaneous Emotion Database (2014) A 3D video database of spontaneous behavior during emotion elicitation Complete with frame-level FACS annotations of facial behavior (n = 41) Primary Article/Citation: https://jmgirard.com/pubs/zhang2014/ Website/Request Access: http://bit.ly/2yg39Cn"
  },
  {
    "objectID": "resources.html#software",
    "href": "resources.html#software",
    "title": "Motion Metrics",
    "section": "Software",
    "text": "Software\n\nMaturing\n\nlordicon (2022) Extension for adding lordicon.com icons to Quarto HTML documents Source Code: https://github.com/jmgirard/lordicon\ncircumplex: Analysis and Visualization of Circumplex Data (2017–2021) R functions and educational materials about circumplex data analysis/visualization Documentation: https://circumplex.jmgirard.com Source Code: http://github.com/jmgirard/circumplex\nmReliability: Inter-rater Reliability in MATLAB (2015–2018) MATLAB functions and educational materials about inter-rater reliability Documentation: https://mreliability.jmgirard.com Source Code: http://github.com/jmgirard/mreliability\nDARMA: Dual Axis Rating and Media Annotation (2014–2018) Software for collecting and reviewing 2D continuous ratings of media files Documentation: https://darma.jmgirard.com Source Code: http://github.com/jmgirard/darma\nCARMA: Continuous Affect Rating and Media Annotation (2014–2018) Software for collecting and reviewing 1D continuous ratings of media files Documentation: https://carma.jmgirard.com Source Code: http://github.com/jmgirard/carma\n\n\n\nDeveloping\n\nvarde (2022) R functions for decomposing variance in multilevel models Source Code: https://github.com/jmgirard/varde\nwcc (2022) R functions for windowed cross-correlation (synchrony) analyses Source Code: https://github.com/jmgirard/wcc\nprofiles (2022) R functions and RStudio add-ins for managing RStudio profiles Source Code: https://github.com/jmgirard/profiles\ntidymedia (2021–2022) R functions for working with audio and video files in a tidy format Source Code: https://github.com/jmgirard/tidymedia\nagreement (2018–2022) R functions for inter-rater reliability Source Code: https://github.com/jmgirard/agreement"
  },
  {
    "objectID": "resources.html#trainings",
    "href": "resources.html#trainings",
    "title": "Motion Metrics",
    "section": "Trainings",
    "text": "Trainings\n\nApplied Machine Learning in R (2021, 2022) Five-day workshop offered as part of the Pittsburgh Summer Methodology Series https://pittmethods.github.io/appliedml\nIntroduction to R for Social Scientists (2020, 2022) Three-day workshop offered as part of the Pittsburgh Summer Methodology Series https://pittmethods.github.io/r4ss"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Motion Metrics",
    "section": "",
    "text": "Home\n    Publications\n  \n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n         \n          Publication\n        \n         \n          Year\n        \n     \n  \n    \n      \n      \n    \n\n\n  \n    Long-term unsupervised mobility assessment in movement disorders\n    Elke Warmerdam and Jeffrey M Hausdorff and Arash Atrsaei and Yuhan Zhou and Anat Mirelman and Kamiar Aminian and Alberto J Espay and Clint Hansen and Luc J W Evers and Andreas Keller and Claudine Lamoth and Andrea Pilotto and Lynn Rochester and Gerhard Schmidt and Bastiaan R Bloem and Walter Maetzler\n    The Lancet Neurology\n    (2022)\n    \n      Details\n    \n    \n      DOI\n    \n  \n  \n    Validation of IMU-based gait event detection during curved walking and turning in older adults and Parkinson’s Disease patients\n    Robbin Romijnders, Elke Warmerdam, Clint Hansen, Julius Welzel, Gerhard Schmidt, Walter Maetzler\n     Journal of NeuroEngineering and Rehabilitation\n    (2021)\n    \n      Details\n    \n    \n      DOI\n    \n  \n  \n    How Mobile Health Technology and Electronic Health Records Will Change Care of Patients with Parkinson’s Disease\n    Clint Hansen, Alvaro Sanchez-Ferro, Walter Maetzler\n    Journal of Parkinson's disease\n    (2018)\n    \n      Details\n    \n    \n      DOI\n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "publications/proceedings/wolfert2021.html",
    "href": "publications/proceedings/wolfert2021.html",
    "title": "To rate or not to rate: Investigating evaluation methods for generated co-speech gestures",
    "section": "",
    "text": "Wolfert, P., Girard, J. M., Kucherenko, T., & Belpaeme, T. (2021). To Rate or Not To Rate: Investigating Evaluation Methods for Generated Co-Speech Gestures. In Proceedings of the 23rd International Conference on Multimodal Interaction (pp. 494–502). Association for Computing Machinery."
  },
  {
    "objectID": "publications/proceedings/wolfert2021.html#citation-apa-7",
    "href": "publications/proceedings/wolfert2021.html#citation-apa-7",
    "title": "To rate or not to rate: Investigating evaluation methods for generated co-speech gestures",
    "section": "",
    "text": "Wolfert, P., Girard, J. M., Kucherenko, T., & Belpaeme, T. (2021). To Rate or Not To Rate: Investigating Evaluation Methods for Generated Co-Speech Gestures. In Proceedings of the 23rd International Conference on Multimodal Interaction (pp. 494–502). Association for Computing Machinery."
  },
  {
    "objectID": "publications/proceedings/wolfert2021.html#abstract",
    "href": "publications/proceedings/wolfert2021.html#abstract",
    "title": "To rate or not to rate: Investigating evaluation methods for generated co-speech gestures",
    "section": "Abstract",
    "text": "Abstract\nWhile automatic performance metrics are crucial for machine learning of artificial human-like behaviour, the gold standard for evaluation remains human judgement. The subjective evaluation of artificial human-like behaviour in embodied conversational agents is however expensive and little is known about the quality of the data it returns. Two approaches to subjective evaluation can be largely distinguished, one relying on ratings, the other on pairwise comparisons. In this study we use co-speech gestures to compare the two against each other and answer questions about their appropriateness for evaluation of artificial behaviour. We consider their ability to rate quality, but also aspects pertaining to the effort of use and the time required to collect subjective data.We use crowd sourcing to rate the quality of co-speech gestures in avatars, assessing which method picks up more detail in subjective assessments. We compared gestures generated by three different machine learning models with various level of behavioural quality. We found that both approaches were able to rank the videos according to quality and that the ranking significantly correlated, showing that in terms of quality there is no preference of one method over the other. We also found that pairwise comparisons were slightly faster and came with improved inter-rater reliability, suggesting that for small-scale studies pairwise comparisons are to be favoured over ratings."
  },
  {
    "objectID": "publications/proceedings/valstar2015.html",
    "href": "publications/proceedings/valstar2015.html",
    "title": "FERA 2015 - Second Facial Expression Recognition and Analysis challenge",
    "section": "",
    "text": "Valstar, M. F., Almaev, T., Girard, J. M., Mckeown, G., Mehu, M., Yin, L., Pantic, M., & Cohn, J. F. (2015). FERA 2015—Second Facial Expression Recognition and Analysis Challenge. Proceedings of the 11th IEEE International Conference on Automatic Face & Gesture Recognition (FG), 1–8."
  },
  {
    "objectID": "publications/proceedings/valstar2015.html#citation-apa-7",
    "href": "publications/proceedings/valstar2015.html#citation-apa-7",
    "title": "FERA 2015 - Second Facial Expression Recognition and Analysis challenge",
    "section": "",
    "text": "Valstar, M. F., Almaev, T., Girard, J. M., Mckeown, G., Mehu, M., Yin, L., Pantic, M., & Cohn, J. F. (2015). FERA 2015—Second Facial Expression Recognition and Analysis Challenge. Proceedings of the 11th IEEE International Conference on Automatic Face & Gesture Recognition (FG), 1–8."
  },
  {
    "objectID": "publications/proceedings/valstar2015.html#abstract",
    "href": "publications/proceedings/valstar2015.html#abstract",
    "title": "FERA 2015 - Second Facial Expression Recognition and Analysis challenge",
    "section": "Abstract",
    "text": "Abstract\nDespite efforts towards evaluation standards in facial expression analysis (e.g. FERA 2011), there is a need for up-to-date standardised evaluation procedures, focusing in particular on current challenges in the field. One of the challenges that is actively being addressed is the automatic estimation of expression intensities. To continue to provide a standardisation platform and to help the field progress beyond its current limitations, the FG 2015 Facial Expression Recognition and Analysis challenge (FERA 2015) will challenge participants to estimate FACS Action Unit (AU) intensity as well as AU occurrence on a common benchmark dataset with reliable manual annotations. Evaluation will be done using a clear and well-defined protocol. In this paper we present the second such challenge in automatic recognition of facial expressions, to be held in conjunction with the 11 IEEE conference on Face and Gesture Recognition, May 2015, in Ljubljana, Slovenia. Three sub-challenges are defined: the detection of AU occurrence, the estimation of AU intensity for pre-segmented data, and fully automatic AU intensity estimation. In this work we outline the evaluation protocol, the data used, and the results of a baseline method for the three sub-challenges."
  },
  {
    "objectID": "publications/proceedings/mcduff2019.html",
    "href": "publications/proceedings/mcduff2019.html",
    "title": "Democratizing psychological insights from analysis of nonverbal behavior",
    "section": "",
    "text": "McDuff, D., & Girard, J. M. (2019). Democratizing psychological insights from analysis of nonverbal behavior. Proceedings of the 8th International Conference on Affective Computing and Intelligent Interaction (ACII), 220–226."
  },
  {
    "objectID": "publications/proceedings/mcduff2019.html#citation-apa-7",
    "href": "publications/proceedings/mcduff2019.html#citation-apa-7",
    "title": "Democratizing psychological insights from analysis of nonverbal behavior",
    "section": "",
    "text": "McDuff, D., & Girard, J. M. (2019). Democratizing psychological insights from analysis of nonverbal behavior. Proceedings of the 8th International Conference on Affective Computing and Intelligent Interaction (ACII), 220–226."
  },
  {
    "objectID": "publications/proceedings/mcduff2019.html#abstract",
    "href": "publications/proceedings/mcduff2019.html#abstract",
    "title": "Democratizing psychological insights from analysis of nonverbal behavior",
    "section": "Abstract",
    "text": "Abstract\nThe affective computing community has invested heavily in building automated tools for the analysis of facial behavior and the expression of emotion. These tools present a valuable, but largely untapped, opportunity for social scientists to perform observational analyses of nonverbal behavior at very large scale. Various tech companies are collecting huge corpora of images and videos from around the world that could be used to study important scientific questions. However, privacy restrictions and intellectual property concerns render these data inaccessible to most academics. Unfortunately, this limits the potential for scientific advancement and leads to the consolidation of data and opportunity into the hands of a few powerful institutions. In this paper, we ask whether similar psychological insights can be gained by analyzing smaller, public datasets that are more within reach for academic researchers. As a proof-of-concept for this idea, we gather, analyze, and release a corpus of public images and metadata and use it to replicate recent psychological findings about smiling, gender, and culture. In so doing, we provide evidence that psychological insights can indeed by democratized through the automated analysis of nonverbal behavior."
  },
  {
    "objectID": "publications/proceedings/lin2020a.html",
    "href": "publications/proceedings/lin2020a.html",
    "title": "Context-Dependent Models for Predicting and Characterizing Facial Expressiveness",
    "section": "",
    "text": "Lin, V., Girard, J. M., & Morency, L.-P. (2020). Context-dependent models for predicting and characterizing facial expressiveness. Proceedings of the 3rd Workshop on Affective Content Analysis Co-Located with the 34th AAAI Conference on Artificial Intelligence, 2614, 11–28."
  },
  {
    "objectID": "publications/proceedings/lin2020a.html#citation-apa-7",
    "href": "publications/proceedings/lin2020a.html#citation-apa-7",
    "title": "Context-Dependent Models for Predicting and Characterizing Facial Expressiveness",
    "section": "",
    "text": "Lin, V., Girard, J. M., & Morency, L.-P. (2020). Context-dependent models for predicting and characterizing facial expressiveness. Proceedings of the 3rd Workshop on Affective Content Analysis Co-Located with the 34th AAAI Conference on Artificial Intelligence, 2614, 11–28."
  },
  {
    "objectID": "publications/proceedings/lin2020a.html#abstract",
    "href": "publications/proceedings/lin2020a.html#abstract",
    "title": "Context-Dependent Models for Predicting and Characterizing Facial Expressiveness",
    "section": "Abstract",
    "text": "Abstract\nIn recent years, extensive research has emerged in affective computing on topics like automatic emotion recognition and determining the signals that characterize individual emotions. Much less studied, however, is expressiveness—the extent to which someone shows any feeling or emotion. Expressiveness is related to personality and mental health and plays a crucial role in social interaction. As such, the ability to automatically detect or predict expressiveness can facilitate significant advancements in areas ranging from psychiatric care to artificial social intelligence. Motivated by these potential applications, we present an extension of the BP4D+ dataset with human ratings of expressiveness and develop methods for (1) automatically predicting expressiveness from visual data and (2) defining relationships between interpretable visual signals and expressiveness. In addition, we study the emotional context in which expressiveness occurs and hypothesize that different sets of signals are indicative of expressiveness in different con-texts (e.g., in response to surprise or in response to pain). Analysis of our statistical models confirms our hypothesis. Consequently, by looking at expressiveness separately in distinct emotional contexts, our predictive models show significant improvements over baselines and achieve com-parable results to human performance in terms of correlation with the ground truth."
  },
  {
    "objectID": "publications/proceedings/lin2020a.html#awards",
    "href": "publications/proceedings/lin2020a.html#awards",
    "title": "Context-Dependent Models for Predicting and Characterizing Facial Expressiveness",
    "section": "Awards",
    "text": "Awards\nThis paper won the Best Paper Award at the Workshop on Affective Content Analysis."
  },
  {
    "objectID": "publications/proceedings/girard2023a.html",
    "href": "publications/proceedings/girard2023a.html",
    "title": "DynAMoS: The Dynamic Affective Movie Clip Database for Subjectivity Analysis",
    "section": "",
    "text": "Girard, J. M., Tie, Y., & Liebenthal, E. (2023). DynAMoS: The Dynamic Affective Movie Clip Database for Subjectivity Analysis. Proceedings of the 11th International Conference on Affective Computing and Intelligent Interaction (ACII)."
  },
  {
    "objectID": "publications/proceedings/girard2023a.html#citation-apa-7",
    "href": "publications/proceedings/girard2023a.html#citation-apa-7",
    "title": "DynAMoS: The Dynamic Affective Movie Clip Database for Subjectivity Analysis",
    "section": "",
    "text": "Girard, J. M., Tie, Y., & Liebenthal, E. (2023). DynAMoS: The Dynamic Affective Movie Clip Database for Subjectivity Analysis. Proceedings of the 11th International Conference on Affective Computing and Intelligent Interaction (ACII)."
  },
  {
    "objectID": "publications/proceedings/girard2023a.html#abstract",
    "href": "publications/proceedings/girard2023a.html#abstract",
    "title": "DynAMoS: The Dynamic Affective Movie Clip Database for Subjectivity Analysis",
    "section": "Abstract",
    "text": "Abstract\nIn this paper, we describe the design, collection, and validation of a new video database that includes holistic and dynamic emotion ratings from 83 participants watching 22 affective movie clips. In contrast to previous work in Affective Computing, which pursued a single “ground truth” label for the affective content of each moment of each video (e.g., by averaging the ratings of 2 to 7 trained participants), we embrace the subjectivity inherent to emotional experiences and provide the full distribution of all participants’ ratings (with an average of 76.7 raters per video). We argue that this choice represents a paradigm shift with the potential to unlock new research directions, generate new hypotheses, and inspire novel methods in the Affective Computing community. We also describe several interdisciplinary use cases for the database: to provide dynamic norms for emotion elicitation studies (e.g., in psychology, medicine, and neuroscience), to train and test affective content analysis algorithms (e.g., for dynamic emotion recognition, video summarization, and movie recommendation), and to study subjectivity in emotional reactions (e.g., to identify moments of emotional ambiguity or ambivalence within movies, identify predictors of subjectivity, and develop personalized affective content analysis algorithms). The database is made freely available to researchers for noncommercial use at https://dynamos.mgb.org."
  },
  {
    "objectID": "publications/proceedings/girard2017c.html",
    "href": "publications/proceedings/girard2017c.html",
    "title": "Historical heterogeneity predicts smiling: Evidence from large-scale observational analyses",
    "section": "",
    "text": "Girard, J. M., & McDuff, D. (2017). Historical heterogeneity predicts smiling: Evidence from large-scale observational analyses. Proceedings of the 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG), 719–726."
  },
  {
    "objectID": "publications/proceedings/girard2017c.html#citation-apa-7",
    "href": "publications/proceedings/girard2017c.html#citation-apa-7",
    "title": "Historical heterogeneity predicts smiling: Evidence from large-scale observational analyses",
    "section": "",
    "text": "Girard, J. M., & McDuff, D. (2017). Historical heterogeneity predicts smiling: Evidence from large-scale observational analyses. Proceedings of the 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG), 719–726."
  },
  {
    "objectID": "publications/proceedings/girard2017c.html#abstract",
    "href": "publications/proceedings/girard2017c.html#abstract",
    "title": "Historical heterogeneity predicts smiling: Evidence from large-scale observational analyses",
    "section": "Abstract",
    "text": "Abstract\nFacial behavior is a valuable source of information about an individual’s feelings and intentions. However, many factors combine to influence and moderate facial behavior including personality, gender, context, and culture. Due to the high cost of traditional observational methods, the relationship between culture and facial behavior is not well-understood. In the current study, we explored the sociocultural factors that influence facial behavior using large-scale observational analyses. We developed and implemented an algorithm to automatically analyze the smiling of 866,726 participants across 31 different countries. We found that participants smiled more when from a country that is higher in individualism, has a lower population density, and has a long history of immigration diversity (i.e., historical heterogeneity). Our findings provide the first evidence that historical heterogeneity predicts actual smiling behavior. Furthermore, they converge with previous findings using selfreport methods. Taken together, these findings support the theory that historical heterogeneity explains, and may even contribute to the development of, permissive cultural display rules that encourage the open expression of emotion."
  },
  {
    "objectID": "publications/proceedings/girard2015d.html",
    "href": "publications/proceedings/girard2015d.html",
    "title": "How much training data for facial action unit detection?",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Jeni, L. A., Lucey, S., & De la Torre, F. (2015). How much training data for facial action unit detection? Proceedings of the 11th IEEE International Conference on Automatic Face & Gesture Recognition (FG), 1–8."
  },
  {
    "objectID": "publications/proceedings/girard2015d.html#citation-apa-7",
    "href": "publications/proceedings/girard2015d.html#citation-apa-7",
    "title": "How much training data for facial action unit detection?",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Jeni, L. A., Lucey, S., & De la Torre, F. (2015). How much training data for facial action unit detection? Proceedings of the 11th IEEE International Conference on Automatic Face & Gesture Recognition (FG), 1–8."
  },
  {
    "objectID": "publications/proceedings/girard2015d.html#abstract",
    "href": "publications/proceedings/girard2015d.html#abstract",
    "title": "How much training data for facial action unit detection?",
    "section": "Abstract",
    "text": "Abstract\nBy systematically varying the number of subjects and the number of frames per subject, we explored the influence of training set size on appearance and shape-based approaches to facial action unit (AU) detection. Digital video and expert coding of spontaneous facial activity from 80 subjects (over 350,000 frames) were used to train and test support vector machine classifiers. Appearance features were shape-normalized SIFT descriptors and shape features were 66 facial landmarks. Ten-fold cross-validation was used in all evaluations. Number of subjects and number of frames per subject differentially affected appearance and shape-based classifiers. For appearance features, which are high-dimensional, increasing the number of training subjects from 8 to 64 incrementally improved performance, regardless of the number of frames taken from each subject (ranging from 450 through 3600). In contrast, for shape features, increases in the number of training subjects and frames were associated with mixed results. In summary, maximal performance was attained using appearance features from large numbers of subjects with as few as 450 frames per subject. These findings suggest that variation in the number of subjects rather than number of frames per subject yields most efficient performance."
  },
  {
    "objectID": "publications/proceedings/girard2011.html",
    "href": "publications/proceedings/girard2011.html",
    "title": "Criteria and metrics for thresholded AU detection",
    "section": "",
    "text": "Jeni, L. A., Girard, J. M., Cohn, J. F., & De la Torre, F. (2013). Continuous AU intensity estimation using localized, sparse facial feature space. Proceedings of the 10th IEEE International Conference on Automated Face & Gesture Recognition (FG), 1–7."
  },
  {
    "objectID": "publications/proceedings/girard2011.html#citation-apa-7",
    "href": "publications/proceedings/girard2011.html#citation-apa-7",
    "title": "Criteria and metrics for thresholded AU detection",
    "section": "",
    "text": "Jeni, L. A., Girard, J. M., Cohn, J. F., & De la Torre, F. (2013). Continuous AU intensity estimation using localized, sparse facial feature space. Proceedings of the 10th IEEE International Conference on Automated Face & Gesture Recognition (FG), 1–7."
  },
  {
    "objectID": "publications/proceedings/girard2011.html#abstract",
    "href": "publications/proceedings/girard2011.html#abstract",
    "title": "Criteria and metrics for thresholded AU detection",
    "section": "Abstract",
    "text": "Abstract\nImplementing a computerized facial expression analysis system for automatic coding requires that a threshold for the system’s classifier outputs be selected. However, there are many potential ways to select a threshold. How do different criteria and metrics compare? Manually FACS coded video of 45 clinical interviews (Spectrum dataset) were processed using person-specific active appearance models (AAM). Support vector machine (SVM) classifiers were trained using an independent dataset (RU-FACS). Spectrum sessions were randomly assigned to training (n=32) and testing sets (n=13). Six different threshold selection criteria were compared for automatic AU coding. Three major findings emerged: 1) Thresholds that attempt to balance the confusion matrix (using kappa, \\(F_1\\), or MCC) performed significantly better on all metrics than thresholds that select arbitrary error or accuracy rates (such as TPR, FPR, or EER). 2) AU detection scores for kappa, \\(F_1\\), and MCC were highly intercorrelated; accuracy was uncorrelated with the others. And 3) Kappa, MCC, and \\(F_1\\) were all positively correlated with base rate. They increased with increases in AU base rates. Accuracy, by contrast, showed the opposite pattern. It was strongly negatively correlated with base rate. These findings suggest that better automatic coding can be obtained by using threshold-selection criteria that balance the confusion matrix and benefit from increased AU base rates in the training data."
  },
  {
    "objectID": "publications/proceedings/girard2011.html#author-note",
    "href": "publications/proceedings/girard2011.html#author-note",
    "title": "Criteria and metrics for thresholded AU detection",
    "section": "Author Note",
    "text": "Author Note\n\nWhen I wrote this paper back in 2011, I was just learning about performance evaluation. This was a first, and rather naive attempt at understanding the connection between agreement, prevalence, and threshold selection. Readers interested in more sophisticated approaches to these issues are encouraged to look up Guangchao Charles Feng, who has done nice work in this area.\nr tufte::quote_footer('--- Jeffrey Girard, 2018-06-14')"
  },
  {
    "objectID": "publications/articles/swartz2023.html",
    "href": "publications/articles/swartz2023.html",
    "title": "Randomized trial of brief interpersonal psychotherapy and cognitive behavioral therapy for depression delivered both in-person and by telehealth",
    "section": "",
    "text": "Swartz, H. A., Bylsma, L. M., Fournier, J. C., Girard, J. M., Spotts, C., Cohn, J. F., & Morency, L.-P. (2023). Randomized trial of brief interpersonal psychotherapy and cognitive behavioral therapy for depression delivered both in-person and by telehealth. Journal of Affective Disorders, 333, 543–552."
  },
  {
    "objectID": "publications/articles/swartz2023.html#citation-apa-7",
    "href": "publications/articles/swartz2023.html#citation-apa-7",
    "title": "Randomized trial of brief interpersonal psychotherapy and cognitive behavioral therapy for depression delivered both in-person and by telehealth",
    "section": "",
    "text": "Swartz, H. A., Bylsma, L. M., Fournier, J. C., Girard, J. M., Spotts, C., Cohn, J. F., & Morency, L.-P. (2023). Randomized trial of brief interpersonal psychotherapy and cognitive behavioral therapy for depression delivered both in-person and by telehealth. Journal of Affective Disorders, 333, 543–552."
  },
  {
    "objectID": "publications/articles/swartz2023.html#abstract",
    "href": "publications/articles/swartz2023.html#abstract",
    "title": "Randomized trial of brief interpersonal psychotherapy and cognitive behavioral therapy for depression delivered both in-person and by telehealth",
    "section": "Abstract",
    "text": "Abstract\nBackground: Expert consensus guidelines recommend Cognitive Behavioral Therapy (CBT) and Interpersonal Psychotherapy (IPT), interventions that were historically delivered face-to-face, as first-line treatments for Major Depressive Disorder (MDD). Despite the ubiquity of telehealth following the COVID-19 pandemic, little is known about differential outcomes with CBT versus IPT delivered in-person (IP) or via telehealth (TH) or whether working alliance is affected.\nMethods: Adults meeting DSM-5 criteria for MDD were randomly assigned to either 8 sessions of IPT or CBT (group). Mid-trial, COVID-19 forced a change of therapy delivery from IP to TH (study phase). We compared changes in Hamilton Rating Scale for Depression (HRSD-17) and Working Alliance Inventory (WAI) scores for individuals by group and phase: CBT-IP (n = 24), CBT-TH (n = 11), IPT-IP (n = 25) and IPT-TH (n = 17).\nResults: HRSD-17 scores declined significantly from pre to post treatment (pre: M = 17.7, SD = 4.4 vs. post: M = 11.7, SD = 5.9; p &lt; .001; d = 1.45) without significant group or phase effects. WAI scores did not differ by group or phase. Number of completed therapy sessions was greater for TH (M = 7.8, SD = 1.2) relative to IP (M = 7.2, SD = 1.6) (Mann-Whitney U = 387.50, z = 2.24, p = .025).\nLimitations: Participants were not randomly assigned to IP versus TH. Sample size is small.\nConclusions: This study provides preliminary evidence supporting the efficacy of both brief IPT and CBT, delivered by either TH or IP, for depression. It showed that working alliance is preserved in TH, and delivery via TH may improve therapy adherence. Prospective, randomized controlled trials are needed to definitively test efficacy of brief IPT and CBT delivered via TH versus IP."
  },
  {
    "objectID": "publications/articles/sewall2021.html",
    "href": "publications/articles/sewall2021.html",
    "title": "A Bayesian multilevel analysis of the longitudinal associations between relationship quality and suicidal ideation and attempts among youth with bipolar disorder",
    "section": "",
    "text": "Sewall, C. J. R., Girard, J. M., Merranko, J., Hafeman, D., Goldstein, B. I., Strober, M., Hower, H., Weinstock, L. M., Yen, S., Ryan, N. D., Keller, M. B., Liao, F., Diler, R. S., Gill, M. K., Axelson, D., Birmaher, B., & Goldstein, T. R. (2021). A Bayesian multilevel analysis of the longitudinal associations between relationship quality and suicidal ideation and attempts among youth with bipolar disorder. The Journal of Child Psychology and Psychiatry, 62(7), 905–9115."
  },
  {
    "objectID": "publications/articles/sewall2021.html#citation-apa-7",
    "href": "publications/articles/sewall2021.html#citation-apa-7",
    "title": "A Bayesian multilevel analysis of the longitudinal associations between relationship quality and suicidal ideation and attempts among youth with bipolar disorder",
    "section": "",
    "text": "Sewall, C. J. R., Girard, J. M., Merranko, J., Hafeman, D., Goldstein, B. I., Strober, M., Hower, H., Weinstock, L. M., Yen, S., Ryan, N. D., Keller, M. B., Liao, F., Diler, R. S., Gill, M. K., Axelson, D., Birmaher, B., & Goldstein, T. R. (2021). A Bayesian multilevel analysis of the longitudinal associations between relationship quality and suicidal ideation and attempts among youth with bipolar disorder. The Journal of Child Psychology and Psychiatry, 62(7), 905–9115."
  },
  {
    "objectID": "publications/articles/sewall2021.html#abstract",
    "href": "publications/articles/sewall2021.html#abstract",
    "title": "A Bayesian multilevel analysis of the longitudinal associations between relationship quality and suicidal ideation and attempts among youth with bipolar disorder",
    "section": "Abstract",
    "text": "Abstract\n\nBackground\nYouth with bipolar disorder (BD) are at high risk for suicidal thoughts and behaviors and frequently experience interpersonal impairment, which is a risk factor for suicide. Yet, no study to date has examined the longitudinal associations between relationship quality in family/peer domains and suicidal thoughts and behaviors among youth with BD. Thus, we investigated how between-person differences – reflecting the average relationship quality across time – and within-person changes, reflecting recent fluctuations in relationship quality, act as distal and/or proximal risk factors for suicidal ideation (SI) and suicide attempts.\n\n\nMethods\nWe used longitudinal data from the Course and Outcome of Bipolar Youth Study (N = 413). Relationship quality variables were decomposed into stable (i.e., average) and varying (i.e., recent) components and entered, along with major clinical covariates, into separate Bayesian multilevel models predicting SI and suicide attempt. We also examined how the relationship quality effects interacted with age and sex.\n\n\nResults\nPoorer average relationship quality with parents (β = −.33, 95% Bayesian highest density interval (HDI) [−0.54, −0.11]) or friends (β = −.33, 95% HDI [−0.55, −0.11]) was longitudinally associated with increased risk of SI but not suicide attempt. Worsening recent relationship quality with parents (β = −.10, 95% HDI [−0.19, −0.03]) and, to a lesser extent, friends (β = −.06, 95% HDI [−0.15, 0.03]) was longitudinally associated with increased risk of SI, but only worsening recent relationship quality with parents was also associated with increased risk of suicide attempt (β = −.15, 95% HDI [−0.31, 0.01]). The effects of certain relationship quality variables were moderated by gender but not age.\n\n\nConclusions\nAmong youth with BD, having poorer average relationship quality with peers and/or parents represents a distal risk factor for SI but not suicide attempts. Additionally, worsening recent relationship quality with parents may be a time-sensitive indicator of increased risk for SI or suicide attempt."
  },
  {
    "objectID": "publications/articles/pacella2018.html",
    "href": "publications/articles/pacella2018.html",
    "title": "The association between daily posttraumatic stress symptoms and pain over the first 14-days after injury: An experience sampling study",
    "section": "",
    "text": "Pacella, M. L., Girard, J. M., Wright, A. G. C., Suffoletto, B., & Callaway, C. W. (2018). The association between daily posttraumatic stress symptoms and pain over the first 14 days after injury: An experience sampling study. Academic Emergency Medicine, 25(8), 844–855."
  },
  {
    "objectID": "publications/articles/pacella2018.html#citation-apa-7",
    "href": "publications/articles/pacella2018.html#citation-apa-7",
    "title": "The association between daily posttraumatic stress symptoms and pain over the first 14-days after injury: An experience sampling study",
    "section": "",
    "text": "Pacella, M. L., Girard, J. M., Wright, A. G. C., Suffoletto, B., & Callaway, C. W. (2018). The association between daily posttraumatic stress symptoms and pain over the first 14 days after injury: An experience sampling study. Academic Emergency Medicine, 25(8), 844–855."
  },
  {
    "objectID": "publications/articles/pacella2018.html#abstract",
    "href": "publications/articles/pacella2018.html#abstract",
    "title": "The association between daily posttraumatic stress symptoms and pain over the first 14-days after injury: An experience sampling study",
    "section": "Abstract",
    "text": "Abstract\n\nObjectives\nPsychosocial factors and responses to injury modify the transition from acute to chronic pain. Specifically, posttraumatic stress disorder (PTSD) symptoms (reexperiencing, avoidance, and hyperarousal symptoms) exacerbate and cooccur with chronic pain. Yet no study has prospectively considered the associations among these psychological processes and pain reports using experience sampling methods (ESMs) during the acute aftermath of injury. This study applied ESM via daily text messaging to monitor and detect relationships among psychosocial factors and postinjury pain across the first 14 days after emergency department (ED) discharge.\n\n\nMethods\nWe recruited 75 adults (59% male; mean ± SD age = 34 ± 11.73 years) who experienced a potentially traumatic injury (i.e., involving life threat or serious injury) in the past 24 hours from the EDs of two Level I trauma centers. Participants received five questions per day via text messaging from Day 1 to Day 14 post–ED discharge; three questions measured PTSD symptoms, one question measured perceived social support, and one question measured physical pain.\n\n\nResults\nSixty-seven participants provided sufficient data for inclusion in the final analyses, and the average response rate per subject was 86%. Pain severity score decreased from a mean ± SD of 7.2 ± 2.0 to 4.4 ± 2.69 over 14 days and 50% of the variance in daily pain scores was within person. In multilevel structural equation models, pain scores decreased over time, and daily fluctuations of hyperarousal (B = 0.22, 95% confidetnce interval = 0.08–0.36) were uniquely associated with daily fluctuations in reported pain level within each person.\n\n\nConclusions\nDaily hyperarousal symptoms predict same-day pain severity over the acute postinjury recovery period. We also demonstrated feasibility to screen and identify patients at risk for pain chronicity in the acute aftermath of injury. Early interventions aimed at addressing hyperarousal (e.g., anxiolytics) could potentially aid in reducing experience of pain."
  },
  {
    "objectID": "publications/articles/hopwood2020.html",
    "href": "publications/articles/hopwood2020.html",
    "title": "Properties of the Continuous Assessment of Interpersonal Dynamics across sex, level of familiarity, and interpersonal conflict",
    "section": "",
    "text": "Hopwood, C. J., Harrison, A. L., Amole, M. C., Girard, J. M., Wright, A. G. C., Thomas, K. M., Sadler, P., Ansell, E. B., Chaplin, T. M., Morey, L. C., Crowley, M. J., Durbin, C. E., & Kashy, D. A. (2020). Properties of the continuous assessment of interpersonal dynamics across sex, level of familiarity, and interpersonal conflict. Assessment, 27(1), 40–56."
  },
  {
    "objectID": "publications/articles/hopwood2020.html#citation-apa-7",
    "href": "publications/articles/hopwood2020.html#citation-apa-7",
    "title": "Properties of the Continuous Assessment of Interpersonal Dynamics across sex, level of familiarity, and interpersonal conflict",
    "section": "",
    "text": "Hopwood, C. J., Harrison, A. L., Amole, M. C., Girard, J. M., Wright, A. G. C., Thomas, K. M., Sadler, P., Ansell, E. B., Chaplin, T. M., Morey, L. C., Crowley, M. J., Durbin, C. E., & Kashy, D. A. (2020). Properties of the continuous assessment of interpersonal dynamics across sex, level of familiarity, and interpersonal conflict. Assessment, 27(1), 40–56."
  },
  {
    "objectID": "publications/articles/hopwood2020.html#abstract",
    "href": "publications/articles/hopwood2020.html#abstract",
    "title": "Properties of the Continuous Assessment of Interpersonal Dynamics across sex, level of familiarity, and interpersonal conflict",
    "section": "Abstract",
    "text": "Abstract\nThe Continuous Assessment of Interpersonal Dynamics (CAID) is a method in which trained observers code individuals’ dominance and warmth continuously while they interact in dyads. This method has significant promise for assessing dynamic interpersonal processes. The purpose of this study was to examine the impact of individual sex, dyadic familiarity, and situational conflict on patterns of interpersonal warmth, dominance, and complementarity as assessed via CAID. We used six samples with 603 dyads, including 2 samples of unacquainted mixed-sex undergraduates interacting in a collaborative task, 2 samples of couples interacting in both collaborative and conflict tasks, and 2 samples of mothers and children interacting in both collaborative and conflict tasks. Complementarity effects were robust across all samples, and individuals tended to be relatively warm and dominant. Results from multilevel models indicated that women were slightly warmer than men whereas there were no sex differences in dominance. Unfamiliar dyads and dyads interacting in more collaborative tasks were relatively warmer, more submissive, and more complementary on warmth but less complementary on dominance. These findings speak to the utility of the CAID method for assessing interpersonal dynamics and provide norms for researchers who use the method for different types of samples and applications."
  },
  {
    "objectID": "publications/articles/girard2022a.html",
    "href": "publications/articles/girard2022a.html",
    "title": "Computational analysis of spoken language in acute psychosis and mania",
    "section": "",
    "text": "Girard, J. M., Vail, A. K., Liebenthal, E., Brown, K., Kilciksiz, C. M., Pennant, L., Liebson, E., Öngür, D., Morency, L.-P., & Baker, J. T. (2022). Computational analysis of spoken language in acute psychosis and mania. Schizophrenia Research, 245, 97–115."
  },
  {
    "objectID": "publications/articles/girard2022a.html#citation-apa-7",
    "href": "publications/articles/girard2022a.html#citation-apa-7",
    "title": "Computational analysis of spoken language in acute psychosis and mania",
    "section": "",
    "text": "Girard, J. M., Vail, A. K., Liebenthal, E., Brown, K., Kilciksiz, C. M., Pennant, L., Liebson, E., Öngür, D., Morency, L.-P., & Baker, J. T. (2022). Computational analysis of spoken language in acute psychosis and mania. Schizophrenia Research, 245, 97–115."
  },
  {
    "objectID": "publications/articles/girard2022a.html#abstract",
    "href": "publications/articles/girard2022a.html#abstract",
    "title": "Computational analysis of spoken language in acute psychosis and mania",
    "section": "Abstract",
    "text": "Abstract\n\nObjectives\nThis study aimed to (1) determine the feasibility of collecting behavioral data from participants hospitalized with acute psychosis and (2) begin to evaluate the clinical information that can be computationally derived from such data.\n\n\nMethods\nBehavioral data was collected across 99 sessions from 38 participants recruited from an inpatient psychiatric unit. Each session started with a semi-structured interview modeled on a typical “clinical rounds” encounter and included administration of the Positive and Negative Syndrome Scale (PANSS).\n\n\nAnalysis\nWe quantified aspects of participants’ verbal behavior during the interview using lexical, coherence, and disfluency features. We then used two complementary approaches to explore our second objective. The first approach used predictive models to estimate participants’ PANSS scores from their language features. Our second approach used inferential models to quantify the relationships between individual language features and symptom measures.\n\n\nResults\nOur predictive models showed promise but lacked sufficient data to achieve clinically useful accuracy. Our inferential models identified statistically significant relationships between numerous language features and symptom domains.\n\n\nConclusion\nOur interview recording procedures were well-tolerated and produced adequate data for transcription and analysis. The results of our inferential modeling suggest that automatic measurements of expressive language contain signals highly relevant to the assessment of psychosis. These findings establish the potential of measuring language during a clinical interview in a naturalistic setting and generate specific hypotheses that can be tested in future studies. This, in turn, will lead to more accurate modeling and better understanding of the relationships between expressive language and psychosis."
  },
  {
    "objectID": "publications/articles/girard2018a.html",
    "href": "publications/articles/girard2018a.html",
    "title": "DARMA: Software for dual axis rating and media annotation",
    "section": "",
    "text": "Girard, J. M., & Wright, A. G. C. (2018). DARMA: Software for Dual Axis Rating and Media Annotation. Behavior Research Methods, 50(3), 902–909."
  },
  {
    "objectID": "publications/articles/girard2018a.html#citation-apa-7",
    "href": "publications/articles/girard2018a.html#citation-apa-7",
    "title": "DARMA: Software for dual axis rating and media annotation",
    "section": "",
    "text": "Girard, J. M., & Wright, A. G. C. (2018). DARMA: Software for Dual Axis Rating and Media Annotation. Behavior Research Methods, 50(3), 902–909."
  },
  {
    "objectID": "publications/articles/girard2018a.html#abstract",
    "href": "publications/articles/girard2018a.html#abstract",
    "title": "DARMA: Software for dual axis rating and media annotation",
    "section": "Abstract",
    "text": "Abstract\nContinuous measurement systems provide a means of measuring dynamic behavioral and experiential processes as they play out over time. DARMA is a modernized continuous measurement system that synchronizes media playback and the continuous recording of two-dimensional measurements. These measurements can be observational or self-reported and are provided in real-time through the manipulation of a computer joystick. DARMA also provides tools for reviewing and comparing collected measurements and for customizing various settings. DARMA is a domain-independent software tool that was designed to aid researchers who are interested in gaining a deeper understanding of behavior and experience. It is especially well-suited to the study of affective and interpersonal processes, such as the perception and expression of emotional states and the communication of social signals. DARMA is open-source using the GNU General Public License (GPL) and is available for free download from http://darma.jmgirard.com."
  },
  {
    "objectID": "publications/articles/girard2016a.html",
    "href": "publications/articles/girard2016a.html",
    "title": "A primer on observational measurement",
    "section": "",
    "text": "Girard, J. M., & Cohn, J. F. (2016). A primer on observational measurement. Assessment, 23(4), 404–413."
  },
  {
    "objectID": "publications/articles/girard2016a.html#citation-apa-7",
    "href": "publications/articles/girard2016a.html#citation-apa-7",
    "title": "A primer on observational measurement",
    "section": "",
    "text": "Girard, J. M., & Cohn, J. F. (2016). A primer on observational measurement. Assessment, 23(4), 404–413."
  },
  {
    "objectID": "publications/articles/girard2016a.html#abstract",
    "href": "publications/articles/girard2016a.html#abstract",
    "title": "A primer on observational measurement",
    "section": "Abstract",
    "text": "Abstract\nObservational measurement plays an integral role in a variety of scientific endeavors within biology, psychology, sociology, education, medicine, and marketing. The current article provides an interdisciplinary primer on observational measurement; in particular, it highlights recent advances in observational methodology and the challenges that accompany such growth. First, we detail the various types of instrument that can be used to standardize measurements across observers. Second, we argue for the importance of validity in observational measurement and provide several approaches to validation based on contemporary validity theory. Third, we outline the challenges currently faced by observational researchers pertaining to measurement drift, observer reactivity, reliability analysis, and time/expense. Fourth, we describe recent advances in computer-assisted measurement, fully automated measurement, and statistical data analysis. Finally, we identify several key directions for future observational research to explore."
  },
  {
    "objectID": "publications/articles/girard2015b.html",
    "href": "publications/articles/girard2015b.html",
    "title": "Estimating smile intensity: A better way",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., & De la Torre, F. (2015). Estimating smile intensity: A better way. Pattern Recognition Letters, 66, 13–21."
  },
  {
    "objectID": "publications/articles/girard2015b.html#citation-apa-7",
    "href": "publications/articles/girard2015b.html#citation-apa-7",
    "title": "Estimating smile intensity: A better way",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., & De la Torre, F. (2015). Estimating smile intensity: A better way. Pattern Recognition Letters, 66, 13–21."
  },
  {
    "objectID": "publications/articles/girard2015b.html#abstract",
    "href": "publications/articles/girard2015b.html#abstract",
    "title": "Estimating smile intensity: A better way",
    "section": "Abstract",
    "text": "Abstract\nBoth the occurrence and intensity of facial expressions are critical to what the face reveals. While much progress has been made towards the automatic detection of facial expression occurrence, controversy exists about how to estimate expression intensity. The most straight-forward approach is to train multiclass or regression models using intensity ground truth. However, collecting intensity ground truth is even more time consuming and expensive than collecting binary ground truth. As a shortcut, some researchers have proposed using the decision values of binary-trained maximum margin classifiers as a proxy for expression intensity. We provide empirical evidence that this heuristic is flawed in practice as well as in theory. Unfortunately, there are no shortcuts when it comes to estimating smile intensity: researchers must take the time to collect and train on intensity ground truth. However, if they do so, high reliability with expert human coders can be achieved. Intensity-trained multiclass and regression models outperformed binary-trained classifier decision values on smile intensity estimation across multiple databases and methods for feature extraction and dimensionality reduction. Multiclass models even outperformed binary–trained classifiers on smile occurrence detection."
  },
  {
    "objectID": "publications/articles/girard2014c.html",
    "href": "publications/articles/girard2014c.html",
    "title": "BP4D-Spontaneous: A high-resolution spontaneous 3D dynamic facial expression database",
    "section": "",
    "text": "Zhang, X., Yin, L., Cohn, J. F., Canavan, S., Reale, M., Horowitz, A., Liu, P., & Girard, J. M. (2014). BP4D-Spontaneous: A high-resolution spontaneous 3D dynamic facial expression database. Image and Vision Computing, 32(10), 692–706."
  },
  {
    "objectID": "publications/articles/girard2014c.html#citation-apa-7",
    "href": "publications/articles/girard2014c.html#citation-apa-7",
    "title": "BP4D-Spontaneous: A high-resolution spontaneous 3D dynamic facial expression database",
    "section": "",
    "text": "Zhang, X., Yin, L., Cohn, J. F., Canavan, S., Reale, M., Horowitz, A., Liu, P., & Girard, J. M. (2014). BP4D-Spontaneous: A high-resolution spontaneous 3D dynamic facial expression database. Image and Vision Computing, 32(10), 692–706."
  },
  {
    "objectID": "publications/articles/girard2014c.html#abstract",
    "href": "publications/articles/girard2014c.html#abstract",
    "title": "BP4D-Spontaneous: A high-resolution spontaneous 3D dynamic facial expression database",
    "section": "Abstract",
    "text": "Abstract\nFacial expression is central to human experience. Its efficiency and valid measurement are challenges that automated facial image analysis seeks to address. Most publically available databases are limited to 2D static images or video of posed facial behavior. Because posed and un-posed (aka “spontaneous”) facial expressions differ along several dimensions including complexity and timing, well-annotated video of un-posed facial behavior is needed. Moreover, because the face is a three-dimensional deformable object, 2D video may be insufficient, and therefore 3D video archives are required. We present a newly developed 3D video database of spontaneous facial expressions in a diverse group of young adults. Well-validated emotion inductions were used to elicit expressions of emotion and paralinguistic communication. Frame-level ground-truth for facial actions was obtained using the Facial Action Coding System. Facial features were tracked in both 2D and 3D domains. To the best of our knowledge, this new database is the first of its kind for the public. The work promotes the exploration of 3D spatiotemporal features in subtle facial expression, better understanding of the relation between pose and motion dynamics in facial action units, and deeper understanding of naturally occurring facial action."
  },
  {
    "objectID": "publications/articles/girard2014a.html",
    "href": "publications/articles/girard2014a.html",
    "title": "Nonverbal social withdrawal in depression: Evidence from manual and automatic analyses",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Mahoor, M. H., Mavadati, S. M., Hammal, Z., & Rosenwald, D. P. (2014). Nonverbal social withdrawal in depression: Evidence from manual and automatic analyses. Image and Vision Computing, 32(10), 641–647."
  },
  {
    "objectID": "publications/articles/girard2014a.html#citation-apa-7",
    "href": "publications/articles/girard2014a.html#citation-apa-7",
    "title": "Nonverbal social withdrawal in depression: Evidence from manual and automatic analyses",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Mahoor, M. H., Mavadati, S. M., Hammal, Z., & Rosenwald, D. P. (2014). Nonverbal social withdrawal in depression: Evidence from manual and automatic analyses. Image and Vision Computing, 32(10), 641–647."
  },
  {
    "objectID": "publications/articles/girard2014a.html#abstract",
    "href": "publications/articles/girard2014a.html#abstract",
    "title": "Nonverbal social withdrawal in depression: Evidence from manual and automatic analyses",
    "section": "Abstract",
    "text": "Abstract\nThe relationship between nonverbal behavior and severity of depression was investigated by following depressed participants over the course of treatment and video recording a series of clinical interviews. Facial expressions and head pose were analyzed from video using manual and automatic systems. Both systems were highly consistent for FACS action units (AUs) and showed similar effects for change over time in depression severity. When symptom severity was high, participants made fewer affiliative facial expressions (AUs 12 and 15) and more non-affiliative facial expressions (AU 14). Participants also exhibited diminished head motion (i.e., amplitude and velocity) when symptom severity was high. These results are consistent with the Social Withdrawal hypothesis: that depressed individuals use nonverbal behavior to maintain or increase interpersonal distance. As individuals recover, they send more signals indicating a willingness to affiliate. The finding that automatic facial expression analysis was both consistent with manual coding and revealed the same pattern of findings suggests that automatic facial expression analysis may be ready to relieve the burden of manual coding in behavioral and clinical science."
  },
  {
    "objectID": "publications/articles/fairbairn2015.html",
    "href": "publications/articles/fairbairn2015.html",
    "title": "Speech volume indexes gender differences in the social-emotional effects of alcohol",
    "section": "",
    "text": "Fairbairn, C. E., Sayette, M. A., Amole, M. C., Dimoff, J. D., Cohn, J. F., & Girard, J. M. (2015). Speech volume indexes sex differences in the social-emotional effects of alcohol. Experimental & Clinical Psychopharmacology, 23(4), 255–264."
  },
  {
    "objectID": "publications/articles/fairbairn2015.html#citation-apa-7",
    "href": "publications/articles/fairbairn2015.html#citation-apa-7",
    "title": "Speech volume indexes gender differences in the social-emotional effects of alcohol",
    "section": "",
    "text": "Fairbairn, C. E., Sayette, M. A., Amole, M. C., Dimoff, J. D., Cohn, J. F., & Girard, J. M. (2015). Speech volume indexes sex differences in the social-emotional effects of alcohol. Experimental & Clinical Psychopharmacology, 23(4), 255–264."
  },
  {
    "objectID": "publications/articles/fairbairn2015.html#abstract",
    "href": "publications/articles/fairbairn2015.html#abstract",
    "title": "Speech volume indexes gender differences in the social-emotional effects of alcohol",
    "section": "Abstract",
    "text": "Abstract\nMen and women differ dramatically in their rates of alcohol use disorder (AUD), and researchers have long been interested in identifying mechanisms underlying male vulnerability to problem drinking. Surveys suggest that social processes underlie sex differences in drinking patterns, with men reporting greater social enhancement from alcohol than women, and all-male social drinking contexts being associated with particularly high rates of hazardous drinking. But experimental evidence for sex differences in social-emotional response to alcohol has heretofore been lacking. Research using larger sample sizes, a social context, and more sensitive measures of alcohol’s rewarding effects may be necessary to better understand sex differences in the etiology of AUD. This study explored the acute effects of alcohol during social exchange on speech volume –an objective measure of social-emotional experience that was reliably captured at the group level. Social drinkers (360 male; 360 female) consumed alcohol (.82g/kg males; .74g/kg females), placebo, or a no-alcohol control beverage in groups of three over 36-minutes. Within each of the three beverage conditions, equal numbers of groups consisted of all males, all females, 2 females and 1 male, and 1 female and 2 males. Speech volume was monitored continuously throughout the drink period, and group volume emerged as a robust correlate of self-report and facial indexes of social reward. Notably, alcohol-related increases in group volume were observed selectively in all-male groups but not in groups containing any females. Results point to social enhancement as a promising direction for research exploring factors underlying sex differences in problem drinking."
  },
  {
    "objectID": "posts/vail2022/index.html",
    "href": "posts/vail2022/index.html",
    "title": "ICMI 2022 Paper",
    "section": "",
    "text": "I am very pleased to announce that a new conference paper from our team has been published in the Proceedings of the 2022 International Conference on Multimodal Interaction.\nThis paper, entitled “Toward causal understanding of therapist-client relationships: A study of language modality and social entrainment,” explores the associations between working alliance ratings and linguistic entrainment between therapist and patients with depression during brief psychotherapy.\nThis paper was authored by Vail, Girard, Bylsma, Cohn, Fournier, Swartz, and Morency and was supported in part by our “Dyadic Behavior Informatics for Psychotherapy Process and Outcome” grant from the National Science Foundation.\nClick here to access the paper"
  },
  {
    "objectID": "posts/rowwise2023/index.html",
    "href": "posts/rowwise2023/index.html",
    "title": "Row-wise means in dplyr",
    "section": "",
    "text": "For those looking for a quick answer, here is an example of my recommended approach, which calculates a new variable, the mean fuel efficiency (mfe) of each car, as the row-wise mean of two existing variables, highway fuel efficiency (hwy) and city fuel efficiency (cty):\n\nlibrary(tidyverse) # needs dplyr version 1.1.0+\n\n# Calculate mean fuel efficiency (mfe) from highway (hwy) and city (cty)\nmpg |&gt; mutate(mfe = rowMeans(pick(hwy, cty)))"
  },
  {
    "objectID": "posts/rowwise2023/index.html#tldr",
    "href": "posts/rowwise2023/index.html#tldr",
    "title": "Row-wise means in dplyr",
    "section": "",
    "text": "For those looking for a quick answer, here is an example of my recommended approach, which calculates a new variable, the mean fuel efficiency (mfe) of each car, as the row-wise mean of two existing variables, highway fuel efficiency (hwy) and city fuel efficiency (cty):\n\nlibrary(tidyverse) # needs dplyr version 1.1.0+\n\n# Calculate mean fuel efficiency (mfe) from highway (hwy) and city (cty)\nmpg |&gt; mutate(mfe = rowMeans(pick(hwy, cty)))"
  },
  {
    "objectID": "posts/rowwise2023/index.html#introduction",
    "href": "posts/rowwise2023/index.html#introduction",
    "title": "Row-wise means in dplyr",
    "section": "Introduction",
    "text": "Introduction\ndplyr is an amazing tool for data wrangling and I use it daily. However, there is one type of operation that I frequently do that has historically caused me some confusion and frustration: row-wise means. Once I figured out what was going on, I wanted to share what I learned through this brief blog post. It will focus on how to avoid some common issues I ran into and how to speed up rowwise operations with large data frames. I hope some find it helpful.\n\n# Load packages used in this post\nlibrary(tidyverse)\nlibrary(microbenchmark)\n\n# Set random seed for reproduciblity\nset.seed(2023)"
  },
  {
    "objectID": "posts/rowwise2023/index.html#simulated-example-data",
    "href": "posts/rowwise2023/index.html#simulated-example-data",
    "title": "Row-wise means in dplyr",
    "section": "Simulated Example Data",
    "text": "Simulated Example Data\nLet’s say we have a tibble (or data frame) containing 10 observations and 4 numerical variables: y, x1, x2, and x3. We can simulate this quickly using rnorm() to sample from different normal distributions.\n\n# Simulate data\ny  &lt;- rnorm(n = 10, mean = 100, sd = 15)\nx1 &lt;- rnorm(n = 10, mean =   0, sd =  1)\nx2 &lt;- rnorm(n = 10, mean =  10, sd = 10)\nx3 &lt;- rnorm(n = 10, mean =  20, sd =  5)\nn10 &lt;- tibble(y, x1, x2, x3)"
  },
  {
    "objectID": "posts/rowwise2023/index.html#doing-it-by-hand",
    "href": "posts/rowwise2023/index.html#doing-it-by-hand",
    "title": "Row-wise means in dplyr",
    "section": "Doing it “by-hand”",
    "text": "Doing it “by-hand”\nNow let’s say we want to add a new variable xmean to the tibble containing each observation’s mean of x1, x2, and x3.\nWe can use mutate() and math to achieve this:\n\n# Example 0 (works but inconvenient)\nn10 |&gt; \n  mutate(xmean = (x1 + x2 + x3) / 3)\n\n# A tibble: 10 × 5\n       y     x1    x2    x3 xmean\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  98.7  0.327  5.88  18.6  8.28\n 2  85.3 -0.413  7.06  26.4 11.0 \n 3  71.9  0.562 22.2   15.9 12.9 \n 4  97.2  0.663 12.4   19.8 11.0 \n 5  90.5 -0.603  5.55  16.8  7.25\n 6 116.   0.698 -8.48  17.8  3.34\n 7  86.3  0.596  3.71  22.5  8.95\n 8 115.   0.452  1.39  25.6  9.16\n 9  94.0  0.897 25.1   24.9 17.0 \n10  93.0  0.572 37.4   19.4 19.1"
  },
  {
    "objectID": "posts/rowwise2023/index.html#a-failed-attempt",
    "href": "posts/rowwise2023/index.html#a-failed-attempt",
    "title": "Row-wise means in dplyr",
    "section": "A Failed Attempt",
    "text": "A Failed Attempt\nBut this approach will be a hassle if you have any missing values or if you have lots of variables to average. Instead, if you are just learning {dplyr}, you would probably try to combine the mean() and mutate() functions as below.\n\n# Example 1 (doesn't work)\nn10 |&gt; \n  mutate(xmean = mean(c(x1, x2, x3)))\n\n# A tibble: 10 × 5\n       y     x1    x2    x3 xmean\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  98.7  0.327  5.88  18.6  10.8\n 2  85.3 -0.413  7.06  26.4  10.8\n 3  71.9  0.562 22.2   15.9  10.8\n 4  97.2  0.663 12.4   19.8  10.8\n 5  90.5 -0.603  5.55  16.8  10.8\n 6 116.   0.698 -8.48  17.8  10.8\n 7  86.3  0.596  3.71  22.5  10.8\n 8 115.   0.452  1.39  25.6  10.8\n 9  94.0  0.897 25.1   24.9  10.8\n10  93.0  0.572 37.4   19.4  10.8\n\n\nHowever, you’ll notice in the output above that the new xmean variable contains repetitions of a constant value. What is going on here? Basically, what mutate() did was take all the numbers in x1, x2, and x3, combine them into one long vector of 30 numbers, and send that vector to the mean() function. The mean() function then returns a single value—the mean of all 30 numbers—and tries to put that into the new column xmean. But because the column needs to be a vector of 10 numbers to fit into the tibble, that single value gets recycled (i.e., repeated 10 times). To verify this is what happened, we can do the operation by hand and see that we get the same number:\n\nmean(c(x1, x2, x3))\n\n[1] 10.79233\n\n\nSo, clearly mutate() is not doing what we intended it to do."
  },
  {
    "objectID": "posts/rowwise2023/index.html#the-across-approach",
    "href": "posts/rowwise2023/index.html#the-across-approach",
    "title": "Row-wise means in dplyr",
    "section": "The across() approach",
    "text": "The across() approach\nLuckily, dplyr 1.0.0 added some great features for doing operations within rows. The simplest version simply adds a call to the rowwise() function to our pipeline before the mutate().\n\n# Example 2 (works but slow)\nn10 |&gt; \n  rowwise() |&gt;  \n  mutate(xmean = mean(c(x1, x2, x3)))\n\n# A tibble: 10 × 5\n# Rowwise: \n       y     x1    x2    x3 xmean\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  98.7  0.327  5.88  18.6  8.28\n 2  85.3 -0.413  7.06  26.4 11.0 \n 3  71.9  0.562 22.2   15.9 12.9 \n 4  97.2  0.663 12.4   19.8 11.0 \n 5  90.5 -0.603  5.55  16.8  7.25\n 6 116.   0.698 -8.48  17.8  3.34\n 7  86.3  0.596  3.71  22.5  8.95\n 8 115.   0.452  1.39  25.6  9.16\n 9  94.0  0.897 25.1   24.9 17.0 \n10  93.0  0.572 37.4   19.4 19.1 \n\n\nThis did what we wanted it to do, despite the actual mutate() call being identical to what is was before! Pretty cool. We can even save some time by selecting the variables to include in the mean() operation automatically, instead of listing them out in the c() function. This isn’t such a time-savings in this case with only three variables, but in settings with more variables it can really add up. To do so, we just need to use a tidy selection function; in this case, all the variables we want to include start with the letter “x” so let’s use starts_with().\n\n# Example 3 (doesn't work)\nn10 |&gt; \n  rowwise()|&gt; \n  mutate(xmean = mean(starts_with(\"x\")))\n\nError in `mutate()`:\nℹ In argument: `xmean = mean(starts_with(\"x\"))`.\nℹ In row 1.\nCaused by error:\n! `starts_with()` must be used within a *selecting* function.\nℹ See &lt;https://tidyselect.r-lib.org/reference/faq-selection-context.html&gt; for\n  details.\n\n\nShoot, that didn’t work. But why not? Basically, the problem is that mutate() doesn’t know what do to with selection functions like starts_with(). The error message basically says that we are in the “wrong context” for a selection function."
  },
  {
    "objectID": "posts/rowwise2023/index.html#tidy-selection-with-c_across",
    "href": "posts/rowwise2023/index.html#tidy-selection-with-c_across",
    "title": "Row-wise means in dplyr",
    "section": "Tidy selection with c_across()",
    "text": "Tidy selection with c_across()\nLuckily, dplyr 1.0.0 also added the c_across() function, which will allow us to change the context to one that does allow selection functions. The code below now works as intended, first selecting all the variables starting with “x” and then computing their row-wise means.\n\n# Example 4 (works but slow)\nn10 |&gt; \n  rowwise() |&gt; \n  mutate(xmean = mean(c_across(starts_with(\"x\"))))\n\n# A tibble: 10 × 5\n# Rowwise: \n       y     x1    x2    x3 xmean\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  98.7  0.327  5.88  18.6  8.28\n 2  85.3 -0.413  7.06  26.4 11.0 \n 3  71.9  0.562 22.2   15.9 12.9 \n 4  97.2  0.663 12.4   19.8 11.0 \n 5  90.5 -0.603  5.55  16.8  7.25\n 6 116.   0.698 -8.48  17.8  3.34\n 7  86.3  0.596  3.71  22.5  8.95\n 8 115.   0.452  1.39  25.6  9.16\n 9  94.0  0.897 25.1   24.9 17.0 \n10  93.0  0.572 37.4   19.4 19.1"
  },
  {
    "objectID": "posts/rowwise2023/index.html#remember-to-ungroup",
    "href": "posts/rowwise2023/index.html#remember-to-ungroup",
    "title": "Row-wise means in dplyr",
    "section": "Remember to ungroup()",
    "text": "Remember to ungroup()\nThere are two things to note about rowwise(), however. First, it transformed our tibble into an implicitly “grouped” tibble, which is what allowed our mutate() function to calculate row-wise means instead of overall means (basically, it is treating each row as a separate group and calculating the means per group/row). However, after that mutate() call, the tibble remains grouped. This is handy if we want to continue doing row-wise operations, but how do we tell it to stop once we are done with row-wise operations and want to return to “normal” behavior? Let’s see when this could be a problem; one example is if we want to calculate the maximum row-wise mean xmean_max.\n\n# Example 5 (doesn't work)\nn10 |&gt; \n  rowwise() |&gt; \n  mutate(xmean = mean(c_across(starts_with(\"x\")))) |&gt; \n  summarize(xmean_max = max(xmean))\n\n# A tibble: 10 × 1\n   xmean_max\n       &lt;dbl&gt;\n 1      8.28\n 2     11.0 \n 3     12.9 \n 4     11.0 \n 5      7.25\n 6      3.34\n 7      8.95\n 8      9.16\n 9     17.0 \n10     19.1 \n\n\nHere we wanted to summarize over all values of xmean and expected a single maximum value. Instead we got the same 10 values back. What happened? Basically, our tibble was still implicitly grouped by row and the summarize() function respected that, calculating the maximum of each group/row. To avoid this behavior, we can add the ungroup() function to our pipeline (reverting the tibble back to a standard one without implicit grouping).\n\n# Example 6 (works but slow)\nn10 |&gt; \n  rowwise() |&gt; \n  mutate(xmean = mean(c_across(starts_with(\"x\")))) |&gt; \n  ungroup() |&gt;\n  summarize(xmean_max = max(xmean))\n\n# A tibble: 1 × 1\n  xmean_max\n      &lt;dbl&gt;\n1      19.1\n\n\nNow we get the desired behavior, and so I am usually very careful to add ungroup() to my pipeline as soon as I am done with row-wise operations (otherwise you might end up with some unexpected problems)."
  },
  {
    "objectID": "posts/rowwise2023/index.html#faster-means-with-rowmeans",
    "href": "posts/rowwise2023/index.html#faster-means-with-rowmeans",
    "title": "Row-wise means in dplyr",
    "section": "Faster means with rowMeans()",
    "text": "Faster means with rowMeans()\nThe other thing to note about rowwise() is that it can be slow. With a small tibble like this, it doesn’t matter much, but the difference could be meaningful for larger and more complex data. In these cases, you have some alternatives. This blog post describes some of them, but does not address the specific case of means, which is what I want to do the most in practice. A faster alternative in this case is to use the rowMeans() function. As you might imagine, this function takes in a numeric matrix or dataframe and returns the mean of each row.\n\nrowMeans(n10)\n\n [1] 30.89482 29.57077 27.64172 32.52192 28.06156 31.59763 28.28442 35.62799\n [9] 36.23378 37.56994\n\n\nBut we want to exclude the y variable and append it to the n10 tibble. How to do so? We might reasonably try to put it into mutate() like we did with mean():\n\n# Example 7 (doesn't work)\nn10 |&gt; \n  mutate(xmean = rowMeans(c(x1, x2, x3)))\n\nError in `mutate()`:\nℹ In argument: `xmean = rowMeans(c(x1, x2, x3))`.\nCaused by error in `rowMeans()`:\n! 'x' must be an array of at least two dimensions\n\n\nBut here we run into a problem. rowMeans() is expecting a numeric matrix or data frame, but is being provided with a vector of 30 numbers again (as in Example 1). Thus, it doesn’t have rows to calculate means within and returns an error. We can solve this by transforming the vector to a matrix:\n\n# Example 8 (works but inconvenient)\nn10 |&gt; \n  mutate(xmean = rowMeans(matrix(c(x1, x2, x3), ncol = 3)))\n\n# A tibble: 10 × 5\n       y     x1    x2    x3 xmean\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  98.7  0.327  5.88  18.6  8.28\n 2  85.3 -0.413  7.06  26.4 11.0 \n 3  71.9  0.562 22.2   15.9 12.9 \n 4  97.2  0.663 12.4   19.8 11.0 \n 5  90.5 -0.603  5.55  16.8  7.25\n 6 116.   0.698 -8.48  17.8  3.34\n 7  86.3  0.596  3.71  22.5  8.95\n 8 115.   0.452  1.39  25.6  9.16\n 9  94.0  0.897 25.1   24.9 17.0 \n10  93.0  0.572 37.4   19.4 19.1 \n\n\nBut dplyr 1.1.0 added a way to streamline this kind of thing. We can use the new pick() function to create this context and avoid the need for the dot operator. Note that we using pick() here instead of c_across() because the latter is for working within rows (in combination with rowwise()) and here we want the row-wise operations to be handled by rowMeans().\n\n# Example 11 (works and convenient and fast)\nn10 |&gt; \n  mutate(xmean = rowMeans(pick(starts_with(\"x\"))))\n\n# A tibble: 10 × 5\n       y     x1    x2    x3 xmean\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  98.7  0.327  5.88  18.6  8.28\n 2  85.3 -0.413  7.06  26.4 11.0 \n 3  71.9  0.562 22.2   15.9 12.9 \n 4  97.2  0.663 12.4   19.8 11.0 \n 5  90.5 -0.603  5.55  16.8  7.25\n 6 116.   0.698 -8.48  17.8  3.34\n 7  86.3  0.596  3.71  22.5  8.95\n 8 115.   0.452  1.39  25.6  9.16\n 9  94.0  0.897 25.1   24.9 17.0 \n10  93.0  0.572 37.4   19.4 19.1 \n\n\nTo test the speed of each approach, we can use the microbenchmark package, which will precisely time each approach over multiple iterations (in this case, 100). Let’s wrap up by testing the speech of each approach with increasingly large data sets.\n\n# Create function wrappers so the microbenchmark output is prettier\nA_rowwise &lt;- function(.data) { \n  .data |&gt; \n    rowwise() |&gt; \n    mutate(xmean = mean(c_across(starts_with(\"x\")))) |&gt; \n    ungroup()\n}\n\nB_rowMeans &lt;- function(.data) { \n  .data |&gt; \n    mutate(xmean = rowMeans(pick(starts_with(\"x\"))))\n}\n\n# Simulate larger datasets\nn100 &lt;- n10 |&gt; slice(rep(1:n(), times = 10))\nn1000 &lt;- n100 |&gt; slice(rep(1:n(), times = 10))\nn10000 &lt;- n1000 |&gt; slice(rep(1:n(), times = 10))\n\n# Perform microbenchmarking\nspeedtest &lt;- \n  microbenchmark(\n    A_rowwise(n10),\n    A_rowwise(n100),\n    A_rowwise(n1000),\n    A_rowwise(n10000),\n    B_rowMeans(n10),\n    B_rowMeans(n100), \n    B_rowMeans(n1000),\n    B_rowMeans(n10000),\n    times = 100L\n  )\n\n\n\n\n\n\nApproach\nmean_time_ms\nsd_time_ms\nmin_time_ms\nmax_time_ms\n\n\n\n\nA_rowwise(n10)\n12.025\n1.279\n10.999\n18.844\n\n\nA_rowwise(n100)\n90.208\n4.903\n84.182\n126.810\n\n\nA_rowwise(n1000)\n878.763\n42.149\n835.994\n1053.001\n\n\nA_rowwise(n10000)\n8767.835\n217.319\n8500.523\n10053.289\n\n\nB_rowMeans(n10)\n2.852\n0.530\n2.470\n7.109\n\n\nB_rowMeans(n100)\n2.875\n0.577\n2.451\n5.720\n\n\nB_rowMeans(n1000)\n2.865\n0.501\n2.458\n5.665\n\n\nB_rowMeans(n10000)\n2.972\n0.540\n2.575\n5.646\n\n\n\n\n\n\n\n\nautoplot(speedtest)\n\n\n\n\nThe speed difference between approaches grows with the number of observations and becomes quite noticeable at 1000 observations or more. So, although I think it is worth learning the new rowwise() and c_across() functions, in settings where observations are many and speed in paramount, it may be worthwhile to learn “parallel” functions such as rowMeans(), rowSums(), pmin(), pmax(), and paste()."
  },
  {
    "objectID": "posts/rowwise2023/index.html#session-info",
    "href": "posts/rowwise2023/index.html#session-info",
    "title": "Row-wise means in dplyr",
    "section": "Session Info",
    "text": "Session Info\n\n\n\nClick here for session info\n\n\nsessionInfo()\n\nR version 4.2.3 (2023-03-15 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22624)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] kableExtra_1.3.4     knitr_1.41           microbenchmark_1.4.9\n [4] forcats_0.5.2        stringr_1.5.0        dplyr_1.1.0         \n [7] purrr_1.0.1          readr_2.1.3          tidyr_1.2.1         \n[10] tibble_3.1.8         ggplot2_3.4.0        tidyverse_1.3.2     \n\nloaded via a namespace (and not attached):\n [1] svglite_2.1.1       lubridate_1.9.0     assertthat_0.2.1   \n [4] digest_0.6.31       utf8_1.2.2          R6_2.5.1           \n [7] cellranger_1.1.0    backports_1.4.1     reprex_2.0.2       \n[10] evaluate_0.20       highr_0.10          httr_1.4.4         \n[13] pillar_1.8.1        rlang_1.1.0         googlesheets4_1.0.1\n[16] readxl_1.4.1        rstudioapi_0.14     rmarkdown_2.19     \n[19] webshot_0.5.4       googledrive_2.0.0   htmlwidgets_1.6.2  \n[22] munsell_0.5.0       broom_1.0.2         compiler_4.2.3     \n[25] modelr_0.1.10       xfun_0.36           systemfonts_1.0.4  \n[28] pkgconfig_2.0.3     htmltools_0.5.4     tidyselect_1.2.0   \n[31] viridisLite_0.4.1   fansi_1.0.3         crayon_1.5.2       \n[34] tzdb_0.3.0          dbplyr_2.3.0        withr_2.5.0        \n[37] grid_4.2.3          jsonlite_1.8.4      gtable_0.3.1       \n[40] lifecycle_1.0.3     DBI_1.1.3           magrittr_2.0.3     \n[43] scales_1.2.1        cli_3.6.0           stringi_1.7.12     \n[46] farver_2.1.1        renv_0.16.0         fs_1.5.2           \n[49] xml2_1.3.3          ellipsis_0.3.2      generics_0.1.3     \n[52] vctrs_0.6.0         tools_4.2.3         glue_1.6.2         \n[55] hms_1.1.2           fastmap_1.1.0       yaml_2.3.6         \n[58] timechange_0.2.0    colorspace_2.0-3    gargle_1.2.1       \n[61] rvest_1.0.3         haven_2.5.1"
  },
  {
    "objectID": "posts/rowwise2023/index.html#notes-references",
    "href": "posts/rowwise2023/index.html#notes-references",
    "title": "Row-wise means in dplyr",
    "section": "Notes / References",
    "text": "Notes / References\n\n[1] dplyr vignette: Row-wise operations\n[2] dplyr 1.1.0: pick(), reframe(), arrange()\n[3] dplyr 1.0.0: working within rows\n[4] dplyr issue #4544\nNote: This is an update of an older blog post on my personal website."
  },
  {
    "objectID": "posts/gpsp2023/index.html",
    "href": "posts/gpsp2023/index.html",
    "title": "GPSP 2023 Convention",
    "section": "",
    "text": "Kassy and I attended the Great Plains Student Psychology Convention last weekend, hosted at Emporia State University. I gave an invited keynote address on “Reconsidering the Duchenne Smile,” and Kassy presented her first AffCom lab poster on “Adult attachment and the working alliance during brief psychotherapy for depression.” She was also awarded an Outstanding Graduate Poster award. Congratulations, Kassy!"
  },
  {
    "objectID": "posts/dischargeR01/index.html",
    "href": "posts/dischargeR01/index.html",
    "title": "NIMH R01 Grant",
    "section": "",
    "text": "I am very pleased to announce that we have been awarded an R01 (Research Project) Grant from the National Institute of Mental Health (R01-MH125740) to study the use of multimodal information and machine learning to estimate the discharge readiness of psychiatric inpatients with severe mental illness. The project period is from 04/2021 through 02/2025 with a total funding of $1,160,953."
  },
  {
    "objectID": "posts/dischargeR01/index.html#project-team",
    "href": "posts/dischargeR01/index.html#project-team",
    "title": "NIMH R01 Grant",
    "section": "Project Team",
    "text": "Project Team\n\nMPI: Justin Baker, McLean Hospital, Harvard Medical School\nMPI: Louis-Philippe Morency, Carnegie Mellon University\nCo-I: Jeffrey Girard, University of Kansas"
  },
  {
    "objectID": "posts/dischargeR01/index.html#project-summary",
    "href": "posts/dischargeR01/index.html#project-summary",
    "title": "NIMH R01 Grant",
    "section": "Project Summary",
    "text": "Project Summary\nWhich psychiatric symptoms and behaviors are the most important to assess and manage during critical points in psychiatric healthcare, such as the time leading up to hospital discharge? At present, psychiatry lacks objective tests that could inform this and other clinically challenging–and potentially costly–decisions. Establishing valid objective markers of psychiatric disease processes is especially challenging compared with the development of biomarkers in other fields. One key challenge is lack of available data from psychiatrically ill patients during key periods in their care trajectory, which the present project seeks to address. A second major challenge, also addressed as a core feature in this project, is the complex, context-dependence of human behavioral expression, which greatly complicates efforts to establish robust, objective measures that reflect underlying mental health disease processes. This project will address both barriers, introducing a new computational framework, named Context-Adaptive Multimodal Informatics, to identify and evaluate behavioral biomarkers related to discharge-readiness and symptoms in severe mental illness. The project aims to address five fundamental research challenges: (1) Acquire a multimodal psychiatric discharge-planning dataset of 400 inpatients with severe mental illness; (2) Create self-aware linear and neural models to identify multimodal behavioral biomarkers; (3) Develop context-sensitive linear and neural models to contextualize behavioral biomarkers and quantify the influence of context on behavior; (4) Build a new adaptive assessment planning framework which creates a personalized patient analysis to rank contexts and modalities for the next assessment session; (5) Assess the trustworthiness and generalizability of our measurements, models, and insights. This research will improve basic understanding of social context and behavioral biomarkers, build objective measures for mental health assessment, and more broadly, pave the way for a restructured care-delivery system in which resources are allocated intelligently to ensure assessments are informative with respect to desired clinical objectives."
  },
  {
    "objectID": "posts/dischargeR01/index.html#public-health-relevance",
    "href": "posts/dischargeR01/index.html#public-health-relevance",
    "title": "NIMH R01 Grant",
    "section": "Public Health Relevance",
    "text": "Public Health Relevance\nCharting mental illness trajectories to determine when, where, and how to intervene is a key objective of the NIMH mission to transform the understanding and treatment of mental illness. This project will: (1) deepen our understanding of the trajectory leading to hospitalization discharge with knowledge about behavioral biomarkers and their relationship to symptoms and discharge-readiness; (2) contribute to the knowledge on how social context impacts patient’s behaviors since some symptoms or predictive biomarkers may only be seen in a specific context such as social or solitary interactions; and (3) contribute a credible, objective framework for behavioral measurement by defining interpretable biomarkers and latent factors for symptoms and discharge assessment. The project will also enable a judicious use of remote sensing and non-direct clinical encounters (e.g., telemedicine), which are increasingly viewed as essential capabilities in the medical fields to reduce costs while maintaining or elevating care standards."
  },
  {
    "objectID": "people/staff/yermol_dasha.html",
    "href": "people/staff/yermol_dasha.html",
    "title": "Dasha Yermol",
    "section": "",
    "text": "Home\n    People\n    Research Staff\n    Dasha Yermol"
  },
  {
    "objectID": "people/staff/yermol_dasha.html#biography",
    "href": "people/staff/yermol_dasha.html#biography",
    "title": "Dasha Yermol",
    "section": "Biography",
    "text": "Biography\nDasha Yermol studies how emotion perception is influenced by individual differences (e.g., mental health and personality), the functions of affective synchrony between interacting dyads, and differences in emotion expression due to social factors (e.g., context, social roles).\nDasha obtained her B.A. in psychology with comprehensive honors from the University of Wisconsin-Madison in 2021. After graduating from UW-Madison, she was a lab manager at a child development research lab for two years, studying how children learn language using eye-tracking methods.\nDasha is currently a PhD Student in the Brain, Behavior, and Quantitative Science (BBQ) program. She is also a graduate teaching assistant for undergraduate psychology courses. Outside of the lab, Dasha enjoys drinking great coffee, being outdoors (biking, swimming, hiking), and tending to her many houseplants."
  },
  {
    "objectID": "people/staff/yermol_dasha.html#education",
    "href": "people/staff/yermol_dasha.html#education",
    "title": "Dasha Yermol",
    "section": "Education",
    "text": "Education\n\nUniversity of Kansas | Lawrence, KS, USA PhD Student (Brain, Behavior, and Quantitative Science), 2023–Current\nUniversity of Wisconsin-Madison | Madison, WI, USA BS in Psychology | 2021"
  },
  {
    "objectID": "people/staff/simmons_aaron.html",
    "href": "people/staff/simmons_aaron.html",
    "title": "Robbin Romijnders",
    "section": "",
    "text": "Home\n    People\n    Robbin Romijnders"
  },
  {
    "objectID": "people/staff/simmons_aaron.html#biography",
    "href": "people/staff/simmons_aaron.html#biography",
    "title": "Robbin Romijnders",
    "section": "Biography",
    "text": "Biography\nTBA"
  },
  {
    "objectID": "people/staff/simmons_aaron.html#education",
    "href": "people/staff/simmons_aaron.html#education",
    "title": "Robbin Romijnders",
    "section": "Education",
    "text": "Education\n\nUniversity of Kansas | Lawrence, KS, USA PhD Student (Brain, Behavior, and Quantitative Science), 2023–Current\nUniversity of California: Davis | Davis, CA, USA BS in Psychology (Mathematics Emphasis) | 2015"
  },
  {
    "objectID": "people/staff/girard_jeffrey.html",
    "href": "people/staff/girard_jeffrey.html",
    "title": "Clint Hansen",
    "section": "",
    "text": "Home\n    People\n    Clint Hansen"
  },
  {
    "objectID": "people/staff/girard_jeffrey.html#biography",
    "href": "people/staff/girard_jeffrey.html#biography",
    "title": "Clint Hansen",
    "section": "Biography",
    "text": "Biography\nDr. Jeffrey Girard studies how emotions are expressed through verbal and nonverbal behavior, as well as how interpersonal communication is influenced by individual differences (e.g., personality and mental health) and social factors (e.g., culture and context). This work is deeply interdisciplinary and draws insights and tools from various areas of social science, computer science, statistics, and medicine.\nHe completed his doctoral training under the mentorship of Dr. Jeffrey Cohn (nonverbal behavior, machine learning, depression) and Dr. Aidan Wright (interpersonal functioning, statistics, personality). He then completed his predoctoral clinical internship at the University of Mississippi Medical Center (inpatient psychiatry, trauma treatment, neuropsychology) and a two-year postdoctoral research fellowship at Carnegie Mellon University under the mentorship of Dr. Louis-Philippe Morency (verbal behavior, machine learning, computer science).\nHe is now an Assistant Professor in the department of Psychology at the University of Kansas, where he directs the Affective Communication and Computing lab and the Brain, Behavior, and Quantitative Science (BBQ) program. He teaches undergraduate and graduate courses on psychological assessment, data science, and statistics within the department and offers several summer “bootcamp” courses for graduate students and faculty (on data science and machine learning) through Pitt Methods. He is also the developer and maintainer of numerous open-source research software packages (see Resources)."
  },
  {
    "objectID": "people/staff/girard_jeffrey.html#experience",
    "href": "people/staff/girard_jeffrey.html#experience",
    "title": "Clint Hansen",
    "section": "Experience",
    "text": "Experience\n\nUniversity of Kansas | Lawrence, KS, USA Assistant Professor of Psychology | 2020–Current Wright Faculty Scholar | 2020–2025 Director of the BBQ Program | 2022–Current\nCarnegie Mellon University | Pittsburgh, PA, USA Postdoctoral Researcher, School of Computer Science | 2018–2020\nUniversity of Mississippi Medical Center | Jackson, MS, USA Clinical Intern, Department of Psychiatry | 2017–2018"
  },
  {
    "objectID": "people/staff/girard_jeffrey.html#journal-editing",
    "href": "people/staff/girard_jeffrey.html#journal-editing",
    "title": "Clint Hansen",
    "section": "Journal Editing",
    "text": "Journal Editing\n\nNPP Digital Psychiatry and Neuroscience  Consulting Editor | 2023–Current\nJournal of Psychopathology and Clinical Science  Consulting Editor | 2023–Current\nIEEE Transactions on Affective Computing  Associate Editor | 2022–Current\nCollabra Psychology  Associate Editor | 2021–Current\nClinical Psychological Science  Consulting Editor | 2021–Current\nPsychological Assessment  Consulting Editor | 2020–2022"
  },
  {
    "objectID": "people/staff/girard_jeffrey.html#education",
    "href": "people/staff/girard_jeffrey.html#education",
    "title": "Clint Hansen",
    "section": "Education",
    "text": "Education\n\nUniversity of Pittsburgh | Pittsburgh, PA, USA PhD in Psychology (Clinical) | 2013–2018\nUniversity of Pittsburgh | Pittsburgh, PA, USA MS in Psychology (Clinical) | 2010–2013\nUniversity of Washington | Seattle, WA, USA BA in Psychology & Philosophy (Honors) | 2005–2008"
  },
  {
    "objectID": "people/ras/kang_taegyu.html",
    "href": "people/ras/kang_taegyu.html",
    "title": "Tae Gyu Kang",
    "section": "",
    "text": "Home\n    People\n    Research Assistants\n    Tae Gyu Kang"
  },
  {
    "objectID": "people/ras/kang_taegyu.html#biography",
    "href": "people/ras/kang_taegyu.html#biography",
    "title": "Tae Gyu Kang",
    "section": "Biography",
    "text": "Biography\nTae Gyu (Lucas) Kang is an international student from South Korea and a senior of University of Kansas. Lucas expects to graduate in May 2024 and plans to attend graduate school, after he serves his military duty. Lucas’ research interests include how personality traits of Big 5 model causes different reaction and symptom severity of mood disorder, how to minimize adolescents’ stress and risky behaviors under highly stressful/vulnerable condition."
  },
  {
    "objectID": "people/ras/kang_taegyu.html#semesters",
    "href": "people/ras/kang_taegyu.html#semesters",
    "title": "Tae Gyu Kang",
    "section": "Semesters",
    "text": "Semesters\n\nSpring 2023, Research Assistant\nFall 2023, Research Assistant"
  },
  {
    "objectID": "people/ras/kang_taegyu.html#education",
    "href": "people/ras/kang_taegyu.html#education",
    "title": "Tae Gyu Kang",
    "section": "Education",
    "text": "Education\n\nUniversity of Kansas | Lawrence, KS, USA Psychology Major"
  },
  {
    "objectID": "people/alumni/white_addison.html",
    "href": "people/alumni/white_addison.html",
    "title": "Addison White",
    "section": "",
    "text": "Home\n    People\n    Alumni\n    Addison White"
  },
  {
    "objectID": "people/alumni/white_addison.html#semesters",
    "href": "people/alumni/white_addison.html#semesters",
    "title": "Addison White",
    "section": "Semesters",
    "text": "Semesters\n\nFall 2020, Research Assistant\nSpring 2021, Research Assistant\nFall 2021, Research Assistant"
  },
  {
    "objectID": "people/alumni/white_addison.html#education",
    "href": "people/alumni/white_addison.html#education",
    "title": "Addison White",
    "section": "Education",
    "text": "Education\nUniversity of Kansas | Lawrence, KS, USA Psychology Major"
  },
  {
    "objectID": "people/alumni/rivera_miguel.html",
    "href": "people/alumni/rivera_miguel.html",
    "title": "Miguel Rivera Jr.",
    "section": "",
    "text": "Home\n    People\n    Alumni\n    Miguel Rivera Jr."
  },
  {
    "objectID": "people/alumni/rivera_miguel.html#semesters",
    "href": "people/alumni/rivera_miguel.html#semesters",
    "title": "Miguel Rivera Jr.",
    "section": "Semesters",
    "text": "Semesters\n\nSpring 2021, Research Assistant\nFall 2021, Honors Research Assistant\nSpring 2022, Honors Research Assistant"
  },
  {
    "objectID": "people/alumni/rivera_miguel.html#honors-thesis",
    "href": "people/alumni/rivera_miguel.html#honors-thesis",
    "title": "Miguel Rivera Jr.",
    "section": "Honors Thesis",
    "text": "Honors Thesis\n“Depression severity as indicated by vocal pitch, word choice, and response latency”"
  },
  {
    "objectID": "people/alumni/rivera_miguel.html#awards",
    "href": "people/alumni/rivera_miguel.html#awards",
    "title": "Miguel Rivera Jr.",
    "section": "Awards",
    "text": "Awards\n\nUndergraduate Research Award (2021), KU Center for Undergraduate Research"
  },
  {
    "objectID": "people/alumni/rivera_miguel.html#education",
    "href": "people/alumni/rivera_miguel.html#education",
    "title": "Miguel Rivera Jr.",
    "section": "Education",
    "text": "Education\nUniversity of Kansas | Lawrence, KS, USA Psychology Major"
  },
  {
    "objectID": "people/alumni/oberg_sara.html",
    "href": "people/alumni/oberg_sara.html",
    "title": "Sara Oberg",
    "section": "",
    "text": "Home\n    People\n    Alumni\n    Sara Oberg"
  },
  {
    "objectID": "people/alumni/oberg_sara.html#semesters",
    "href": "people/alumni/oberg_sara.html#semesters",
    "title": "Sara Oberg",
    "section": "Semesters",
    "text": "Semesters\n\nFall 2020, Research Assistant\nSpring 2021, Research Assistant"
  },
  {
    "objectID": "people/alumni/oberg_sara.html#education",
    "href": "people/alumni/oberg_sara.html#education",
    "title": "Sara Oberg",
    "section": "Education",
    "text": "Education\nUniversity of Kansas | Lawrence, KS, USA Psychology Major"
  },
  {
    "objectID": "people/alumni/logan_laura.html",
    "href": "people/alumni/logan_laura.html",
    "title": "Laura Logan",
    "section": "",
    "text": "Home\n    People\n    Alumni\n    Laura Logan"
  },
  {
    "objectID": "people/alumni/logan_laura.html#semesters",
    "href": "people/alumni/logan_laura.html#semesters",
    "title": "Laura Logan",
    "section": "Semesters",
    "text": "Semesters\n\nFall 2020, Research Assistant\nSpring 2021, Research Assistant\nFall 2021, Honors Research Assistant\nSpring 2022, Honors Research Assistant"
  },
  {
    "objectID": "people/alumni/logan_laura.html#honors-thesis",
    "href": "people/alumni/logan_laura.html#honors-thesis",
    "title": "Laura Logan",
    "section": "Honors Thesis",
    "text": "Honors Thesis\n“Examining the Accuracy of Communicative Modalities of Depression”"
  },
  {
    "objectID": "people/alumni/logan_laura.html#education",
    "href": "people/alumni/logan_laura.html#education",
    "title": "Laura Logan",
    "section": "Education",
    "text": "Education\nUniversity of Kansas | Lawrence, KS, USA Psychology Major"
  },
  {
    "objectID": "people/alumni/liu_yutian.html",
    "href": "people/alumni/liu_yutian.html",
    "title": "Yutian Liu",
    "section": "",
    "text": "Home\n    People\n    Alumni\n    Yutian Liu"
  },
  {
    "objectID": "people/alumni/liu_yutian.html#semesters",
    "href": "people/alumni/liu_yutian.html#semesters",
    "title": "Yutian Liu",
    "section": "Semesters",
    "text": "Semesters\n\nFall 2021, Research Assistant\nSpring 2022, Research Assistant"
  },
  {
    "objectID": "people/alumni/liu_yutian.html#education",
    "href": "people/alumni/liu_yutian.html#education",
    "title": "Yutian Liu",
    "section": "Education",
    "text": "Education\nUniversity of Kansas | Lawrence, KS, USA Behavioral Neuroscience Major"
  },
  {
    "objectID": "people/alumni/iskalis_taylor.html",
    "href": "people/alumni/iskalis_taylor.html",
    "title": "Taylor Iskalis",
    "section": "",
    "text": "Home\n    People\n    Alumni\n    Taylor Iskalis"
  },
  {
    "objectID": "people/alumni/iskalis_taylor.html#semesters",
    "href": "people/alumni/iskalis_taylor.html#semesters",
    "title": "Taylor Iskalis",
    "section": "Semesters",
    "text": "Semesters\n\nFall 2020, Research Assistant\nSpring 2021, Research Assistant\nFall 2021, Research Assistant\nSpring 2022, Research Assistant"
  },
  {
    "objectID": "people/alumni/iskalis_taylor.html#conference-presentations",
    "href": "people/alumni/iskalis_taylor.html#conference-presentations",
    "title": "Taylor Iskalis",
    "section": "Conference Presentations",
    "text": "Conference Presentations\n\n“Interpersonal Functioning in Patients with PTSD at Treatment Intake” Jeffrey M. Girard, Keilan Johnson, Taylor Iskalis, & Joel Sprunger 2022 Annual Meeting of the Society for Interpersonal Theory and Research"
  },
  {
    "objectID": "people/alumni/iskalis_taylor.html#education",
    "href": "people/alumni/iskalis_taylor.html#education",
    "title": "Taylor Iskalis",
    "section": "Education",
    "text": "Education\nUniversity of Kansas | Lawrence, KS, USA Psychology Major"
  },
  {
    "objectID": "people/alumni/dankof_kathleen.html",
    "href": "people/alumni/dankof_kathleen.html",
    "title": "Kathleen Dankof",
    "section": "",
    "text": "Home\n    People\n    Alumni\n    Kathleen Dankof"
  },
  {
    "objectID": "people/alumni/dankof_kathleen.html#education",
    "href": "people/alumni/dankof_kathleen.html#education",
    "title": "Kathleen Dankof",
    "section": "Education",
    "text": "Education\nUniversity of Kansas | Lawrence, KS, USA Psychology Major"
  },
  {
    "objectID": "people/alumni/bachmuth_rebecca.html",
    "href": "people/alumni/bachmuth_rebecca.html",
    "title": "Rebecca Bachmuth",
    "section": "",
    "text": "Home\n    People\n    Alumni\n    Rebecca Bachmuth"
  },
  {
    "objectID": "people/alumni/bachmuth_rebecca.html#semesters",
    "href": "people/alumni/bachmuth_rebecca.html#semesters",
    "title": "Rebecca Bachmuth",
    "section": "Semesters",
    "text": "Semesters\n\nSpring 2021, Research Assistant"
  },
  {
    "objectID": "people/alumni/bachmuth_rebecca.html#education",
    "href": "people/alumni/bachmuth_rebecca.html#education",
    "title": "Rebecca Bachmuth",
    "section": "Education",
    "text": "Education\nUniversity of Kansas | Lawrence, KS, USA Biochemistry Major"
  },
  {
    "objectID": "people/alumni/aleesa_abdulhamed.html",
    "href": "people/alumni/aleesa_abdulhamed.html",
    "title": "Abdulhamed Aleesa",
    "section": "",
    "text": "Home\n    People\n    Research Assistants\n    Abdulhamed Aleesa"
  },
  {
    "objectID": "people/alumni/aleesa_abdulhamed.html#biography",
    "href": "people/alumni/aleesa_abdulhamed.html#biography",
    "title": "Abdulhamed Aleesa",
    "section": "Biography",
    "text": "Biography\nAbdulhamed Aleesa is an international student from Kuwait and a senior at the University of Kansas. He expects to graduate in the spring of 2023 with a B.A. in psychology and sociology. Furthermore, he intends to pursue higher studies in either clinical or counseling psychology.\nResearch interests include: Mental health, counseling, and different clinical and counseling intervention methods. Patterns of thinking and maladaptive schemas, assessing the psychological functioning of individuals during depressive episodes and recovery periods. What exactly leads to depressive thoughts and thinking, and how do escape such episodes. How do depressed individuals cope with varying levels of depression while being on or off medicine? The role of personality type, personality disorder, and family upbringing in shaping one’s worldview and ways of dealing with stress, stressors, and negative emotions. Is all the theoretical side of psychology bad/hard to measure nowadays? How much of Freudian and Jungian theory still holds today? How valid and reliable is it to combine theoretical and clinically proven techniques together? How big is the room for improvisation in clinical settings?\nFinally, can a healthy lifestyle with a supporting circle of friends and loved ones and dedication to a hobby or holding a job that one is passionate about being able to supplant the role of medication? What if one is diagnosed with clinical depression and is unable to function while being medicated? What is the best way to lessen the symptoms of such an individual?"
  },
  {
    "objectID": "people/alumni/aleesa_abdulhamed.html#semesters",
    "href": "people/alumni/aleesa_abdulhamed.html#semesters",
    "title": "Abdulhamed Aleesa",
    "section": "Semesters",
    "text": "Semesters\n\nFall 2022, Research Assistant\nSpring 2023, Research Assistant"
  },
  {
    "objectID": "people/alumni/aleesa_abdulhamed.html#education",
    "href": "people/alumni/aleesa_abdulhamed.html#education",
    "title": "Abdulhamed Aleesa",
    "section": "Education",
    "text": "Education\n\nUniversity of Kansas | Lawrence, KS, USA Psychology and Sociology Major"
  },
  {
    "objectID": "news.html",
    "href": "news.html",
    "title": "Motion Metrics",
    "section": "",
    "text": "Home\n    News\n  \n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nGPSP 2023 Convention\n\n\n\n\n\n\n\nannouncement\n\n\nconference\n\n\n\n\nGreat Plains Student Psychology Convention\n\n\n\n\n\n\nApr 11, 2023\n\n\nJeffrey Girard\n\n\n\n\n\n\n  \n\n\n\n\nRow-wise means in dplyr\n\n\n\n\n\n\n\nteaching\n\n\ndata science\n\n\n\n\nGuide to calculating mean scores in dplyr\n\n\n\n\n\n\nMar 18, 2023\n\n\nJeffrey Girard\n\n\n\n\n\n\n  \n\n\n\n\nAdventures in Moderation\n\n\n\n\n\n\n\nteaching\n\n\npresentation\n\n\n\n\nProbing Interactions in ANOVA and Regression\n\n\n\n\n\n\nNov 18, 2022\n\n\nJeffrey Girard\n\n\n\n\n\n\n  \n\n\n\n\nICMI 2022 Paper\n\n\n\n\n\n\n\nannouncement\n\n\npaper\n\n\n\n\nToward causal understanding of therapist-client relationships\n\n\n\n\n\n\nNov 7, 2022\n\n\nJeffrey Girard\n\n\n\n\n\n\n  \n\n\n\n\nGoogle Research Grant\n\n\n\n\n\n\n\nannouncement\n\n\nfunding\n\n\n\n\nNovel Scalable Mental Health Screening Procedures on Ubiquitous Sensing Devices\n\n\n\n\n\n\nOct 28, 2022\n\n\nJeffrey Girard\n\n\n\n\n\n\n  \n\n\n\n\nNIMH R21 Grant\n\n\n\n\n\n\n\nannouncement\n\n\nfunding\n\n\n\n\nMultimodal Dynamics of Parent-Child Interactions and Suicide Risk\n\n\n\n\n\n\nSep 8, 2022\n\n\nJeffrey Girard\n\n\n\n\n\n\n  \n\n\n\n\nRecruitment 2023–2024\n\n\n\n\n\n\n\nannouncement\n\n\nrecruitment\n\n\n\n\nInformation for applicants to the psychology graduate program to start in Fall 2023\n\n\n\n\n\n\nAug 30, 2022\n\n\nJeffrey Girard\n\n\n\n\n\n\n  \n\n\n\n\nNIMH R01 Grant\n\n\n\n\n\n\n\nannouncement\n\n\nfunding\n\n\n\n\nContext-Adaptive Multimodal Informatics for Psychiatric Discharge Planning\n\n\n\n\n\n\nApr 15, 2021\n\n\nJeffrey Girard\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "girard/index.html",
    "href": "girard/index.html",
    "title": "Girard Redirect",
    "section": "",
    "text": "Please follow this link."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Motion Metrics",
    "section": "",
    "text": "Home\n    Contact"
  },
  {
    "objectID": "contact.html#visit-us",
    "href": "contact.html#visit-us",
    "title": "Motion Metrics",
    "section": "Visit Us",
    "text": "Visit Us\nMotion Metrics is located in Kiel."
  },
  {
    "objectID": "contact.html#email-us",
    "href": "contact.html#email-us",
    "title": "Motion Metrics",
    "section": "Email Us",
    "text": "Email Us"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Motion Metrics",
    "section": "",
    "text": "At Motion Metric, we believe that precise and accurate movement analysis is the key to unlocking improved outcomes for patients and athletes alike. Our mission is to provide the highest quality movement analysis services, with a focus on gait analysis and sports performance analysis, to help our clients achieve their goals and optimize their performance.\n\n\nOur Goals\nAt Motion Metric, our goal is to be the leading provider of movement analysis services. We strive to deliver the highest quality services to our clients, with a focus on accuracy, precision, and customization. We are committed to staying at the forefront of our field, and we invest in ongoing research and development to ensure that we are always using the latest techniques and technologies to deliver the best possible outcomes for our clients."
  },
  {
    "objectID": "opportunities.html",
    "href": "opportunities.html",
    "title": "Motion Metrics",
    "section": "",
    "text": "Home\n    Opportunities"
  },
  {
    "objectID": "opportunities.html#for-undergraduate-students",
    "href": "opportunities.html#for-undergraduate-students",
    "title": "Motion Metrics",
    "section": "For Undergraduate Students",
    "text": "For Undergraduate Students\nThe lab is frequently home to undergraduate students from the Psychology and Behavioral Neuroscience departments (and occasionally others).\nUndergraduate students work as research assistants, learning about the scientific method and assisting in the collection and processing of research data. Many enroll in PSYC 480 and receive credit hours for this work. Research assistants attend weekly lab meetings and can expect a letter of recommendation from Dr. Girard describing their work in and contributions to the lab (please give one month of advanced notice). Opportunities to earn co-authorship on research products (e.g., conference presentations) are also occasionally offered to advanced research assistants.\nHonors students will also occasionally design and complete an independent research project (i.e., honors thesis) under the supervision of Dr. Girard over the course of their senior year. These students typically apply for (and have often won) research awards of $1000 from the Center for Undergraduate Research and make oral or poster presentations about their work at the Undergraduate Research Symposium.\nApply to be a Research Assistant"
  },
  {
    "objectID": "opportunities.html#for-prospective-graduate-students",
    "href": "opportunities.html#for-prospective-graduate-students",
    "title": "Motion Metrics",
    "section": "For Prospective Graduate Students",
    "text": "For Prospective Graduate Students\nThe KU Psychology Department grants doctoral degrees (i.e., PhDs) in three tracks: Brain, Behavior, and Quantitative Science (BBQ); Clinical Psychological Science (CPS); and Social Psychology; note that the department does not offer a terminal Masters degree.\nDr. Girard advises graduate students in the BBQ and CPS tracks and is a particularly good match for students with interests that span both areas (e.g., CPS students interested in advanced methods or BBQ students interested in medical applications).\nGraduate students will complete coursework in the department and conduct research in the lab. They will receive a world-class education in the scientific method and have many opportunities to contribute to ongoing research projects.\nMore information about each admissions cycle can be found on the News page.\nApply to the BBQ Program\nApply to the CPS Program"
  },
  {
    "objectID": "opportunities.html#for-prospective-postdocs-and-staff",
    "href": "opportunities.html#for-prospective-postdocs-and-staff",
    "title": "Motion Metrics",
    "section": "For Prospective Postdocs and Staff",
    "text": "For Prospective Postdocs and Staff\nPostdoctoral fellowships and other staff positions in the lab will be made available as funding permits. Please check the News page for more information."
  },
  {
    "objectID": "people/alumni/amgalanbaatar_dulguun.html",
    "href": "people/alumni/amgalanbaatar_dulguun.html",
    "title": "Dulguun Amgalanbaatar",
    "section": "",
    "text": "Home\n    People\n    Alumni\n    Dulguun Amgalanbaatar"
  },
  {
    "objectID": "people/alumni/amgalanbaatar_dulguun.html#biography",
    "href": "people/alumni/amgalanbaatar_dulguun.html#biography",
    "title": "Dulguun Amgalanbaatar",
    "section": "Biography",
    "text": "Biography\nDulguun Amgalanbaatar is a senior at the University of Kansas majoring in behavioral neuroscience BS. She will graduate in December 2022 and plans to study clinical psychology in graduate school.\nDulguun’s research interests include behavioral and cognitive issues associated with Autism Spectrum Disorders, verbal and nonverbal behavior presentations of PTSD and depression as well as relationship between interpersonal communication problems and these conditions."
  },
  {
    "objectID": "people/alumni/amgalanbaatar_dulguun.html#semesters",
    "href": "people/alumni/amgalanbaatar_dulguun.html#semesters",
    "title": "Dulguun Amgalanbaatar",
    "section": "Semesters",
    "text": "Semesters\n\nFall 2022, Research Assistant"
  },
  {
    "objectID": "people/alumni/amgalanbaatar_dulguun.html#education",
    "href": "people/alumni/amgalanbaatar_dulguun.html#education",
    "title": "Dulguun Amgalanbaatar",
    "section": "Education",
    "text": "Education\n\nUniversity of Kansas | Lawrence, KS, USA Behavioral Neuroscience Major"
  },
  {
    "objectID": "people/alumni/bayyapu_mahitha.html",
    "href": "people/alumni/bayyapu_mahitha.html",
    "title": "Mahitha Reddy Bayyapu",
    "section": "",
    "text": "Home\n    People\n    Research Staff\n    Mahitha Reddy Bayyapu"
  },
  {
    "objectID": "people/alumni/bayyapu_mahitha.html#biography",
    "href": "people/alumni/bayyapu_mahitha.html#biography",
    "title": "Mahitha Reddy Bayyapu",
    "section": "Biography",
    "text": "Biography\nI am an enthusiastic software developer currently pursuing Master’s in Computer and Information Science. I enjoy problem solving using programming. Along with software development, my current interests are also in the field of data science and machine learning. I want to embrace the powers of these technologies and create useful applications. My other interests include painting and drawing."
  },
  {
    "objectID": "people/alumni/bayyapu_mahitha.html#education",
    "href": "people/alumni/bayyapu_mahitha.html#education",
    "title": "Mahitha Reddy Bayyapu",
    "section": "Education",
    "text": "Education\n\nUniversity of Kansas | Lawrence, KS, USA MS Student (Computer and Information Science), 2022–Current\nGandhi Institute of Technology and Science | Hyperabad, India BT in Electronics and Communications Engineering, 2016–2020"
  },
  {
    "objectID": "people/alumni/bayyapu_mahitha.html#work-experience",
    "href": "people/alumni/bayyapu_mahitha.html#work-experience",
    "title": "Mahitha Reddy Bayyapu",
    "section": "Work Experience",
    "text": "Work Experience\n\nTata Consultancy Services | Hyperabad, India Android Application Developer, 2020–2022"
  },
  {
    "objectID": "people/alumni/dick_aubrey.html",
    "href": "people/alumni/dick_aubrey.html",
    "title": "Aubrey Dick",
    "section": "",
    "text": "Home\n    People\n    Alumni\n    Aubrey Dick"
  },
  {
    "objectID": "people/alumni/dick_aubrey.html#semesters",
    "href": "people/alumni/dick_aubrey.html#semesters",
    "title": "Aubrey Dick",
    "section": "Semesters",
    "text": "Semesters\n\nSpring 2021, Research Assistant\nFall 2021, Honors Research Assistant\nSpring 2022, Honors Research Assistant"
  },
  {
    "objectID": "people/alumni/dick_aubrey.html#honors-thesis",
    "href": "people/alumni/dick_aubrey.html#honors-thesis",
    "title": "Aubrey Dick",
    "section": "Honors Thesis",
    "text": "Honors Thesis\n“Emotion Regulation: The development of CBT into DBT and their effectiveness”"
  },
  {
    "objectID": "people/alumni/dick_aubrey.html#education",
    "href": "people/alumni/dick_aubrey.html#education",
    "title": "Aubrey Dick",
    "section": "Education",
    "text": "Education\nUniversity of Kansas | Lawrence, KS, USA Behavioral Neuroscience Major"
  },
  {
    "objectID": "people/alumni/johnson_emma.html",
    "href": "people/alumni/johnson_emma.html",
    "title": "Emma Johnson",
    "section": "",
    "text": "Home\n    People\n    Alumni\n    Emma Johnson"
  },
  {
    "objectID": "people/alumni/johnson_emma.html#semesters",
    "href": "people/alumni/johnson_emma.html#semesters",
    "title": "Emma Johnson",
    "section": "Semesters",
    "text": "Semesters\n\nFall 2020, Research Assistant\nSpring 2021, Research Assistant\nFall 2021, Research Assistant\nSpring 2022, Research Assistant"
  },
  {
    "objectID": "people/alumni/johnson_emma.html#poster-presentations",
    "href": "people/alumni/johnson_emma.html#poster-presentations",
    "title": "Emma Johnson",
    "section": "Poster Presentations",
    "text": "Poster Presentations\n\n“Predicting working alliance during depression treatment from personality and interpersonal style” Emma N. Johnson, Brendon T. Elliott, Jeffrey M. Girard, Lauren Bylsma, Jeffrey Cohn, Jay Fournier, Louis-Philippe Morency, & Holly Swartz 2021 Annual Meeting of the Society for Interpersonal Theory and Research"
  },
  {
    "objectID": "people/alumni/johnson_emma.html#education",
    "href": "people/alumni/johnson_emma.html#education",
    "title": "Emma Johnson",
    "section": "Education",
    "text": "Education\nUniversity of Kansas | Lawrence, KS, USA Behavioral Neuroscience Major"
  },
  {
    "objectID": "people/alumni/li_wanheng.html",
    "href": "people/alumni/li_wanheng.html",
    "title": "Wanheng Li",
    "section": "",
    "text": "Home\n    People\n    Alumni\n    Wanheng Li"
  },
  {
    "objectID": "people/alumni/li_wanheng.html#biography",
    "href": "people/alumni/li_wanheng.html#biography",
    "title": "Wanheng Li",
    "section": "Biography",
    "text": "Biography\nWanheng Li graduated from KU in May 2022 with a double major in behavioral neuroscience and East Asian cultures and languages. She plans to study data science in graduate school.\nHer research interests include interpersonal relationships, nonverbal behavior, and research methodology."
  },
  {
    "objectID": "people/alumni/li_wanheng.html#honors-thesis",
    "href": "people/alumni/li_wanheng.html#honors-thesis",
    "title": "Wanheng Li",
    "section": "Honors Thesis",
    "text": "Honors Thesis\n“Evaluating Evidence on the Duchenne Smile and Felt Positive Emotion”"
  },
  {
    "objectID": "people/alumni/li_wanheng.html#awards",
    "href": "people/alumni/li_wanheng.html#awards",
    "title": "Wanheng Li",
    "section": "Awards",
    "text": "Awards\n\nUndergraduate Research Award (2021), KU Center for Undergraduate Research\nOutstanding Presentation Award (2022), KU Center for Undergraduate Research"
  },
  {
    "objectID": "people/alumni/li_wanheng.html#semesters",
    "href": "people/alumni/li_wanheng.html#semesters",
    "title": "Wanheng Li",
    "section": "Semesters",
    "text": "Semesters\n\nSpring 2021, Research Assistant\nFall 2021, Honors Student\nSpring 2022, Honors Student\nFall 2022, Research Assistant"
  },
  {
    "objectID": "people/alumni/li_wanheng.html#education",
    "href": "people/alumni/li_wanheng.html#education",
    "title": "Wanheng Li",
    "section": "Education",
    "text": "Education\nUniversity of Kansas | Lawrence, KS, USA Behavioral Neuroscience and East Asian Languages and Cultures Major"
  },
  {
    "objectID": "people/alumni/niehorster-cook_leo.html",
    "href": "people/alumni/niehorster-cook_leo.html",
    "title": "Leo Niehorster-Cook",
    "section": "",
    "text": "Home\n    People\n    Alumni\n    Leo Niehorster-Cook"
  },
  {
    "objectID": "people/alumni/niehorster-cook_leo.html#semesters",
    "href": "people/alumni/niehorster-cook_leo.html#semesters",
    "title": "Leo Niehorster-Cook",
    "section": "Semesters",
    "text": "Semesters\n\nFall 2021, Research Assistant"
  },
  {
    "objectID": "people/alumni/niehorster-cook_leo.html#education",
    "href": "people/alumni/niehorster-cook_leo.html#education",
    "title": "Leo Niehorster-Cook",
    "section": "Education",
    "text": "Education\nUniversity of Kansas | Lawrence, KS, USA Psychology, Philosophy, and Linguistics Major"
  },
  {
    "objectID": "people/alumni/patel_heeral.html",
    "href": "people/alumni/patel_heeral.html",
    "title": "Heeral Patel",
    "section": "",
    "text": "Home\n    People\n    Research Assistants\n    Heeral Patel"
  },
  {
    "objectID": "people/alumni/patel_heeral.html#biography",
    "href": "people/alumni/patel_heeral.html#biography",
    "title": "Heeral Patel",
    "section": "Biography",
    "text": "Biography\nHeeral Patel is a senior at the University of Kansas double-majoring in psychology and molecular, cellular, and developmental biology, on the pre-med track. She will graduate in May 2023 and plans to attend medical school the following year.\nHeeral’s research interests include how psychology affects interpersonal communication, clinical applications of nonverbal behavior research, and adolescent psychopathology."
  },
  {
    "objectID": "people/alumni/patel_heeral.html#semesters",
    "href": "people/alumni/patel_heeral.html#semesters",
    "title": "Heeral Patel",
    "section": "Semesters",
    "text": "Semesters\n\nFall 2021, Research Assistant\nSpring 2022, Research Assistant\nFall 2022, Research Assistant\nSpring 2023, Research Assistant"
  },
  {
    "objectID": "people/alumni/patel_heeral.html#education",
    "href": "people/alumni/patel_heeral.html#education",
    "title": "Heeral Patel",
    "section": "Education",
    "text": "Education\n\nUniversity of Kansas | Lawrence, KS, USA Psychology and Molecular, Cellular, and Developmental Biology Major"
  },
  {
    "objectID": "people/alumni/sall_lauryn.html",
    "href": "people/alumni/sall_lauryn.html",
    "title": "Lauryn Sall",
    "section": "",
    "text": "Home\n    People\n    Alumni\n    Lauryn Sall"
  },
  {
    "objectID": "people/alumni/sall_lauryn.html#semesters",
    "href": "people/alumni/sall_lauryn.html#semesters",
    "title": "Lauryn Sall",
    "section": "Semesters",
    "text": "Semesters\n\nSpring 2021, Research Assistant\nFall 2021, Research Assistant"
  },
  {
    "objectID": "people/alumni/sall_lauryn.html#education",
    "href": "people/alumni/sall_lauryn.html#education",
    "title": "Lauryn Sall",
    "section": "Education",
    "text": "Education\nUniversity of Kansas | Lawrence, KS, USA Behavioral Neuroscience Major"
  },
  {
    "objectID": "people/ras/folsom_mallory.html",
    "href": "people/ras/folsom_mallory.html",
    "title": "Mallory Folsom",
    "section": "",
    "text": "Home\n    People\n    Research Assistants\n    Mallory Folsom"
  },
  {
    "objectID": "people/ras/folsom_mallory.html#biography",
    "href": "people/ras/folsom_mallory.html#biography",
    "title": "Mallory Folsom",
    "section": "Biography",
    "text": "Biography\nMallory Folsom is a senior at the University of Kansas majoring in Behavioral Neuroscience, minoring in business, obtaining a service learning certificate and in the university honors program. Upon graduating in May of 2023, Mallory will take a gap year and complete a graduate certificate in Autism Spectrum Disorders while continuing to work as a Registered Behavior Technician. Following this, Mallory will attend medical school and endeavors to become a pediatric psychiatrist.\nMallory’s research interests span all realms of mental health. She is especially interested in the genetic inheritance patterns of developmental disorders like Autism and ADHD. Additionally, Mallory is interested in understanding and cultivating more precise and less invasive pharmacological approaches to improve treatment for pediatric psychiatric patients."
  },
  {
    "objectID": "people/ras/folsom_mallory.html#semesters",
    "href": "people/ras/folsom_mallory.html#semesters",
    "title": "Mallory Folsom",
    "section": "Semesters",
    "text": "Semesters\n\nFall 2020, Research Assistant\nSpring 2021, Research Assistant\nFall 2021, Research Assistant\nSpring 2022, Research Assistant\nFall 2022, Research Assistant\nSpring 2023, Research Assistant"
  },
  {
    "objectID": "people/ras/folsom_mallory.html#education",
    "href": "people/ras/folsom_mallory.html#education",
    "title": "Mallory Folsom",
    "section": "Education",
    "text": "Education\nUniversity of Kansas | Lawrence, KS, USA Behavioral Neuroscience Major"
  },
  {
    "objectID": "people/ras/swift_anna.html",
    "href": "people/ras/swift_anna.html",
    "title": "Anna Swift",
    "section": "",
    "text": "Home\n    People\n    Research Assistants\n    Anna Swift"
  },
  {
    "objectID": "people/ras/swift_anna.html#biography",
    "href": "people/ras/swift_anna.html#biography",
    "title": "Anna Swift",
    "section": "Biography",
    "text": "Biography\nAnna Swift is a senior studying Behavioral Neuroscience and completing certificates in Data Science and Mind & Brain. She will be graduating in the fall of 2023.\nAnna’s research interests include data modeling and analysis, as well as clinical rehabilitation."
  },
  {
    "objectID": "people/ras/swift_anna.html#semesters",
    "href": "people/ras/swift_anna.html#semesters",
    "title": "Anna Swift",
    "section": "Semesters",
    "text": "Semesters\n\nFall 2022, Research Assistant\nSpring 2023, Research Assistant\nFall 2023, Research Assistant"
  },
  {
    "objectID": "people/ras/swift_anna.html#education",
    "href": "people/ras/swift_anna.html#education",
    "title": "Anna Swift",
    "section": "Education",
    "text": "Education\n\nUniversity of Kansas | Lawrence, KS, USA Behavioral Neuroscience Major"
  },
  {
    "objectID": "people/staff/gray_kassandra.html",
    "href": "people/staff/gray_kassandra.html",
    "title": "Julius Welzel",
    "section": "",
    "text": "Home\n    People\n    Julius Welzel"
  },
  {
    "objectID": "people/staff/gray_kassandra.html#biography",
    "href": "people/staff/gray_kassandra.html#biography",
    "title": "Julius Welzel",
    "section": "Biography",
    "text": "Biography\nKassandra Gray’s research interests include interpersonal interactions, emotional expression and communication, individual differences, personality, anxiety, and depression. She is interested in how emotional expression and communication manifest in the therapeutic relationship and the influential role of individual differences (e.g., personality, anxiety, and depression).\nShe completed her undergraduate Bachelor of Science degree at Southern Nazarene University in May 2022 and is currently a doctoral student in the Clinical Psychological Science program in the Department of Psychology at the University of Kansas. She is a graduate teaching assistant (GTA) for undergraduate psychology courses and is also a member of the Affective Communication and Computing lab."
  },
  {
    "objectID": "people/staff/gray_kassandra.html#education",
    "href": "people/staff/gray_kassandra.html#education",
    "title": "Julius Welzel",
    "section": "Education",
    "text": "Education\n\nUniversity of Kansas | Lawrence, KS, USA PhD Student (Clinical Psychological Science), 2022–Current\nSouthern Nazarene University | Bethany, OK, USA BS in Psychology | 2018–2022"
  },
  {
    "objectID": "people/staff/yang_yuanyuan.html",
    "href": "people/staff/yang_yuanyuan.html",
    "title": "Yuanyuan Yang",
    "section": "",
    "text": "Home\n    People\n    Research Staff\n    Yuanyuan Yang"
  },
  {
    "objectID": "people/staff/yang_yuanyuan.html#biography",
    "href": "people/staff/yang_yuanyuan.html#biography",
    "title": "Yuanyuan Yang",
    "section": "Biography",
    "text": "Biography\nTBA"
  },
  {
    "objectID": "people/staff/yang_yuanyuan.html#education",
    "href": "people/staff/yang_yuanyuan.html#education",
    "title": "Yuanyuan Yang",
    "section": "Education",
    "text": "Education\n\nUniversity of Kansas | Lawrence, KS, USA PhD Student (Clinical Psychological Science), 2023–Current\nSUNY University at Stony Brook | Stony Brook, NY, USA MS in Psychology | 2023\nSUNY University at Stony Brook | Stony Brook, NY, USA BS in Psychology, Applied Mathematics & Statistics | 2021"
  },
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "Motion Metrics",
    "section": "",
    "text": "Home\n    People"
  },
  {
    "objectID": "people.html#research-staff",
    "href": "people.html#research-staff",
    "title": "Motion Metrics",
    "section": "Research Staff",
    "text": "Research Staff\n\n\n\n\n\n\n\n\n\n\nClint Hansen\n\n\nCEO\n\n\n\n\n\n\n\n\n\n\n\n\n\nRobbin Romijnders\n\n\nAlgorithm Engineer\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulius Welzel\n\n\nData Wizard\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people.html#research-assistants",
    "href": "people.html#research-assistants",
    "title": "AffCom Lab",
    "section": "Research Assistants",
    "text": "Research Assistants\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people.html#alumni",
    "href": "people.html#alumni",
    "title": "AffCom Lab",
    "section": "Alumni",
    "text": "Alumni\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Name\n        \n         \n          Role\n        \n         \n          Started\n        \n         \n          Ended\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nName\n\n\nRole\n\n\nStarted\n\n\nEnded\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/google2022/index.html",
    "href": "posts/google2022/index.html",
    "title": "Google Research Grant",
    "section": "",
    "text": "I am very pleased to announce that the AffCom Lab has been granted a $30,000 gift from Google LLC to support our work on novel, scalable mental health screening procedures! Specifically, we plan to use these funds to further integrate computer vision, natural language processing, ambulatory assessment, and passive sensing into new screening procedures for affective disorders. This is also an opportunity to deepen the lab’s collaborative ties with industry leaders in digital mental health. Stay tuned for more information in the next few months about a new project we plan to launch in this space!"
  },
  {
    "objectID": "posts/moderation2022/index.html",
    "href": "posts/moderation2022/index.html",
    "title": "Adventures in Moderation",
    "section": "",
    "text": "I was invited to give a 75 minute presentation on testing, visualizing, and probing interactions in ANOVA and regression models at the Social Psychology Professional Seminar at the University of Kansas. Here are my slides:\n\n\nNote. To view the slideshow in full screen mode, click inside it and press the “f” key on your keyboard. You can also use the menu by clicking on the icon at the bottom-left of each slide."
  },
  {
    "objectID": "posts/recruitment2023/index.html",
    "href": "posts/recruitment2023/index.html",
    "title": "Recruitment 2023–2024",
    "section": "",
    "text": "Are you accepting applications for new graduate students?\nYes! I am planning to review applications for graduate students to start in Fall 2023 and may admit one or two Psychology PhD students across the Clinical Psychological Science (CPS) and Brain, Behavior, and Quantitative Science (BBQ) programs. The application deadline for both programs is December 1, 2022.\n\n\nWhat does your lab study?\nThe AffCom lab develops and uses technology to advance scientific understanding of how emotional, social, and mental health-related information is communicated through verbal and nonverbal behavior. Our work is eclectic and spans basic science on emotion and behavior; clinical science on assessment and treatment; and methodological science on the collection, processing, analysis, and visualization of research data.\n\n\nWhat are your research plans for the near future?\nIn the next few years, I plan to study (1) how therapists and patients with depression communicate during psychotherapy sessions, (2) how individuals with and without a history of trauma communicate their experiences and construct narratives, (3) how psychotic symptoms affect the production of communicative behavior, and (4) how technology can be used to more precisely measure emotional expressiveness in both healthy and clinical populations. I am also planning to work on software related to inter-rater agreement, hierarchical latent variable modeling, and web-based video annotation. See the research page for more information.\n\n\nShould I apply to work with you at KU?\nI welcome applications from all qualified students regardless of background. However, admissions are competitive, and applications are not free; thus, I want to give applicants a better idea of what I am looking for so that they can make an informed decision about whether to invest their time and money.\nFirst, note that I currently only advise PhD students in the CPS and BBQ programs. The psychology department does not offer a terminal Masters degree and I am not affiliated with the Child Clinical Psychology program. Second, note that my lab does not specialize in neuroscience or do much brain-focused research (i.e., we skew more towards the behavior and quantitative side of the BBQ program).\nThe primary criteria I use to evaluate applications are (1) evidence of the applicant’s ability to succeed in a research-focused PhD program, (2) a match between the applicant’s academic interests/experiences and my own areas of interest/expertise, and (3) a match between the applicant’s interpersonal style and my own.\nTo be competitive for admission, applicants should have (1) a strong academic record with a Bachelor’s or Master’s degree in psychology or a related field, (2) previous experience working/volunteering in one or more research labs, (3) a high level of maturity and professionalism conducive to collaborative and/or clinical work, and (4) a direct connection to my area of research such as through previous coursework, research experience, or independent study.\n\n\nWhat is the funding situation for graduate students?\nI would not take on a graduate student if I did not believe that I could provide them full funding (tuition remission, stipend, and benefits) for four to five years. Funding comes primarily from teaching assistantships, although research assistantships and scholarships are also possibilities. Supplemental summer funding is also often possible depending on our success in securing research grants.\n\n\nWhat is your stance on scientific reform?\nI am committed to making my science more transparent, open, reproducible, and accountable though the (1) sharing of research preprints, materials, and data, (2) transparent description of hypotheses and methods, and (3) use and development of free and open-source software.\n\n\nCan potential applicants meet with you before applying?\nDue to the high number of inquiries I receive each year, it is unfortunately not possible for me to meet with potential applicants. However, I will carefully review every application that lists my name and those applicants who are invited to interview will have the opportunity to speak with me one-on-one.\n\n\nCan potential applicants speak to students in the lab?\nSimilar to the question above, applicants who are invited to interview will have the opportunity to speak to graduate students and research assistants in the lab (as well as various other members of the department).\n\n\nHow do I apply?\nAdmission information and links can be found on the Psychology Graduate Program webpage. Scroll down and open the “Admission” accordion or click on one of the “Application Instructions” links. You will need to submit a curriculum vitae, statement of purpose, three letters of recommendations, and transcripts."
  },
  {
    "objectID": "posts/suicideR21/index.html",
    "href": "posts/suicideR21/index.html",
    "title": "NIMH R21 Grant",
    "section": "",
    "text": "I am very pleased to announce that we have been awarded an R21 (Exploratory/Developmental Research) Grant from the National Institute of Mental Health (R21-MH130767) to study parent-child communication dynamics and suicide risk. The project period is from 09/2022 through 07/2024 with a total funding of $278,341."
  },
  {
    "objectID": "posts/suicideR21/index.html#project-team",
    "href": "posts/suicideR21/index.html#project-team",
    "title": "NIMH R21 Grant",
    "section": "Project Team",
    "text": "Project Team\n\nMPI: Richard Liu, Massachusetts General Hospital\nMPI: Taylor Burke, Massachusetts General Hospital\nCo-I: Jeffrey Girard, University of Kansas\nCo-I: Louis-Philippe Morency, Carnegie Mellon University"
  },
  {
    "objectID": "posts/suicideR21/index.html#project-summary",
    "href": "posts/suicideR21/index.html#project-summary",
    "title": "NIMH R21 Grant",
    "section": "Project Summary",
    "text": "Project Summary\nSuicide continues to be a growing public health concern. Indeed, suicide rates have increased 33% since 1999. This concern has been particularly pronounced in the case of adolescents, as the suicide rates in this age group have tripled over the last 10 years. Clarifying potential processes of risk for suicidal behavior in this population therefore remains a pressing priority. Increasing theoretical and empirical focus has been devoted to the conceptualization of acute suicidal risk as being a period of elevated arousal. Stress within parent-child relationship dynamics may hold particular importance for understanding adolescents’ proximal risk for suicidal behavior. However, all past studies examining parent-child relationship stress and suicidal behavior have used self-report methodologies and/or retrospective recall. Although studies employing these methodologies provide important knowledge about the association between life stress and mental health outcomes, they are not designed to characterize the interpersonal dynamics of these stressors as they unfold over time, which may be particularly relevant to short-term suicide risk. Recent advances in computational approaches to automatic sensing of acoustic and visual behaviors hold promise to address the need for clarifying indices of arousal in parent-child dynamics associated with adolescent suicide risk. A novel method to assess familial stress processes is through the automated assessment of synchrony, or the behavioral matching, of arousal between adolescents and their parents. In healthy parent-child dyads, parents and adolescents are responsive to each other’s behavioral and emotional cues. However, during high stress or conflict, behavioral matching may be a marker of a high-risk interaction pattern inasmuch as it may be indicative of high arousal maintenance or escalation. Focusing on the RDoC constructs of arousal and social processes, the current R21 proposal aims to leverage recent developments in automated sensing of acoustic and visual behavior to characterize synchrony within the dynamics of a parent-child conflict task in a sample of psychiatrically hospitalized adolescents (n = 100) and their parents. The current study aims to evaluate whether acoustic and visual behavioral markers of arousal synchrony, respectively, are associated with prospective suicidal ideation three months post-discharge. We also aim to pool acoustic and visual behavioral markers of parent-child arousal synchrony through computational modeling to create a multimodal prediction model for prospective suicidal ideation, thereby to advance beyond unimodal analysis to multimodal analysis in classifying suicide risk. This R21 is intended as an initial step toward automating the assessment of parent-child arousal synchrony within clinical contexts to inform clinical decision-making and interventions. This proposal has both scientific and clinical significance because it is the first study to employ computational automation of acoustic and visual markers of suicide risk at the dyadic level and has the potential directly to inform adolescent suicide risk assessment and intervention."
  },
  {
    "objectID": "posts/suicideR21/index.html#public-health-relevance",
    "href": "posts/suicideR21/index.html#public-health-relevance",
    "title": "NIMH R21 Grant",
    "section": "Public Health Relevance",
    "text": "Public Health Relevance\nThis application aims to examine the dynamics of parent-child stress using automatic sensing of acoustic and visual behaviors as markers of suicide risk in adolescents. This proposal has both scientific and clinical significance because it is the first study to employ computational automation of acoustic and visual markers of suicide risk at the dyadic level. This research has the potential to improve our identification of stress-related risk in suicidal adolescents in periods of high risk and to directly inform clinical decision-making and intervention in the context of adolescents’ psychiatric hospitalization."
  },
  {
    "objectID": "publications/articles/bowdring2021.html",
    "href": "publications/articles/bowdring2021.html",
    "title": "In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions",
    "section": "",
    "text": "Bowdring, M. A., Sayette, M. A., Girard, J. M., & Woods, W. C. (2021). In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions. Journal of Nonverbal Behavior, 45(2), 241–259."
  },
  {
    "objectID": "publications/articles/bowdring2021.html#citation-apa-7",
    "href": "publications/articles/bowdring2021.html#citation-apa-7",
    "title": "In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions",
    "section": "",
    "text": "Bowdring, M. A., Sayette, M. A., Girard, J. M., & Woods, W. C. (2021). In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions. Journal of Nonverbal Behavior, 45(2), 241–259."
  },
  {
    "objectID": "publications/articles/bowdring2021.html#abstract",
    "href": "publications/articles/bowdring2021.html#abstract",
    "title": "In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions",
    "section": "Abstract",
    "text": "Abstract\nPhysical attractiveness plays a central role in psychosocial experiences. One of the top research priorities has been to identify factors affecting perceptions of physical attractiveness (PPA). Recent work suggests PPA derives from different sources (e.g., target, perceiver, stimulus type). Although smiles in particular are believed to enhance PPA, support has been surprisingly limited. This study comprehensively examines the effect of smiles on PPA and, more broadly, evaluates the roles of target, perceiver, and stimulus type in PPA variation. Perceivers (n=181) rated both static images and 5-s videos of targets displaying smiling and neutral-expressions. Smiling images were rated as more attractive than neutral-expression images (regardless of stimulus motion format). Interestingly, perceptions of physical attractiveness were based more on the perceiver than on either the target or format in which the target was presented. Results clarify the effect of smiles, and highlight the significant role of the perceiver, in PPA."
  },
  {
    "objectID": "publications/articles/gerber2019.html",
    "href": "publications/articles/gerber2019.html",
    "title": "Alexithymia – not autism – is associated with frequency of social interactions in adults",
    "section": "",
    "text": "Gerber, A. H., Girard, J. M., Scott, S. B., & Lerner, M. D. (2019). Alexithymia – Not autism – is associated with frequency of social interactions in adults. Behaviour Research and Therapy, 123, 103477."
  },
  {
    "objectID": "publications/articles/gerber2019.html#citation-apa-7",
    "href": "publications/articles/gerber2019.html#citation-apa-7",
    "title": "Alexithymia – not autism – is associated with frequency of social interactions in adults",
    "section": "",
    "text": "Gerber, A. H., Girard, J. M., Scott, S. B., & Lerner, M. D. (2019). Alexithymia – Not autism – is associated with frequency of social interactions in adults. Behaviour Research and Therapy, 123, 103477."
  },
  {
    "objectID": "publications/articles/gerber2019.html#abstract",
    "href": "publications/articles/gerber2019.html#abstract",
    "title": "Alexithymia – not autism – is associated with frequency of social interactions in adults",
    "section": "Abstract",
    "text": "Abstract\n\nObjective\nWhile much is known about the quality of social behavior among neurotypical individuals and those with autism spectrum disorder (ASD), little work has evaluated quantity of social interactions. This study used ecological momentary assessment (EMA) to quantify in vivo daily patterns of social interaction in adults as a function of demographic and clinical factors.\n\n\nMethod\nAdults with and without ASD (\\(N_{ASD}=23\\), \\(N_{NT}=52\\)) were trained in an EMA protocol to report their social interactions via smartphone over one week. Participants completed measures of IQ, ASD symptom severity and alexithymia symptom severity.\n\n\nResults\nCyclical multilevel models were used to account for nesting of observations. Results suggest a daily cyclical pattern of social interaction that was robust to ASD and alexithymia symptoms. Adults with ASD did not have fewer social interactions than neurotypical peers; however, severity of alexithymia symptoms predicted fewer social interactions regardless of ASD status.\n\n\nConclusions\nThese findings suggest that alexithymia, not ASD severity, may drive social isolation and highlight the need to reevaluate previously accepted notions regarding differences in social behavior utilizing modern methods."
  },
  {
    "objectID": "publications/articles/girard2014b.html",
    "href": "publications/articles/girard2014b.html",
    "title": "CARMA: Software for continuous affect rating and media annotation",
    "section": "",
    "text": "Girard, J. M. (2014). CARMA: Software for continuous affect rating and media annotation. Journal of Open Research Software, 2(1), e5–e5."
  },
  {
    "objectID": "publications/articles/girard2014b.html#citation-apa-7",
    "href": "publications/articles/girard2014b.html#citation-apa-7",
    "title": "CARMA: Software for continuous affect rating and media annotation",
    "section": "",
    "text": "Girard, J. M. (2014). CARMA: Software for continuous affect rating and media annotation. Journal of Open Research Software, 2(1), e5–e5."
  },
  {
    "objectID": "publications/articles/girard2014b.html#abstract",
    "href": "publications/articles/girard2014b.html#abstract",
    "title": "CARMA: Software for continuous affect rating and media annotation",
    "section": "Abstract",
    "text": "Abstract\nCARMA is a media annotation program that collects continuous ratings while displaying audio and video files. It is designed to be highly user-friendly and easily customizable. Based on Gottman and Levenson’s affect rating dial, CARMA enables researchers and study participants to provide moment-by-moment ratings of multimedia files using a computer mouse or keyboard. The rating scale can be configured on a number of parameters including the labels for its upper and lower bounds, its numerical range, and its visual representation. Annotations can be displayed alongside the multimedia file and saved for easy import into statistical analysis software. CARMA provides a tool for researchers in affective computing, human-computer interaction, and the social sciences who need to capture the unfolding of subjective experience and observable behavior over time."
  },
  {
    "objectID": "publications/articles/girard2015a.html",
    "href": "publications/articles/girard2015a.html",
    "title": "Spontaneous facial expression in unscripted social interactions can be measured automatically",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Jeni, L. A., Sayette, M. A., & De la Torre, F. (2015). Spontaneous facial expression in unscripted social interactions can be measured automatically. Behavior Research Methods, 47(4), 1136–1147."
  },
  {
    "objectID": "publications/articles/girard2015a.html#citation-apa-7",
    "href": "publications/articles/girard2015a.html#citation-apa-7",
    "title": "Spontaneous facial expression in unscripted social interactions can be measured automatically",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Jeni, L. A., Sayette, M. A., & De la Torre, F. (2015). Spontaneous facial expression in unscripted social interactions can be measured automatically. Behavior Research Methods, 47(4), 1136–1147."
  },
  {
    "objectID": "publications/articles/girard2015a.html#abstract",
    "href": "publications/articles/girard2015a.html#abstract",
    "title": "Spontaneous facial expression in unscripted social interactions can be measured automatically",
    "section": "Abstract",
    "text": "Abstract\nMethods to assess individual facial actions have potential to shed light on important behavioral phenomena ranging from emotion and social interaction to psychological disorders and health. However, manual coding of such actions is labor intensive and requires extensive training. To date, establishing reliable automated coding of unscripted facial actions has been a daunting challenge impeding development of psychological theories and applications requiring facial expression assessment. It is therefore essential that automated coding systems be developed with enough precision and robustness to ease the burden of manual coding in challenging data involving variation in participant gender, ethnicity, head pose, speech, and occlusion. We report a major advance in automated coding of spontaneous facial actions during an unscripted social interaction involving three strangers. For each participant (n = 80, 47 % women, 15 % Nonwhite), 25 facial action units (AUs) were manually coded from video using the Facial Action Coding System. Twelve AUs occurred more than 3 % of the time and were processed using automated FACS coding. Automated coding showed very strong reliability for the proportion of time that each AU occurred (mean intraclass correlation = 0.89), and the more stringent criterion of frame-by-frame reliability was moderate to strong (mean Matthew’s correlation = 0.61). With few exceptions, differences in AU detection related to gender, ethnicity, pose, and average pixel intensity were small. Fewer than 6 % of frames could be coded manually but not automatically. These findings suggest automated FACS coding has progressed sufficiently to be applied to observational research in emotion and related areas of study."
  },
  {
    "objectID": "publications/articles/girard2015c.html",
    "href": "publications/articles/girard2015c.html",
    "title": "Automated audiovisual depression analysis",
    "section": "",
    "text": "Girard, J. M., & Cohn, J. F. (2015). Automated audiovisual depression analysis. Current Opinion in Psychology, 4, 75–79."
  },
  {
    "objectID": "publications/articles/girard2015c.html#citation-apa-7",
    "href": "publications/articles/girard2015c.html#citation-apa-7",
    "title": "Automated audiovisual depression analysis",
    "section": "",
    "text": "Girard, J. M., & Cohn, J. F. (2015). Automated audiovisual depression analysis. Current Opinion in Psychology, 4, 75–79."
  },
  {
    "objectID": "publications/articles/girard2015c.html#abstract",
    "href": "publications/articles/girard2015c.html#abstract",
    "title": "Automated audiovisual depression analysis",
    "section": "Abstract",
    "text": "Abstract\nAnalysis of observable behavior in depression primarily relies on subjective measures. New computational approaches make possible automated audiovisual measurement of behaviors that humans struggle to quantify (e.g., movement velocity and voice inflection). These tools have the potential to improve screening and diagnosis, identify new behavioral indicators of depression, measure response to clinical intervention, and test clinical theories about underlying mechanisms. Highlights include a study that measured the temporal coordination of vocal tract and facial movements, a study that predicted which adolescents would go on to develop depression based on their voice qualities, and a study that tested the behavioral predictions of clinical theories using automated measures of facial actions and head motion."
  },
  {
    "objectID": "publications/articles/girard2017a.html",
    "href": "publications/articles/girard2017a.html",
    "title": "Interpersonal problems across levels of the psychopathology hierarchy",
    "section": "",
    "text": "Girard, J. M., Wright, A. G. C., Beeney, J. E., Lazarus, S. A., Scott, L. N., Stepp, S. D., & Pilkonis, P. A. (2017). Interpersonal problems across levels of the psychopathology hierarchy. Comprehensive Psychiatry, 79, 53–69."
  },
  {
    "objectID": "publications/articles/girard2017a.html#citation-apa-7",
    "href": "publications/articles/girard2017a.html#citation-apa-7",
    "title": "Interpersonal problems across levels of the psychopathology hierarchy",
    "section": "",
    "text": "Girard, J. M., Wright, A. G. C., Beeney, J. E., Lazarus, S. A., Scott, L. N., Stepp, S. D., & Pilkonis, P. A. (2017). Interpersonal problems across levels of the psychopathology hierarchy. Comprehensive Psychiatry, 79, 53–69."
  },
  {
    "objectID": "publications/articles/girard2017a.html#abstract",
    "href": "publications/articles/girard2017a.html#abstract",
    "title": "Interpersonal problems across levels of the psychopathology hierarchy",
    "section": "Abstract",
    "text": "Abstract\nWe examined the relationship between psychopathology and interpersonal problems in a sample of 825 clinical and community participants. Sixteen psychiatric diagnoses and five transdiagnostic dimensions were examined in relation to self-reported interpersonal problems. The structural summary method was used with the Inventory of Interpersonal Problems Circumplex Scales to examine interpersonal problem profiles for each diagnosis and dimension. We built a structural model of mental disorders including factors corresponding to detachment (avoidant personality, social phobia, major depression), internalizing (dependent personality, borderline personality, panic disorder, posttraumatic stress, major depression), disinhibition (antisocial personality, drug dependence, alcohol dependence, borderline personality), dominance (histrionic personality, narcissistic personality, paranoid personality), and compulsivity (obsessive-compulsive personality). All dimensions showed good interpersonal prototypicality (e.g., detachment was defined by a socially avoidant/nonassertive interpersonal profile) except for internalizing, which was diffusely associated with elevated interpersonal distress. The findings for individual disorders were largely consistent with the dimension that each disorder loaded on, with the exception of the internalizing and dominance disorders, which were interpersonally heterogeneous. These results replicate previous findings and provide novel insights into social dysfunction in psychopathology by wedding the power of hierarchical (i.e., dimensional) modeling and interpersonal circumplex assessment."
  },
  {
    "objectID": "publications/articles/girard2021a.html",
    "href": "publications/articles/girard2021a.html",
    "title": "Reconsidering the Duchenne smile: Formalizing and testing hypotheses about eye constriction and positive emotion",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Yin, L., & Morency, L.-P. (2021). Reconsidering the Duchenne smile: Formalizing and testing hypotheses about eye constriction and positive emotion. Affective Science, 2(1), 32–47."
  },
  {
    "objectID": "publications/articles/girard2021a.html#citation-apa-7",
    "href": "publications/articles/girard2021a.html#citation-apa-7",
    "title": "Reconsidering the Duchenne smile: Formalizing and testing hypotheses about eye constriction and positive emotion",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Yin, L., & Morency, L.-P. (2021). Reconsidering the Duchenne smile: Formalizing and testing hypotheses about eye constriction and positive emotion. Affective Science, 2(1), 32–47."
  },
  {
    "objectID": "publications/articles/girard2021a.html#abstract",
    "href": "publications/articles/girard2021a.html#abstract",
    "title": "Reconsidering the Duchenne smile: Formalizing and testing hypotheses about eye constriction and positive emotion",
    "section": "Abstract",
    "text": "Abstract\nThe common view of emotional expressions is that certain configurations of facial-muscle movements reliably reveal certain categories of emotion. The principal exemplar of this view is the Duchenne smile, a configuration of facial-muscle movements (i.e., smiling with eye constriction) that has been argued to reliably reveal genuine positive emotion. In this paper, we formalized a list of hypotheses that have been proposed regarding the Duchenne smile, briefly reviewed the literature weighing on these hypotheses, identified limitations and unanswered questions, and conducted two empirical studies to begin addressing these limitations and answering these questions. Both studies analyzed a database of 751 smiles observed while 136 participants completed experimental tasks designed to elicit amusement, embarrassment, fear, and physical pain. Study 1 focused on participants’ self-reported positive emotion and Study 2 focused on how third-party observers would perceive videos of these smiles. Most of the hypotheses that have been proposed about the Duchenne smile were either contradicted by or only weakly supported by our data. Eye constriction did provide some information about experienced positive emotion, but this information was lacking in specificity, already provided by other smile characteristics, and highly dependent on context. Eye constriction provided more information about perceived positive emotion, including some unique information over other smile characteristics, but context was also important here as well. Overall, our results suggest that accurately inferring positive emotion from a smile requires more sophisticated methods than simply looking for the presence/absence (or even the intensity) of eye constriction."
  },
  {
    "objectID": "publications/articles/grove2019.html",
    "href": "publications/articles/grove2019.html",
    "title": "Narcissistic admiration and rivalry: An interpersonal approach to construct validation",
    "section": "",
    "text": "Grove, J. L., Smith, T. W., Girard, J. M., & Wright, A. G. (2019). Narcissistic admiration and rivalry: An interpersonal approach to construct validation. Journal of Personality Disorders, 33(6), 751–775."
  },
  {
    "objectID": "publications/articles/grove2019.html#citation-apa-7",
    "href": "publications/articles/grove2019.html#citation-apa-7",
    "title": "Narcissistic admiration and rivalry: An interpersonal approach to construct validation",
    "section": "",
    "text": "Grove, J. L., Smith, T. W., Girard, J. M., & Wright, A. G. (2019). Narcissistic admiration and rivalry: An interpersonal approach to construct validation. Journal of Personality Disorders, 33(6), 751–775."
  },
  {
    "objectID": "publications/articles/grove2019.html#abstract",
    "href": "publications/articles/grove2019.html#abstract",
    "title": "Narcissistic admiration and rivalry: An interpersonal approach to construct validation",
    "section": "Abstract",
    "text": "Abstract\nThe present study applied the interpersonal perspective in testing the narcissistic admiration and rivalry concept (NARC) and examining the construct validity of the corresponding Narcissistic Admiration and Rivalry Questionnaire (NARQ). Two undergraduate samples (Sample 1: N = 290; Sample 2: N = 188) completed self-report measures of interpersonal processes based in the interpersonal circumplex (IPC), as well as measures of related constructs. In examining IPC correlates, we used a novel bootstrapping approach to determine if admiration and rivalry related to differing interpersonal profiles. Consistent with our hypotheses, admiration was distinctly related to generally agentic (i.e., dominant) interpersonal processes, whereas rivalry generally reflected (low) communal (i.e., hostile) interpersonal processes. Furthermore, NARQ-admiration and NARQ-rivalry related to generally adaptive and maladaptive aspects of status-related constructs, emotional, personality, and social adjustment, respectively. This research provides further support for the NARC, as well as construct validation for the NARQ."
  },
  {
    "objectID": "publications/articles/mcduff2017.html",
    "href": "publications/articles/mcduff2017.html",
    "title": "Large-scale observational evidence of cross-cultural differences in facial behavior",
    "section": "",
    "text": "McDuff, D., Girard, J. M., & El Kaliouby, R. (2017). Large-scale observational evidence of cross-cultural differences in facial behavior. Journal of Nonverbal Behavior, 41(1), 1–19."
  },
  {
    "objectID": "publications/articles/mcduff2017.html#citation-apa-7",
    "href": "publications/articles/mcduff2017.html#citation-apa-7",
    "title": "Large-scale observational evidence of cross-cultural differences in facial behavior",
    "section": "",
    "text": "McDuff, D., Girard, J. M., & El Kaliouby, R. (2017). Large-scale observational evidence of cross-cultural differences in facial behavior. Journal of Nonverbal Behavior, 41(1), 1–19."
  },
  {
    "objectID": "publications/articles/mcduff2017.html#abstract",
    "href": "publications/articles/mcduff2017.html#abstract",
    "title": "Large-scale observational evidence of cross-cultural differences in facial behavior",
    "section": "Abstract",
    "text": "Abstract\nSelf-report studies have found evidence that cultures differ in the display rules they have for facial expressions (i.e., for what is appropriate for different people at different times). However, observational studies of actual patterns of facial behavior have been rare and typically limited to the analysis of dozens of participants from two or three regions. We present the first large-scale evidence of cultural differences in observed facial behavior, including 740,984 participants from 12 countries around the world. We used an Internet-based framework to collect video data of participants in two different settings: in their homes and in market research facilities. Using computer vision algorithms designed for this dataset, we measured smiling and brow furrowing expressions as participants watched television ads. Our results reveal novel findings and provide empirical evidence to support theories about cultural and gender differences in display rules. Participants from more individualist cultures displayed more brow furrowing overall, whereas smiling depended on both culture and setting. Specifically, participants from more individualist countries were more expressive in the facility setting, while participants from more collectivist countries were more expressive in the home setting. Female participants displayed more smiling and less brow furrowing than male participants overall, with the latter difference being more pronounced in more individualist countries. This is the first study to leverage advances in computer science to enable large-scale observational research that would not have been possible using traditional methods."
  },
  {
    "objectID": "publications/articles/ross2017.html",
    "href": "publications/articles/ross2017.html",
    "title": "Momentary patterns of covariation between specific affects and interpersonal behavior: Linking relationship science and personality assessment",
    "section": "",
    "text": "Ross, J. M., Girard, J. M., Wright, A. G. C., Beeney, J. E., Scott, L. N., Hallquist, M. N., Lazarus, S. A., Stepp, S. D., & Pilkonis, P. A. (2017). Momentary patterns of covariation between specific affects and interpersonal behavior: Linking relationship science and personality assessment. Psychological Assessment, 29(2), 123–134."
  },
  {
    "objectID": "publications/articles/ross2017.html#citation-apa-7",
    "href": "publications/articles/ross2017.html#citation-apa-7",
    "title": "Momentary patterns of covariation between specific affects and interpersonal behavior: Linking relationship science and personality assessment",
    "section": "",
    "text": "Ross, J. M., Girard, J. M., Wright, A. G. C., Beeney, J. E., Scott, L. N., Hallquist, M. N., Lazarus, S. A., Stepp, S. D., & Pilkonis, P. A. (2017). Momentary patterns of covariation between specific affects and interpersonal behavior: Linking relationship science and personality assessment. Psychological Assessment, 29(2), 123–134."
  },
  {
    "objectID": "publications/articles/ross2017.html#abstract",
    "href": "publications/articles/ross2017.html#abstract",
    "title": "Momentary patterns of covariation between specific affects and interpersonal behavior: Linking relationship science and personality assessment",
    "section": "Abstract",
    "text": "Abstract\nRelationships are among the most salient factors affecting happiness and wellbeing for individuals and families. Relationship science has identified the study of dyadic behavioral patterns between couple members during conflict as an important window in to relational functioning with both short-term and long-term consequences. Several methods have been developed for the momentary assessment of behavior during interpersonal transactions. Among these, the most popular is the Specific Affect Coding System (SPAFF), which organizes social behavior into a set of discrete behavioral constructs. This study examines the interpersonal meaning of the SPAFF codes through the lens of interpersonal theory, which uses the fundamental dimensions of Dominance and Affiliation to organize interpersonal behavior. A sample of 67 couples completed a conflict task, which was video recorded and coded using SPAFF and a method for rating momentary interpersonal behavior, the Continuous Assessment of Interpersonal Dynamics (CAID). Actor partner interdependence models in a multilevel structural equation modeling framework were used to study the covariation of SPAFF codes and CAID ratings. Results showed that a number of SPAFF codes had clear interpersonal signatures, but many did not. Additionally, actor and partner effects for the same codes were strongly consistent with interpersonal theory’s principle of complementarity. Thus, findings reveal points of convergence and divergence in the 2 systems and provide support for central tenets of interpersonal theory. Future directions based on these initial findings are discussed."
  },
  {
    "objectID": "publications/articles/shepherd2017.html",
    "href": "publications/articles/shepherd2017.html",
    "title": "Comparison of comprehensive and abstinence-only sexuality education in young African American adolescents",
    "section": "",
    "text": "Shepherd, L. M., Sly, K. F., & Girard, J. M. (2017). Comparison of comprehensive and abstinence-only sexuality education in young African American adolescents. Journal of Adolescence, 61, 50–63."
  },
  {
    "objectID": "publications/articles/shepherd2017.html#citation-apa-7",
    "href": "publications/articles/shepherd2017.html#citation-apa-7",
    "title": "Comparison of comprehensive and abstinence-only sexuality education in young African American adolescents",
    "section": "",
    "text": "Shepherd, L. M., Sly, K. F., & Girard, J. M. (2017). Comparison of comprehensive and abstinence-only sexuality education in young African American adolescents. Journal of Adolescence, 61, 50–63."
  },
  {
    "objectID": "publications/articles/shepherd2017.html#abstract",
    "href": "publications/articles/shepherd2017.html#abstract",
    "title": "Comparison of comprehensive and abstinence-only sexuality education in young African American adolescents",
    "section": "Abstract",
    "text": "Abstract\nThe purpose of this study was to identify predictors of sexual behavior and condom use in African American adolescents, as well as to evaluate the effectiveness of comprehensive sexuality and abstinence-only education to reduce adolescent sexual behavior and increase condom use. Participants included 450 adolescents aged 12-14 years in the southern United States. Regression analyses showed favorable attitudes toward sexual behavior and social norms significantly predicted recent sexual behavior, and favorable attitudes toward condoms significantly predicted condom usage. Self-efficacy was not found to be predictive of adolescents’ sexual behavior or condom use. There were no significant differences in recent sexual behavior based on type of sexuality education. Adolescents who received abstinence-only education had reduced favorable attitudes toward condom use, and were more likely to have unprotected sex than the comparison group. Findings suggest that adolescents who receive abstinence-only education are at greater risk of engaging in unprotected sex."
  },
  {
    "objectID": "publications/articles/vanoest2022.html",
    "href": "publications/articles/vanoest2022.html",
    "title": "Weighting schemes and incomplete data: A generalized Bayesian framework for chance-corrected interrater agreement",
    "section": "",
    "text": "van Oest, R., & Girard, J. M. (2022). Weighting schemes and incomplete data: A generalized Bayesian framework for chance-corrected interrater agreement. Psychological Methods, 27(6), 1069–1088."
  },
  {
    "objectID": "publications/articles/vanoest2022.html#citation-apa-7",
    "href": "publications/articles/vanoest2022.html#citation-apa-7",
    "title": "Weighting schemes and incomplete data: A generalized Bayesian framework for chance-corrected interrater agreement",
    "section": "",
    "text": "van Oest, R., & Girard, J. M. (2022). Weighting schemes and incomplete data: A generalized Bayesian framework for chance-corrected interrater agreement. Psychological Methods, 27(6), 1069–1088."
  },
  {
    "objectID": "publications/articles/vanoest2022.html#abstract",
    "href": "publications/articles/vanoest2022.html#abstract",
    "title": "Weighting schemes and incomplete data: A generalized Bayesian framework for chance-corrected interrater agreement",
    "section": "Abstract",
    "text": "Abstract\nVan Oest (2019) developed a framework to assess interrater agreement for nominal categories and complete data. We generalize this framework to all four situations of nominal or ordinal categories and complete or incomplete data. The mathematical solution yields a chance-corrected agreement coefficient that accommodates any weighting scheme for penalizing rater disagreements and any number of raters and categories. By incorporating Bayesian estimates of the category proportions, the generalized coefficient also captures situations in which raters classify only subsets of items; that is, incomplete data. Furthermore, this coefficient encompasses existing chance-corrected agreement coefficients: the S-coefficient, Scott’s pi, Fleiss’ kappa, and Van Oest’s uniform prior coefficient, all augmented with a weighting scheme and the option of incomplete data. We use simulation to compare these nested coefficients. The uniform prior coefficient tends to perform best, in particular, if one category has a much larger proportion than others. The gap with Scott’s pi and Fleiss’ kappa widens if the weighting scheme becomes more lenient to small disagreements and often if more item classifications are missing; missingness biases play a moderating role. The uniform prior coefficient often performs much better than the S-coefficient, but the S-coefficient sometimes performs best for small samples, missing data, and lenient weighting schemes. The generalized framework implies a new interpretation of chance-corrected weighted agreement coefficients: These coefficients estimate the probability that both raters in a pair assign an item to its correct category without guessing. Whereas Van Oest showed this interpretation for unweighted agreement, we generalize to weighted agreement."
  },
  {
    "objectID": "publications/articles/vanoest2022.html#translational-abstract",
    "href": "publications/articles/vanoest2022.html#translational-abstract",
    "title": "Weighting schemes and incomplete data: A generalized Bayesian framework for chance-corrected interrater agreement",
    "section": "Translational Abstract",
    "text": "Translational Abstract\nMany studies and assessments require classification of subjective items (e.g., text) into categories (e.g., based on content). To assess whether the results are reproducible, it is good practice to let two or more raters independently classify the items, compute the proportion of pairwise rater agreement, and adjust for agreement expected by chance. Most chance-corrected agreement coefficients assume nominal categories and include only full agreements in which raters choose the same category. However, many situations (e.g., point scales) imply ordinal categories, where raters may receive partial credit for disagreements, based on the distance of their chosen categories and captured by a weighting scheme. Furthermore, raters often classify only subsets of items, where the missing data occur either by accident or by design. The present study develops a framework to estimate chance-corrected agreement for all four combinations of nominal or ordinal categories and complete or incomplete data. The resulting coefficient requires only a few lines of programming code and captures several existing coefficients via different values of its input parameters; it augments all nested coefficients with a weighting scheme and the option of missing item classifications. We use simulation to compare the coefficient performances for different weighting schemes, missing data mechanisms, and category proportions: The so-called uniform prior coefficient often (but not always) performs best. Furthermore, our framework implies that chance-corrected agreement coefficients, both unweighted and weighted, estimate the probability that both raters in a pair assign an item to its correct category without guessing."
  },
  {
    "objectID": "publications/proceedings/girard2013.html",
    "href": "publications/proceedings/girard2013.html",
    "title": "Social risk and depression: Evidence from manual and automatic facial expression analysis",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Mahoor, M. H., Mavadati, S. M., & Rosenwald, D. P. (2013). Social risk and depression: Evidence from manual and automatic facial expression analysis. Proceedings of the 10th IEEE International Conference on Automatic Face & Gesture Recognition (FG), 1–8."
  },
  {
    "objectID": "publications/proceedings/girard2013.html#citation-apa-7",
    "href": "publications/proceedings/girard2013.html#citation-apa-7",
    "title": "Social risk and depression: Evidence from manual and automatic facial expression analysis",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Mahoor, M. H., Mavadati, S. M., & Rosenwald, D. P. (2013). Social risk and depression: Evidence from manual and automatic facial expression analysis. Proceedings of the 10th IEEE International Conference on Automatic Face & Gesture Recognition (FG), 1–8."
  },
  {
    "objectID": "publications/proceedings/girard2013.html#abstract",
    "href": "publications/proceedings/girard2013.html#abstract",
    "title": "Social risk and depression: Evidence from manual and automatic facial expression analysis",
    "section": "Abstract",
    "text": "Abstract\nInvestigated the relationship between change over time in severity of depression symptoms and facial expression. Depressed participants were followed over the course of treatment and video recorded during a series of clinical interviews. Facial expressions were analyzed from the video using both manual and automatic systems. Automatic and manual coding were highly consistent for FACS action units, and showed similar effects for change over time in depression severity. For both systems, when symptom severity was high, participants made more facial expressions associated with contempt, smiled less, and those smiles that occurred were more likely to be accompanied by facial actions associated with contempt. These results are consistent with the “social risk hypothesis” of depression. According to this hypothesis, when symptoms are severe, depressed participants withdraw from other people in order to protect themselves from anticipated rejection, scorn, and social exclusion. As their symptoms fade, participants send more signals indicating a willingness to affiliate. The finding that automatic facial expression analysis was both consistent with manual coding and produced the same pattern of depression effects suggests that automatic facial expression analysis may be ready for use in behavioral and clinical science."
  },
  {
    "objectID": "publications/proceedings/girard2017b.html",
    "href": "publications/proceedings/girard2017b.html",
    "title": "Sayette group formation task (GFT) spontaneous facial expression database",
    "section": "",
    "text": "Girard, J. M., Chu, W.-S., Jeni, L. A., Cohn, J. F., De La Torre, F., & Sayette, M. A. (2017). Sayette Group Formation Task (GFT) Spontaneous Facial Expression Database. Proceedings of the 12th IEEE International Conference on Automatic Face and Gesture Recognition (FG), 581–588."
  },
  {
    "objectID": "publications/proceedings/girard2017b.html#citation-apa-7",
    "href": "publications/proceedings/girard2017b.html#citation-apa-7",
    "title": "Sayette group formation task (GFT) spontaneous facial expression database",
    "section": "",
    "text": "Girard, J. M., Chu, W.-S., Jeni, L. A., Cohn, J. F., De La Torre, F., & Sayette, M. A. (2017). Sayette Group Formation Task (GFT) Spontaneous Facial Expression Database. Proceedings of the 12th IEEE International Conference on Automatic Face and Gesture Recognition (FG), 581–588."
  },
  {
    "objectID": "publications/proceedings/girard2017b.html#abstract",
    "href": "publications/proceedings/girard2017b.html#abstract",
    "title": "Sayette group formation task (GFT) spontaneous facial expression database",
    "section": "Abstract",
    "text": "Abstract\nDespite the important role that facial expressions play in interpersonal communication and our knowledge that interpersonal behavior is influenced by social context, no currently available facial expression database includes multiple interacting participants. The Sayette Group Formation Task (GFT) database addresses the need for well-annotated video of multiple participants during unscripted interactions. The database includes 172,800 video frames from 96 participants in 32 three-person groups. To aid in the development of automated facial expression analysis systems, GFT includes expert annotations of FACS occurrence and intensity, facial landmark tracking, and baseline results for linear SVM, deep learning, active patch learning, and personalized classification. Baseline performance is quantified and compared using identical partitioning and a variety of metrics (including means and confidence intervals). The highest performance scores were found for the deep learning and active patch learning methods. Learn more at http://osf.io/7wcyz"
  },
  {
    "objectID": "publications/proceedings/girard2019a.html",
    "href": "publications/proceedings/girard2019a.html",
    "title": "Reconsidering the Duchenne Smile: Indicator of Positive Emotion or Artifact of Smile Intensity?",
    "section": "",
    "text": "Girard, J. M., Shandar, G., Liu, Z., Cohn, J. F., Yin, L., & Morency, L.-P. (2019). Reconsidering the Duchenne smile: Indicator of positive emotion or artifact of smile intensity? Proceedings of the 8th International Conference on Affective Computing and Intelligent Interaction (ACII), 594–599."
  },
  {
    "objectID": "publications/proceedings/girard2019a.html#citation-apa-7",
    "href": "publications/proceedings/girard2019a.html#citation-apa-7",
    "title": "Reconsidering the Duchenne Smile: Indicator of Positive Emotion or Artifact of Smile Intensity?",
    "section": "",
    "text": "Girard, J. M., Shandar, G., Liu, Z., Cohn, J. F., Yin, L., & Morency, L.-P. (2019). Reconsidering the Duchenne smile: Indicator of positive emotion or artifact of smile intensity? Proceedings of the 8th International Conference on Affective Computing and Intelligent Interaction (ACII), 594–599."
  },
  {
    "objectID": "publications/proceedings/girard2019a.html#abstract",
    "href": "publications/proceedings/girard2019a.html#abstract",
    "title": "Reconsidering the Duchenne Smile: Indicator of Positive Emotion or Artifact of Smile Intensity?",
    "section": "Abstract",
    "text": "Abstract\nThe Duchenne smile hypothesis is that smiles that include eye constriction (AU6) are the product of genuine positive emotion, whereas smiles that do not are either falsified or related to negative emotion. This hypothesis has become very influential and is often used in scientific and applied settings to justify the inference that a smile is either true or false. However, empirical support for this hypothesis has been equivocal and some researchers have proposed that, rather than being a reliable indicator of positive emotion, AU6 may just be an artifact produced by intense smiles. Initial support for this proposal has been found when comparing smiles related to genuine and feigned positive emotion; however, it has not yet been examined when comparing smiles related to genuine positive and negative emotion. The current study addressed this gap in the literature by examining spontaneous smiles from 136 participants during the elicitation of amusement, embarrassment, fear, and pain (from the BP4D+ dataset). Bayesian multilevel regression models were used to quantify the associations between AU6 and self-reported amusement while controlling for smile intensity. Models were estimated to infer amusement from AU6 and to explain the intensity of AU6 using amusement. In both cases, controlling for smile intensity substantially reduced the hypothesized association, whereas the effect of smile intensity itself was quite large and reliable. These results provide further evidence that the Duchenne smile is likely an artifact of smile intensity rather than a reliable and unique indicator of genuine positive emotion."
  },
  {
    "objectID": "publications/proceedings/jeni2013.html",
    "href": "publications/proceedings/jeni2013.html",
    "title": "Continuous AU intensity estimation using localized, sparse facial feature space",
    "section": "",
    "text": "Jeni, L. A., Girard, J. M., Cohn, J. F., & De la Torre, F. (2013). Continuous AU intensity estimation using localized, sparse facial feature space. Proceedings of the 10th IEEE International Conference on Automated Face & Gesture Recognition (FG), 1–7."
  },
  {
    "objectID": "publications/proceedings/jeni2013.html#citation-apa-7",
    "href": "publications/proceedings/jeni2013.html#citation-apa-7",
    "title": "Continuous AU intensity estimation using localized, sparse facial feature space",
    "section": "",
    "text": "Jeni, L. A., Girard, J. M., Cohn, J. F., & De la Torre, F. (2013). Continuous AU intensity estimation using localized, sparse facial feature space. Proceedings of the 10th IEEE International Conference on Automated Face & Gesture Recognition (FG), 1–7."
  },
  {
    "objectID": "publications/proceedings/jeni2013.html#abstract",
    "href": "publications/proceedings/jeni2013.html#abstract",
    "title": "Continuous AU intensity estimation using localized, sparse facial feature space",
    "section": "Abstract",
    "text": "Abstract\nMost work in automatic facial expression analysis seeks to detect discrete facial actions. Yet, the meaning and function of facial actions often depends in part on their intensity. We propose a part-based, sparse representation for automated measurement of continuous variation in AU intensity. We evaluated its effectiveness in two publically available databases, CK+ and the soon to be released Binghamton high-resolution spontaneous 3D dyadic facial expression database. The former consists of posed facial expressions and ordinal level intensity (absent, low, and high). The latter consists of spontaneous facial expression in response to diverse, well-validated emotion inductions, and 6 ordinal levels of AU intensity. In a preliminary test, we started from discrete emotion labels and ordinal-scale intensity annotation in the CK+ dataset. The algorithm achieved state-of-the-art performance. These preliminary results supported the utility of the part-based, sparse representation. Second, we applied the algorithm to the more demanding task of continuous AU intensity estimation in spontaneous facial behavior in the Binghamton database. Manual 6-point ordinal coding and continuous measurement were highly consistent. Visual analysis of the overlay of continuous measurement by the algorithm and manual ordinal coding strongly supported the representational power of the proposed method to smoothly interpolate across the full range of AU intensity."
  },
  {
    "objectID": "publications/proceedings/lin2020b.html",
    "href": "publications/proceedings/lin2020b.html",
    "title": "Toward Multimodal Modeling of Emotional Expressiveness",
    "section": "",
    "text": "Lin, V., Girard, J. M., Sayette, M. A., & Morency, L.-P. (2020). Toward Multimodal Modeling of Emotional Expressiveness. Proceedings of the 22nd International Conference on Multimodal Interaction, 548–557."
  },
  {
    "objectID": "publications/proceedings/lin2020b.html#citation-apa-7",
    "href": "publications/proceedings/lin2020b.html#citation-apa-7",
    "title": "Toward Multimodal Modeling of Emotional Expressiveness",
    "section": "",
    "text": "Lin, V., Girard, J. M., Sayette, M. A., & Morency, L.-P. (2020). Toward Multimodal Modeling of Emotional Expressiveness. Proceedings of the 22nd International Conference on Multimodal Interaction, 548–557."
  },
  {
    "objectID": "publications/proceedings/lin2020b.html#abstract",
    "href": "publications/proceedings/lin2020b.html#abstract",
    "title": "Toward Multimodal Modeling of Emotional Expressiveness",
    "section": "Abstract",
    "text": "Abstract\nEmotional expressiveness captures the extent to which a person tends to outwardly display their emotions through behavior. Due to the close relationship between emotional expressiveness and behavioral health, as well as the crucial role that it plays in social interaction, the ability to automatically predict emotional expressiveness stands to spur advances in science, medicine, and industry. In this paper, we explore three related research questions. First, how well can emotional expressiveness be predicted from visual,linguistic, and multimodal behavioral signals? Second, how important is each behavioral modality to the prediction of emotional expressiveness? Third, which behavioral signals are reliably related to emotional expressiveness? To answer these questions, we add highly reliable transcripts and human ratings of perceived emotional expressiveness to an existing video database and use this data to train, validate, and test predictive models. Our best model shows promising predictive performance on this dataset (RMSE=0.65,R2=0.45,r=0.74). Multimodal models tend to perform best overall, and models trained on the linguistic modality tend to outperform models trained on the visual modality. Finally,examination of our interpretable models’ coefficients reveals a number of visual and linguistic behavioral signals—such as facial action unit intensity, overall word count, and use of words related to social processes—that reliably predict emotional expressiveness."
  },
  {
    "objectID": "publications/proceedings/lin2020b.html#awards",
    "href": "publications/proceedings/lin2020b.html#awards",
    "title": "Toward Multimodal Modeling of Emotional Expressiveness",
    "section": "Awards",
    "text": "Awards\nThis paper was nominated for Best Paper at ICMI 2020."
  },
  {
    "objectID": "publications/proceedings/vail2022.html",
    "href": "publications/proceedings/vail2022.html",
    "title": "Toward causal understanding of therapist-client relationships: A study of language modality and social entrainment",
    "section": "",
    "text": "Vail, A. K., Girard, J. M., Bylsma, L. M., Cohn, J. F., Fournier, J., Swartz, H. A., & Morency, L.-P. (2022). Toward causal understanding of therapist-client relationships: A study of language modality and social entrainment. Proceedings of the 24th ACM International Conference on Multimodal Interaction (ICMI), 487–494."
  },
  {
    "objectID": "publications/proceedings/vail2022.html#citation-apa-7",
    "href": "publications/proceedings/vail2022.html#citation-apa-7",
    "title": "Toward causal understanding of therapist-client relationships: A study of language modality and social entrainment",
    "section": "",
    "text": "Vail, A. K., Girard, J. M., Bylsma, L. M., Cohn, J. F., Fournier, J., Swartz, H. A., & Morency, L.-P. (2022). Toward causal understanding of therapist-client relationships: A study of language modality and social entrainment. Proceedings of the 24th ACM International Conference on Multimodal Interaction (ICMI), 487–494."
  },
  {
    "objectID": "publications/proceedings/vail2022.html#abstract",
    "href": "publications/proceedings/vail2022.html#abstract",
    "title": "Toward causal understanding of therapist-client relationships: A study of language modality and social entrainment",
    "section": "Abstract",
    "text": "Abstract\nThe relationship between a therapist and their client is one of the most critical determinants of successful therapy. The working alliance is a multifaceted concept capturing the collaborative aspect of the therapist-client relationship; a strong working alliance has been extensively linked to many positive therapeutic outcomes. Although therapy sessions are decidedly multimodal interactions, the language modality is of particular interest given its recognized relationship to similar dyadic concepts such as rapport, cooperation, and affiliation. Specifically, in this work we study language entrainment, which measures how much the therapist and client adapt toward each other’s use of language over time. Despite the growing body of work in this area, however, relatively few studies examine causal relationships between human behavior and these relationship metrics: does an individual’s perception of their partner affect how they speak, or does how they speak affect their perception? We explore these questions in this work through the use of structural equation modeling (SEM) techniques, which allow for both multilevel and temporal modeling of the relationship between the quality of the therapist-client working alliance and the participants’ language entrainment. In our first experiment, we demonstrate that these techniques perform well in comparison to other common machine learning models, with the added benefits of interpretability and causal analysis. In our second analysis, we interpret the learned models to examine the relationship between working alliance and language entrainment and address our exploratory research questions. The results reveal that a therapist’s language entrainment can have a significant impact on the client’s perception of the working alliance, and that the client’s language entrainment is a strong indicator of their perception of the working alliance. We discuss the implications of these results and consider several directions for future work in multimodality."
  },
  {
    "objectID": "publications/proceedings/valstar2017.html",
    "href": "publications/proceedings/valstar2017.html",
    "title": "FERA 2017 - Addressing head pose in the third facial expression recognition and analysis challenge",
    "section": "",
    "text": "Valstar, M. F., Sanchez-Lozano, E., Cohn, J. F., Jeni, L. A., Girard, J. M., Zhang, Z., Yin, L., & Pantic, M. (2017). FERA 2017—Addressing head pose in the third facial expression recognition and analysis challenge. Proceedings of the 12th IEEE Conference on Automatic Face & Gesture Recognition (FG), 839–847."
  },
  {
    "objectID": "publications/proceedings/valstar2017.html#citation-apa-7",
    "href": "publications/proceedings/valstar2017.html#citation-apa-7",
    "title": "FERA 2017 - Addressing head pose in the third facial expression recognition and analysis challenge",
    "section": "",
    "text": "Valstar, M. F., Sanchez-Lozano, E., Cohn, J. F., Jeni, L. A., Girard, J. M., Zhang, Z., Yin, L., & Pantic, M. (2017). FERA 2017—Addressing head pose in the third facial expression recognition and analysis challenge. Proceedings of the 12th IEEE Conference on Automatic Face & Gesture Recognition (FG), 839–847."
  },
  {
    "objectID": "publications/proceedings/valstar2017.html#abstract",
    "href": "publications/proceedings/valstar2017.html#abstract",
    "title": "FERA 2017 - Addressing head pose in the third facial expression recognition and analysis challenge",
    "section": "Abstract",
    "text": "Abstract\nThe field of Automatic Facial Expression Analysis has grown rapidly in recent years. However, despite progress in new approaches as well as benchmarking efforts, most evaluations still focus on either posed expressions, near-frontal recordings, or both. This makes it hard to tell how existing expression recognition approaches perform under conditions where faces appear in a wide range of poses (or camera views), displaying ecologically valid expressions. The main obstacle for assessing this is the availability of suitable data, and the challenge proposed here addresses this limitation. The FG 2017 Facial Expression Recognition and Analysis challenge (FERA 2017) extends FERA 2015 to the estimation of Action Units occurrence and intensity under different camera views. In this paper we present the third challenge in automatic recognition of facial expressions, to be held in conjunction with the 12th IEEE conference on Face and Gesture Recognition, May 2017, in Washington, United States. Two sub-challenges are defined: the detection of AU occurrence, and the estimation of AU intensity. In this work we outline the evaluation protocol, the data used, and the results of a baseline method for both sub-challenges."
  },
  {
    "objectID": "publications/proceedings/zhang2016.html",
    "href": "publications/proceedings/zhang2016.html",
    "title": "Multimodal spontaneous emotion corpus for human behavior analysis",
    "section": "",
    "text": "Zhang, Z., Girard, J. M., Wu, Y., Zhang, X., Liu, P., Ciftci, U., Canavan, S., Reale, M., Horowitz, A., Yang, H., Cohn, J. F., Ji, Q., & Yin, L. (2016). Multimodal spontaneous emotion corpus for human behavior analysis. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3438–3446."
  },
  {
    "objectID": "publications/proceedings/zhang2016.html#citation-apa-7",
    "href": "publications/proceedings/zhang2016.html#citation-apa-7",
    "title": "Multimodal spontaneous emotion corpus for human behavior analysis",
    "section": "",
    "text": "Zhang, Z., Girard, J. M., Wu, Y., Zhang, X., Liu, P., Ciftci, U., Canavan, S., Reale, M., Horowitz, A., Yang, H., Cohn, J. F., Ji, Q., & Yin, L. (2016). Multimodal spontaneous emotion corpus for human behavior analysis. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3438–3446."
  },
  {
    "objectID": "publications/proceedings/zhang2016.html#abstract",
    "href": "publications/proceedings/zhang2016.html#abstract",
    "title": "Multimodal spontaneous emotion corpus for human behavior analysis",
    "section": "Abstract",
    "text": "Abstract\nEmotion is expressed in multiple modalities, yet most research has considered at most one or two. This stems in part from the lack of large, diverse, well-annotated, multimodal databases with which to develop and test algorithms. We present a well-annotated, multimodal, multidimensional spontaneous emotion corpus of 140 participants. Emotion inductions were highly varied. Data were acquired from a variety of sensors of the face that included high-resolution 3D dynamic imaging, high-resolution 2D video, and thermal (infrared) sensing, and contact physiological sensors that included electrical conductivity of the skin, respiration, blood pressure, and heart rate. Facial expression was annotated for both the occurrence and intensity of facial action units from 2D video by experts in the Facial Action Coding System (FACS). The corpus further includes derived features from 3D, 2D, and IR (infrared) sensors and baseline results for facial expression and action unit detection. The entire corpus will be made available to the research community."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Motion Metrics",
    "section": "",
    "text": "Home\n    Research"
  },
  {
    "objectID": "research.html#research-topics",
    "href": "research.html#research-topics",
    "title": "Motion Metrics",
    "section": "Research Topics",
    "text": "Research Topics\n\nClinical Topics\n\nDepression – We are interested in improving the assessment and treatment of depression. In particular, we are interested in (a) developing computational depression screening tools that can be deployed in under-served (e.g., remote or low resource) communities, (b) studying the connections between mood, depression, and interpersonal functioning, and (c) understanding the psychotherapeutic processes that contribute to recovery from depression.\nPosttraumatic Stress Disorder – We are interested in how individuals who survive traumatic events respond to, make sense of, and communicate about their experiences. Toward this end, we collect and analyze trauma narratives and impact statements from patients seeking treatment for PTSD and from community members who experienced traumatic events but did not develop PTSD. We are using observational methods to measure and analyze the patterns of behaviors, emotions, and cognitions evident in these narratives and statements. Our goals are to improve PTSD assessment and to deepen clinical understanding of the psychological processes that contribute to the development, maintenance, and recovery from PTSD.\nClinical decision-making – We are interested in developing tools to support the clinical decision-making of mental healthcare workers such as (a) determining the optimal time to discharge psychiatric inpatients from the hospital, and (b) estimating patients’ risk of dropping out of psychotherapy early. This topic is deeply connected to the methodology of predictive machine learning (see below).\nStructure of psychopathology – We are interested in advancing scientific knowledge about the ways in which psychiatric signs and symptoms organize into transdiagnostic dimensions or “spectra.” As part of this effort, we are working on creating an R package for the “Bass-Ackwards” factor analytical technique (Goldberg, 2006; Forbes, 2020). We are also interested in disentangling measures of dysfunction and traits/symptoms.\nBehavioral synchrony – We are interested in studying the emergence of behavioral synchronization between members of interacting dyads (e.g., therapists and patients or parents and children). We are applying this to studying the development of the working alliance (i.e., therapeutic relationship) during psychotherapy for depression and the communication dynamics between parents and children at high risk of suicide. As part of this effort, we are working on creating an R package for the windowed cross-correlation technique for quantifying behavioral synchrony (Boker et al., 2002).\n\n\n\nMethodological Topics\n\nInterrater reliability statistics – We are interested in developing statistical techniques that quantify the degree of reliability (e.g., agreement or consistency) between raters in observational and judgment studies. As part of this effort, we are working on finalizing the agreement R package for chance-adjusted indexes of categorical agreement and are working on integrating insights and tools from Bayesian statistics and generalizability theory in the varde R pacakge.\nCircumplex statistics – We are interested in developing statistical techniques that leverage the circular structure of circumplex models of affect and interpersonal functioning. As part of this effort, we are working on expanding the circumplex R package with additional functionality and developing new extensions of the Structural Summary Method (Gurtman, 1992; Zimmermann & Wright, 2017) that accommodate multilevel (e.g., clustered and longitudinal) data and latent variables.\nObservational methods – We are interested in measuring behavior as it actually occurs using human and algorithmic observers. Toward this end, we are working on creating databases of observational records (e.g., videos) to share with other researchers and are developing software and educational resources to aid in the collection and analysis of observational data.\nApplied machine learning – We are interested in applying recent advances in predictive modeling techniques to the study of human behavior (e.g., to aid in clinical decision-making and observational measurement). We are particularly interested in model evaluation, explanation, and comparison techniques that combine insights and techniques from statistical inference and machine learning.\nBayesian multilevel modeling – We are interested in leveraging the wonderful flexibility of Bayesian mixed effects models to answer applied questions about multilevel (e.g., clustered or longitudinal) data. We are also interested in adapting this framework to improve estimation in other topics (e.g., inter-rater reliability statistics, circumplex statistics, and comparing machine learning models)."
  },
  {
    "objectID": "research.html#research-funding",
    "href": "research.html#research-funding",
    "title": "Motion Metrics",
    "section": "Research Funding",
    "text": "Research Funding\n\nCurrent Funding\n\n“Novel Scalable Mental Health Screening Procedures on Ubiquitous Sensing Devices”\n\nGift, Google LLC\nGirard (PI)\n11/2022\n\n“Context-Adaptive Multimodal Informatics for Psychiatric Discharge Planning”\n\nR01, National Institutes of Health (NIMH)\nBaker (PI), Co-Is: Girard, Morency\n04/2021 – 02/2025\n\n“Multimodal Dynamics of Parent-Child Interactions and Suicide Risk”\n\nR21, National Institutes of Health (NIMH)\nBurke (PI), Co-Is: Girard, Li, Morency\n09/2022 – 07/2023\n\n\n\n\nCompleted Funding\n\n“Towards Automated Multimodal Behavioral Screening for Depression”\n\nPittsburgh Health Data Alliance (CMLH)\nMorency (PI), Co-I: Girard\n03/2019 – 02/2021\n\n“Dyadic Behavior Informatics for Psychotherapy Process and Outcome”\n\nNational Science Foundation (IIS, SCH)\nCohn, Morency, Swartz (PIs), Co-Is: Bylsma, Fournier, Girard\n09/2020 – 08/2022\n\n\n\n\nPending Funding\n\n\n\n\n\n\n\n\n\n“Dyadic Communication Dynamics in Hybrid In-Person/Remote Psychotherapy”\n\nR01, National Institutes of Health (NIMH)\nGirard (PI), Co-Is: Bylsma, Cohn, Ilardi, Morency, Swartz\nScored 14th Percentile, Awaiting Funding Decision"
  },
  {
    "objectID": "research.html#research-framework",
    "href": "research.html#research-framework",
    "title": "Motion Metrics",
    "section": "Research Framework",
    "text": "Research Framework\n\nAffective Communication\nThe substantive focus of our work is on how humans communicate important affective (e.g., emotional) information to one another. The four pillars of this work are on structure, context, dynamics, and functionality. In studying the structure of affective communication, we investigate the production and perception of visual behaviors (e.g., facial expressions, gestures, and body motion), vocal behaviors (e.g., pitch, loudness, and timbre), and verbal behaviors (e.g., syntax, semantics, and discourse). In studying the context of affective communication, we investigate the influence of the immediate and distal sociocultural environment. In studying the dynamics of affective communication, we investigate the unfolding of emotion, behavior, and the interpersonal field over time. Finally, in studying the functionality of affective communication, we investigate its relationships with mental health, pathology, and recovery.\n\n\n\nAffective Computing\nSupporting the four pillars of substantive research described above is a methodological foundation of computer and data science. The three main aspects of this methodological work are statistics, computing, and design. The statistical aspects largely focus on developing and applying Bayesian, multilevel, and latent variable models to test hypotheses about interpretable parameters or validate the other aspects. The computational aspects largely focus on building tools to collect large amounts of data and then using machine learning to process that data and make predictions. Finally, the design aspects largely focus on developing software to collect, process, and analyze research data as well as websites to educate researchers on methodological and substantive topics."
  },
  {
    "objectID": "people/staff/hansen_clint.html",
    "href": "people/staff/hansen_clint.html",
    "title": "Clint Hansen",
    "section": "",
    "text": "Home\n    People\n    Clint Hansen"
  },
  {
    "objectID": "people/staff/hansen_clint.html#biography",
    "href": "people/staff/hansen_clint.html#biography",
    "title": "Clint Hansen",
    "section": "Biography",
    "text": "Biography\nDr. Jeffrey Girard studies how emotions are expressed through verbal and nonverbal behavior, as well as how interpersonal communication is influenced by individual differences (e.g., personality and mental health) and social factors (e.g., culture and context). This work is deeply interdisciplinary and draws insights and tools from various areas of social science, computer science, statistics, and medicine.\nHe completed his doctoral training under the mentorship of Dr. Jeffrey Cohn (nonverbal behavior, machine learning, depression) and Dr. Aidan Wright (interpersonal functioning, statistics, personality). He then completed his predoctoral clinical internship at the University of Mississippi Medical Center (inpatient psychiatry, trauma treatment, neuropsychology) and a two-year postdoctoral research fellowship at Carnegie Mellon University under the mentorship of Dr. Louis-Philippe Morency (verbal behavior, machine learning, computer science).\nHe is now an Assistant Professor in the department of Psychology at the University of Kansas, where he directs the Affective Communication and Computing lab and the Brain, Behavior, and Quantitative Science (BBQ) program. He teaches undergraduate and graduate courses on psychological assessment, data science, and statistics within the department and offers several summer “bootcamp” courses for graduate students and faculty (on data science and machine learning) through Pitt Methods. He is also the developer and maintainer of numerous open-source research software packages (see Resources)."
  },
  {
    "objectID": "people/staff/hansen_clint.html#experience",
    "href": "people/staff/hansen_clint.html#experience",
    "title": "Clint Hansen",
    "section": "Experience",
    "text": "Experience\n\nUniversity of Kansas | Lawrence, KS, USA Assistant Professor of Psychology | 2020–Current Wright Faculty Scholar | 2020–2025 Director of the BBQ Program | 2022–Current\nCarnegie Mellon University | Pittsburgh, PA, USA Postdoctoral Researcher, School of Computer Science | 2018–2020\nUniversity of Mississippi Medical Center | Jackson, MS, USA Clinical Intern, Department of Psychiatry | 2017–2018"
  },
  {
    "objectID": "people/staff/hansen_clint.html#journal-editing",
    "href": "people/staff/hansen_clint.html#journal-editing",
    "title": "Clint Hansen",
    "section": "Journal Editing",
    "text": "Journal Editing\n\nNPP Digital Psychiatry and Neuroscience  Consulting Editor | 2023–Current\nJournal of Psychopathology and Clinical Science  Consulting Editor | 2023–Current\nIEEE Transactions on Affective Computing  Associate Editor | 2022–Current\nCollabra Psychology  Associate Editor | 2021–Current\nClinical Psychological Science  Consulting Editor | 2021–Current\nPsychological Assessment  Consulting Editor | 2020–2022"
  },
  {
    "objectID": "people/staff/hansen_clint.html#education",
    "href": "people/staff/hansen_clint.html#education",
    "title": "Clint Hansen",
    "section": "Education",
    "text": "Education\n\nUniversity of Pittsburgh | Pittsburgh, PA, USA PhD in Psychology (Clinical) | 2013–2018\nUniversity of Pittsburgh | Pittsburgh, PA, USA MS in Psychology (Clinical) | 2010–2013\nUniversity of Washington | Seattle, WA, USA BA in Psychology & Philosophy (Honors) | 2005–2008"
  },
  {
    "objectID": "people/staff/romijnders_robbin.html",
    "href": "people/staff/romijnders_robbin.html",
    "title": "Robbin Romijnders",
    "section": "",
    "text": "Home\n    People\n    Robbin Romijnders"
  },
  {
    "objectID": "people/staff/romijnders_robbin.html#biography",
    "href": "people/staff/romijnders_robbin.html#biography",
    "title": "Robbin Romijnders",
    "section": "Biography",
    "text": "Biography\nTBA"
  },
  {
    "objectID": "people/staff/romijnders_robbin.html#education",
    "href": "people/staff/romijnders_robbin.html#education",
    "title": "Robbin Romijnders",
    "section": "Education",
    "text": "Education\n\nUniversity of Kiel | Kiel, Schleswig-Holstein, Germany PhD Student (Electrical and Information Engineering), 2019–2023\nEindhoven University of Technology | Eindhoven, The Netherlands MSc in Biomedical Engineering | 2012–2015\nEindhoven University of Technology | Eindhoven, The Netherlands BSc in Biomedical Engineering | 2008–2012"
  },
  {
    "objectID": "people/staff/welzel_julius.html",
    "href": "people/staff/welzel_julius.html",
    "title": "Julius Welzel",
    "section": "",
    "text": "Home\n    People\n    Julius Welzel"
  },
  {
    "objectID": "people/staff/welzel_julius.html#biography",
    "href": "people/staff/welzel_julius.html#biography",
    "title": "Julius Welzel",
    "section": "Biography",
    "text": "Biography\nKassandra Gray’s research interests include interpersonal interactions, emotional expression and communication, individual differences, personality, anxiety, and depression. She is interested in how emotional expression and communication manifest in the therapeutic relationship and the influential role of individual differences (e.g., personality, anxiety, and depression).\nShe completed her undergraduate Bachelor of Science degree at Southern Nazarene University in May 2022 and is currently a doctoral student in the Clinical Psychological Science program in the Department of Psychology at the University of Kansas. She is a graduate teaching assistant (GTA) for undergraduate psychology courses and is also a member of the Affective Communication and Computing lab."
  },
  {
    "objectID": "people/staff/welzel_julius.html#education",
    "href": "people/staff/welzel_julius.html#education",
    "title": "Julius Welzel",
    "section": "Education",
    "text": "Education\n\nUniversity of Kansas | Lawrence, KS, USA PhD Student (Clinical Psychological Science), 2022–Current\nSouthern Nazarene University | Bethany, OK, USA BS in Psychology | 2018–2022"
  },
  {
    "objectID": "index.html#clinical-movement-analysis",
    "href": "index.html#clinical-movement-analysis",
    "title": "Motion Metrics",
    "section": "Clinical Movement Analysis",
    "text": "Clinical Movement Analysis\nOur clinical movement analysis services are designed to provide healthcare professionals with the information they need to diagnose, monitor, and treat a wide range of movement disorders. We use state-of-the-art equipment and cutting-edge analysis techniques to provide precise and accurate data on gait, posture, balance, and more. Our team of experts includes biomechanists, enginieers, neuroscientists and other specialists, who work together to provide comprehensive assessments and customized treatment plans for each patient."
  },
  {
    "objectID": "index.html#sports-performance-analysis",
    "href": "index.html#sports-performance-analysis",
    "title": "Motion Metrics",
    "section": "Sports Performance Analysis",
    "text": "Sports Performance Analysis\nOur sports performance analysis services are designed to help athletes of all levels achieve their full potential. Whether you are a professional athlete looking to improve your performance on the field or a weekend warrior looking to optimize your training regimen, we can help. We use the latest technology and analysis techniques to provide precise and accurate data on factors such as running gait, jump mechanics, and more. Our team of experts works closely with each athlete to help them achieve their goals.\nOur Goals\nAt Motion Metric, our goal is to be the leading provider of movement analysis services. We strive to deliver the highest quality services to our clients, with a focus on accuracy, precision, and customization. We are committed to staying at the forefront of our field, and we invest in ongoing research and development to ensure that we are always using the latest techniques and technologies to deliver the best possible outcomes for our clients."
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Motion Metrics",
    "section": "",
    "text": "Home\n    Services"
  },
  {
    "objectID": "services.html#research-topics",
    "href": "services.html#research-topics",
    "title": "Motion Metrics",
    "section": "Research Topics",
    "text": "Research Topics\n\nClinical Topics\n\nDepression – We are interested in improving the assessment and treatment of depression. In particular, we are interested in (a) developing computational depression screening tools that can be deployed in under-served (e.g., remote or low resource) communities, (b) studying the connections between mood, depression, and interpersonal functioning, and (c) understanding the psychotherapeutic processes that contribute to recovery from depression.\nPosttraumatic Stress Disorder – We are interested in how individuals who survive traumatic events respond to, make sense of, and communicate about their experiences. Toward this end, we collect and analyze trauma narratives and impact statements from patients seeking treatment for PTSD and from community members who experienced traumatic events but did not develop PTSD. We are using observational methods to measure and analyze the patterns of behaviors, emotions, and cognitions evident in these narratives and statements. Our goals are to improve PTSD assessment and to deepen clinical understanding of the psychological processes that contribute to the development, maintenance, and recovery from PTSD.\nClinical decision-making – We are interested in developing tools to support the clinical decision-making of mental healthcare workers such as (a) determining the optimal time to discharge psychiatric inpatients from the hospital, and (b) estimating patients’ risk of dropping out of psychotherapy early. This topic is deeply connected to the methodology of predictive machine learning (see below).\nStructure of psychopathology – We are interested in advancing scientific knowledge about the ways in which psychiatric signs and symptoms organize into transdiagnostic dimensions or “spectra.” As part of this effort, we are working on creating an R package for the “Bass-Ackwards” factor analytical technique (Goldberg, 2006; Forbes, 2020). We are also interested in disentangling measures of dysfunction and traits/symptoms.\nBehavioral synchrony – We are interested in studying the emergence of behavioral synchronization between members of interacting dyads (e.g., therapists and patients or parents and children). We are applying this to studying the development of the working alliance (i.e., therapeutic relationship) during psychotherapy for depression and the communication dynamics between parents and children at high risk of suicide. As part of this effort, we are working on creating an R package for the windowed cross-correlation technique for quantifying behavioral synchrony (Boker et al., 2002).\n\n\n\nMethodological Topics\n\nInterrater reliability statistics – We are interested in developing statistical techniques that quantify the degree of reliability (e.g., agreement or consistency) between raters in observational and judgment studies. As part of this effort, we are working on finalizing the agreement R package for chance-adjusted indexes of categorical agreement and are working on integrating insights and tools from Bayesian statistics and generalizability theory in the varde R pacakge.\nCircumplex statistics – We are interested in developing statistical techniques that leverage the circular structure of circumplex models of affect and interpersonal functioning. As part of this effort, we are working on expanding the circumplex R package with additional functionality and developing new extensions of the Structural Summary Method (Gurtman, 1992; Zimmermann & Wright, 2017) that accommodate multilevel (e.g., clustered and longitudinal) data and latent variables.\nObservational methods – We are interested in measuring behavior as it actually occurs using human and algorithmic observers. Toward this end, we are working on creating databases of observational records (e.g., videos) to share with other researchers and are developing software and educational resources to aid in the collection and analysis of observational data.\nApplied machine learning – We are interested in applying recent advances in predictive modeling techniques to the study of human behavior (e.g., to aid in clinical decision-making and observational measurement). We are particularly interested in model evaluation, explanation, and comparison techniques that combine insights and techniques from statistical inference and machine learning.\nBayesian multilevel modeling – We are interested in leveraging the wonderful flexibility of Bayesian mixed effects models to answer applied questions about multilevel (e.g., clustered or longitudinal) data. We are also interested in adapting this framework to improve estimation in other topics (e.g., inter-rater reliability statistics, circumplex statistics, and comparing machine learning models)."
  },
  {
    "objectID": "services.html#research-funding",
    "href": "services.html#research-funding",
    "title": "Motion Metrics",
    "section": "Research Funding",
    "text": "Research Funding\n\nCurrent Funding\n\n“Novel Scalable Mental Health Screening Procedures on Ubiquitous Sensing Devices”\n\nGift, Google LLC\nGirard (PI)\n11/2022\n\n“Context-Adaptive Multimodal Informatics for Psychiatric Discharge Planning”\n\nR01, National Institutes of Health (NIMH)\nBaker (PI), Co-Is: Girard, Morency\n04/2021 – 02/2025\n\n“Multimodal Dynamics of Parent-Child Interactions and Suicide Risk”\n\nR21, National Institutes of Health (NIMH)\nBurke (PI), Co-Is: Girard, Li, Morency\n09/2022 – 07/2023\n\n\n\n\nCompleted Funding\n\n“Towards Automated Multimodal Behavioral Screening for Depression”\n\nPittsburgh Health Data Alliance (CMLH)\nMorency (PI), Co-I: Girard\n03/2019 – 02/2021\n\n“Dyadic Behavior Informatics for Psychotherapy Process and Outcome”\n\nNational Science Foundation (IIS, SCH)\nCohn, Morency, Swartz (PIs), Co-Is: Bylsma, Fournier, Girard\n09/2020 – 08/2022\n\n\n\n\nPending Funding\n\n\n\n\n\n\n\n\n\n“Dyadic Communication Dynamics in Hybrid In-Person/Remote Psychotherapy”\n\nR01, National Institutes of Health (NIMH)\nGirard (PI), Co-Is: Bylsma, Cohn, Ilardi, Morency, Swartz\nScored 14th Percentile, Awaiting Funding Decision"
  },
  {
    "objectID": "services.html#research-framework",
    "href": "services.html#research-framework",
    "title": "Motion Metrics",
    "section": "Research Framework",
    "text": "Research Framework\n\nAffective Communication\nThe substantive focus of our work is on how humans communicate important affective (e.g., emotional) information to one another. The four pillars of this work are on structure, context, dynamics, and functionality. In studying the structure of affective communication, we investigate the production and perception of visual behaviors (e.g., facial expressions, gestures, and body motion), vocal behaviors (e.g., pitch, loudness, and timbre), and verbal behaviors (e.g., syntax, semantics, and discourse). In studying the context of affective communication, we investigate the influence of the immediate and distal sociocultural environment. In studying the dynamics of affective communication, we investigate the unfolding of emotion, behavior, and the interpersonal field over time. Finally, in studying the functionality of affective communication, we investigate its relationships with mental health, pathology, and recovery.\n\n\n\nAffective Computing\nSupporting the four pillars of substantive research described above is a methodological foundation of computer and data science. The three main aspects of this methodological work are statistics, computing, and design. The statistical aspects largely focus on developing and applying Bayesian, multilevel, and latent variable models to test hypotheses about interpretable parameters or validate the other aspects. The computational aspects largely focus on building tools to collect large amounts of data and then using machine learning to process that data and make predictions. Finally, the design aspects largely focus on developing software to collect, process, and analyze research data as well as websites to educate researchers on methodological and substantive topics."
  },
  {
    "objectID": "services.html#services",
    "href": "services.html#services",
    "title": "Motion Metrics",
    "section": "Services",
    "text": "Services\n\nClinical Movement Analysis\nOur clinical movement analysis services are designed to provide healthcare professionals with the information they need to diagnose, monitor, and treat a wide range of movement disorders. We use state-of-the-art equipment and cutting-edge analysis techniques to provide precise and accurate data on gait, posture, balance, and more. Our team of experts includes biomechanists, engineers, neuroscientists and other specialists, who work together to provide comprehensive assessments and customized treatment plans for each patient.\n\n\nSports Performance Analysis\nOur sports performance analysis services are designed to help athletes of all levels achieve their full potential. Whether you are a professional athlete looking to improve your performance on the field or a weekend warrior looking to optimize your training regimen, we can help. We use the latest technology and analysis techniques to provide precise and accurate data on factors such as running gait, jump mechanics, and more. Our team of experts works closely with each athlete to help them achieve their goals."
  },
  {
    "objectID": "posts/moderation2022/presentation.html",
    "href": "posts/moderation2022/presentation.html",
    "title": "Adventuresin Moderation",
    "section": "",
    "text": "Jeffrey M. Girard University of Kansas"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#overview",
    "href": "posts/moderation2022/presentation.html#overview",
    "title": "Adventuresin Moderation",
    "section": "Overview",
    "text": "Overview\n\nWhat is moderation in ANOVA and regression models?\nWhat questions can (and can’t) moderation answer?\nWhat tools can we use to test and probe interactions?\nANOVA Examples: 2x2, 3x2\nRegression Examples: (2x2), 2xC, CxC\nWhat are some challenges and opportunities?"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#defining-moderation",
    "href": "posts/moderation2022/presentation.html#defining-moderation",
    "title": "Adventuresin Moderation",
    "section": "Defining Moderation",
    "text": "Defining Moderation\n\nModeration adds contextualization/complexity to a model\nIt allows an IV’s effect to depend on the values of other IVs\nIt is usually tested using (bilinear) product terms\n\n\n\nThere won’t be just one effect of the IV for everyone…\n…it depends on who each person is, in terms of other IVs\n(It is like a gateway drug for mixed effects modeling)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#example-research-questions",
    "href": "posts/moderation2022/presentation.html#example-research-questions",
    "title": "Adventuresin Moderation",
    "section": "Example Research Questions",
    "text": "Example Research Questions\nCategorical-by-Categorical - Does the effect of exercise program on weight loss depend on biological sex?\n\nCategorical-by-Continuous - Does the effect of hours of exercise on weight loss depend on exercise program?\n\n\nContinuous-by-Continuous - Does the effect of hours of exercise on weight loss depend on the effort put in?\n\n\nModeration is not mediation and cannot establish causation\nModeration hypotheses should be specific and falsifiable"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#free-and-open-source-tools",
    "href": "posts/moderation2022/presentation.html#free-and-open-source-tools",
    "title": "Adventuresin Moderation",
    "section": "Free and Open-Source Tools",
    "text": "Free and Open-Source Tools\nR is a cross-platform statistical computing environment\n\nhttps://cran.r-project.org/\n\n\nRStudio adds a helpful environment for working with R\n\nhttps://posit.co/download/rstudio-desktop/\n\n\n\nThe {easystats} package adds statistics functions\n\nhttps://easystats.github.io/easystats/"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#hypotheses-and-data",
    "href": "posts/moderation2022/presentation.html#hypotheses-and-data",
    "title": "Adventuresin Moderation",
    "section": "Hypotheses and Data",
    "text": "Hypotheses and Data\n\nTraining will increase scores (relative to a control condition)\nTraining will increase scores for women more than for men\n\n\n\n\n\n\n\n\nsubject\nscore\ncondition\ngender\n\n\n\n\n1\n5.876475\ntraining\nwoman\n\n\n2\n4.805460\ncontrol\nwoman\n\n\n3\n3.384664\ntraining\nwoman\n\n\n4\n4.086933\ncontrol\nwoman\n\n\n5\n4.749709\ncontrol\nman\n\n\n6\n5.905577\ntraining\nwoman\n\n\n7\n6.005326\ntraining\nman\n\n\n8\n3.208133\ncontrol\nwoman\n\n\n9\n4.838511\ntraining\nwoman\n\n\n10\n5.127727\ntraining\nwoman\n\n\n11\n5.158661\ncontrol\nwoman\n\n\n12\n4.321588\ncontrol\nwoman\n\n\n13\n5.582489\ntraining\nman\n\n\n14\n6.743740\ntraining\nwoman\n\n\n15\n5.280495\ncontrol\nman\n\n\n16\n5.543959\ncontrol\nwoman\n\n\n17\n4.709095\ntraining\nman\n\n\n18\n4.827145\ncontrol\nman\n\n\n19\n6.451114\ntraining\nwoman\n\n\n20\n6.320117\ntraining\nman\n\n\n21\n2.589054\ncontrol\nman\n\n\n22\n4.720086\ncontrol\nwoman\n\n\n23\n5.071068\ncontrol\nman\n\n\n24\n7.096972\ntraining\nman\n\n\n25\n4.274268\ncontrol\nwoman\n\n\n26\n6.460575\ntraining\nman\n\n\n27\n6.456440\ntraining\nman\n\n\n28\n6.265848\ncontrol\nwoman\n\n\n29\n4.763044\ncontrol\nwoman\n\n\n30\n3.979886\ntraining\nwoman\n\n\n31\n5.339553\ncontrol\nwoman\n\n\n32\n6.895974\ntraining\nwoman\n\n\n33\n4.313938\ntraining\nman\n\n\n34\n5.095529\ntraining\nman\n\n\n35\n4.712894\ncontrol\nwoman\n\n\n36\n5.342586\ncontrol\nman\n\n\n37\n4.445261\ntraining\nwoman\n\n\n38\n4.342722\ncontrol\nwoman\n\n\n39\n3.985711\ncontrol\nman\n\n\n40\n5.953967\ntraining\nwoman\n\n\n41\n3.846046\ncontrol\nman\n\n\n42\n5.853868\ntraining\nman\n\n\n43\n5.648286\ntraining\nman\n\n\n44\n5.876512\ntraining\nwoman\n\n\n45\n6.506987\ncontrol\nwoman\n\n\n46\n4.572705\ntraining\nwoman\n\n\n47\n4.412859\ncontrol\nwoman\n\n\n48\n7.958926\ntraining\nwoman\n\n\n49\n6.173742\ntraining\nwoman\n\n\n50\n5.834424\ntraining\nman\n\n\n51\n6.609042\ncontrol\nwoman\n\n\n52\n4.384601\ntraining\nwoman\n\n\n53\n5.010130\ncontrol\nwoman\n\n\n54\n3.947642\ncontrol\nwoman\n\n\n55\n5.933923\ntraining\nwoman\n\n\n56\n5.058090\ncontrol\nman\n\n\n57\n2.959927\ncontrol\nman\n\n\n58\n4.873361\ncontrol\nman\n\n\n59\n3.594148\ncontrol\nman\n\n\n60\n4.714534\ntraining\nman\n\n\n61\n3.264649\ncontrol\nman\n\n\n62\n4.588575\ntraining\nwoman\n\n\n63\n5.089966\ncontrol\nwoman\n\n\n64\n4.163905\ncontrol\nman\n\n\n65\n7.127579\ntraining\nwoman\n\n\n66\n6.005201\ntraining\nman\n\n\n67\n4.437437\ncontrol\nman\n\n\n68\n5.365165\ncontrol\nwoman\n\n\n69\n4.105399\ntraining\nman\n\n\n70\n5.635135\ncontrol\nwoman\n\n\n71\n5.022247\ntraining\nwoman\n\n\n72\n4.180026\ncontrol\nwoman\n\n\n73\n6.752384\ntraining\nman\n\n\n74\n4.181650\ntraining\nwoman\n\n\n75\n6.044261\ntraining\nman\n\n\n76\n4.213103\ncontrol\nman\n\n\n77\n6.115964\ntraining\nwoman\n\n\n78\n5.527988\ncontrol\nwoman\n\n\n79\n5.734455\ntraining\nman\n\n\n80\n5.387702\ntraining\nman\n\n\n81\n4.183703\ntraining\nwoman\n\n\n82\n5.163233\ntraining\nwoman\n\n\n83\n5.478410\ncontrol\nwoman\n\n\n84\n3.306088\ncontrol\nman\n\n\n85\n6.261406\ncontrol\nwoman\n\n\n86\n5.536015\ntraining\nwoman\n\n\n87\n6.881067\ncontrol\nwoman\n\n\n88\n4.633545\ntraining\nwoman\n\n\n89\n4.505152\ntraining\nwoman\n\n\n90\n3.482766\ncontrol\nwoman\n\n\n91\n5.459823\ntraining\nman\n\n\n92\n3.713462\ntraining\nman\n\n\n93\n6.456246\ntraining\nman\n\n\n94\n4.719504\ncontrol\nwoman\n\n\n95\n5.088813\ntraining\nwoman\n\n\n96\n3.936570\ncontrol\nman\n\n\n97\n3.045063\ntraining\nman\n\n\n98\n5.812886\ntraining\nman\n\n\n99\n3.608662\ncontrol\nman\n\n\n100\n4.625465\ncontrol\nman\n\n\n101\n3.716192\ntraining\nwoman\n\n\n102\n4.130171\ntraining\nman\n\n\n103\n4.793977\ncontrol\nman\n\n\n104\n5.767222\ntraining\nman\n\n\n105\n5.087236\ncontrol\nwoman\n\n\n106\n6.551865\ncontrol\nwoman\n\n\n107\n6.295529\ntraining\nman\n\n\n108\n5.841340\ntraining\nman\n\n\n109\n4.390190\ntraining\nman\n\n\n110\n3.357638\ncontrol\nman\n\n\n111\n4.211005\ncontrol\nman\n\n\n112\n6.286031\ncontrol\nwoman\n\n\n113\n3.568811\ncontrol\nwoman\n\n\n114\n5.112539\ntraining\nman\n\n\n115\n4.997397\ntraining\nwoman\n\n\n116\n4.467550\ncontrol\nman\n\n\n117\n4.906547\ncontrol\nman\n\n\n118\n3.718384\ncontrol\nwoman\n\n\n119\n3.944646\ncontrol\nwoman\n\n\n120\n3.917821\ncontrol\nman\n\n\n121\n5.728532\ncontrol\nwoman\n\n\n122\n5.281620\ntraining\nman\n\n\n123\n5.708913\ncontrol\nman\n\n\n124\n5.249298\ntraining\nwoman\n\n\n125\n4.454345\ncontrol\nwoman\n\n\n126\n6.730907\ntraining\nwoman\n\n\n127\n4.962171\ntraining\nwoman\n\n\n128\n6.095822\ntraining\nwoman\n\n\n129\n5.951963\ntraining\nman\n\n\n130\n3.735796\ncontrol\nman\n\n\n131\n2.772235\ncontrol\nwoman\n\n\n132\n5.499271\ntraining\nman\n\n\n133\n6.358859\ncontrol\nwoman\n\n\n134\n4.172441\ntraining\nman\n\n\n135\n5.749763\ntraining\nwoman\n\n\n136\n3.839864\ncontrol\nwoman\n\n\n137\n6.754586\ntraining\nwoman\n\n\n138\n5.222653\ntraining\nman\n\n\n139\n5.272819\ntraining\nwoman\n\n\n140\n3.945108\ncontrol\nman\n\n\n141\n4.287297\ncontrol\nman\n\n\n142\n4.365866\ncontrol\nman\n\n\n143\n5.101909\ntraining\nwoman\n\n\n144\n4.065691\ncontrol\nman\n\n\n145\n5.559181\ncontrol\nwoman\n\n\n146\n4.757044\ncontrol\nwoman\n\n\n147\n3.809915\ntraining\nman\n\n\n148\n3.104902\ncontrol\nman\n\n\n149\n6.030397\ncontrol\nwoman\n\n\n150\n3.626316\ncontrol\nman\n\n\n151\n5.397467\ncontrol\nwoman\n\n\n152\n3.667918\ncontrol\nman\n\n\n153\n6.056584\ntraining\nman\n\n\n154\n4.473312\ncontrol\nwoman\n\n\n155\n3.989827\ncontrol\nwoman\n\n\n156\n4.696250\ncontrol\nwoman\n\n\n157\n3.407245\ncontrol\nman\n\n\n158\n6.073001\ntraining\nman\n\n\n159\n5.562025\ncontrol\nwoman\n\n\n160\n4.917394\ntraining\nwoman\n\n\n161\n6.520379\ntraining\nwoman\n\n\n162\n4.077757\ncontrol\nman\n\n\n163\n5.460912\ntraining\nman\n\n\n164\n5.231007\ncontrol\nwoman\n\n\n165\n5.748438\ncontrol\nman\n\n\n166\n6.326105\ncontrol\nwoman\n\n\n167\n5.451902\ntraining\nman\n\n\n168\n4.902787\ntraining\nman\n\n\n169\n6.107737\ntraining\nman\n\n\n170\n5.220152\ncontrol\nman\n\n\n171\n5.292647\ntraining\nman\n\n\n172\n3.713711\ncontrol\nwoman\n\n\n173\n5.905406\ntraining\nman\n\n\n174\n6.075597\ntraining\nwoman\n\n\n175\n3.740196\ncontrol\nman\n\n\n176\n4.454490\ntraining\nwoman\n\n\n177\n4.733459\ncontrol\nman\n\n\n178\n4.269386\ncontrol\nwoman\n\n\n179\n5.404058\ntraining\nwoman\n\n\n180\n7.179318\ntraining\nman\n\n\n181\n4.113804\ncontrol\nman\n\n\n182\n6.464315\ntraining\nwoman\n\n\n183\n4.278438\ncontrol\nwoman\n\n\n184\n4.631153\ncontrol\nman\n\n\n185\n6.812150\ncontrol\nwoman\n\n\n186\n4.390155\ncontrol\nwoman\n\n\n187\n4.555936\ntraining\nwoman\n\n\n188\n6.047384\ntraining\nman\n\n\n189\n5.327824\ncontrol\nwoman\n\n\n190\n5.106458\ntraining\nwoman\n\n\n191\n4.484803\ntraining\nwoman\n\n\n192\n4.873373\ntraining\nman\n\n\n193\n6.282335\ntraining\nman\n\n\n194\n7.693778\ncontrol\nwoman\n\n\n195\n7.364219\ntraining\nman\n\n\n196\n5.578385\ncontrol\nman\n\n\n197\n4.827206\ncontrol\nwoman\n\n\n198\n3.023665\ncontrol\nwoman\n\n\n199\n4.158985\ncontrol\nman\n\n\n200\n4.455009\ntraining\nman"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#fitting-the-model-in-anova",
    "href": "posts/moderation2022/presentation.html#fitting-the-model-in-anova",
    "title": "Adventuresin Moderation",
    "section": "Fitting the Model in ANOVA",
    "text": "Fitting the Model in ANOVA\n\nWe can use the aov() function to fit simple ANOVAs like this\nThe formula will be DV ~ IV1 * IV2 for a two-way ANOVA\nWe will then use model_parameters() to get Type 3 results\nFinally, we will estimate_means() and estimate_contrasts()\n\n\n\nlibrary(easystats)\nmodel1 &lt;- aov(\n  formula = score ~ condition * gender,\n  data = data_2x2\n)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#anova-model-parameters",
    "href": "posts/moderation2022/presentation.html#anova-model-parameters",
    "title": "Adventuresin Moderation",
    "section": "ANOVA Model Parameters",
    "text": "ANOVA Model Parameters\n\nmp1 &lt;- model_parameters(\n  model = model1, \n  contrasts = c(\"contr.sum\", \"contr.poly\"), \n  type = 3\n)\nprint(mp1)\n\n\n\n\n\nModel Summary\n\n\nParameter\nSum_Squares\ndf\nMean_Square\nF\np\n\n\n\n\n(Intercept)\n1443.22\n1\n1443.22\n1612.45\n&lt; .001\n\n\ncondition\n4.16\n1\n4.16\n4.65\n0.032\n\n\ngender\n13.77\n1\n13.77\n15.38\n&lt; .001\n\n\ncondition:gender\n9.14\n1\n9.14\n10.22\n0.002\n\n\nResiduals\n175.43\n196\n0.90"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#estimating-marginal-means",
    "href": "posts/moderation2022/presentation.html#estimating-marginal-means",
    "title": "Adventuresin Moderation",
    "section": "Estimating Marginal Means",
    "text": "Estimating Marginal Means\n\nem1 &lt;- estimate_means(\n  model = model1, \n  at = c(\"condition\", \"gender\")\n)\nprint(em1)\n\n\n\n\n\nEstimated Marginal Means\n\n\ncondition\ngender\nMean\nSE\n95% CI\n\n\n\n\ncontrol\nwoman\n4.99\n0.12\n(4.74, 5.23)\n\n\ntraining\nwoman\n5.39\n0.14\n(5.12, 5.66)\n\n\ncontrol\nman\n4.25\n0.14\n(3.97, 4.53)\n\n\ntraining\nman\n5.51\n0.13\n(5.25, 5.77)\n\n\n\nMarginal means estimated at condition, gender"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#plotting-marginal-means",
    "href": "posts/moderation2022/presentation.html#plotting-marginal-means",
    "title": "Adventuresin Moderation",
    "section": "Plotting Marginal Means",
    "text": "Plotting Marginal Means\n\nplot(em1)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#estimating-and-testing-contrasts",
    "href": "posts/moderation2022/presentation.html#estimating-and-testing-contrasts",
    "title": "Adventuresin Moderation",
    "section": "Estimating and Testing Contrasts",
    "text": "Estimating and Testing Contrasts\n\nec1a &lt;- estimate_contrasts(\n  model = model1,\n  contrast = \"condition\",\n  at = \"gender\"\n)\nprint(ec1a)\n\n\n\n\n\nMarginal Contrasts Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nLevel1\nLevel2\ngender\nDifference\n95% CI\nSE\nt(196)\np\n\n\n\n\ncontrol\ntraining\nman\n-1.26\n(-1.64, -0.88)\n0.19\n-6.48\n&lt; .001\n\n\ncontrol\ntraining\nwoman\n-0.40\n(-0.77, -0.03)\n0.19\n-2.16\n0.032\n\n\n\nMarginal contrasts estimated at condition, p-value adjustment method: Holm (1979)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#estimating-and-testing-contrasts-1",
    "href": "posts/moderation2022/presentation.html#estimating-and-testing-contrasts-1",
    "title": "Adventuresin Moderation",
    "section": "Estimating and Testing Contrasts",
    "text": "Estimating and Testing Contrasts\n\nec1b &lt;- estimate_contrasts(\n  model = model1,\n  contrast = \"gender\",\n  at = \"condition\"\n)\nprint(ec1b)\n\n\n\n\n\nMarginal Contrasts Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nLevel1\nLevel2\ncondition\nDifference\n95% CI\nSE\nt(196)\np\n\n\n\n\nwoman\nman\ncontrol\n0.74\n( 0.37, 1.11)\n0.19\n3.92\n&lt; .001\n\n\nwoman\nman\ntraining\n-0.12\n(-0.50, 0.26)\n0.19\n-0.64\n0.526\n\n\n\nMarginal contrasts estimated at gender, p-value adjustment method: Holm (1979)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#hypotheses-and-data-1",
    "href": "posts/moderation2022/presentation.html#hypotheses-and-data-1",
    "title": "Adventuresin Moderation",
    "section": "Hypotheses and Data",
    "text": "Hypotheses and Data\n\nTreatment will decrease symptoms (vs. control and placebo)\nTreatment will be more effective for women than for men\n\n\n\n\n\n\n\n\npatient\nsymptoms\ngroup\ngender\n\n\n\n\n1\n3.683067\ntreatment\nman\n\n\n2\n3.831002\nplacebo\nman\n\n\n3\n3.751012\ntreatment\nman\n\n\n4\n3.350506\ntreatment\nman\n\n\n5\n3.999081\nplacebo\nman\n\n\n6\n4.040540\ntreatment\nman\n\n\n7\n3.789107\nplacebo\nwoman\n\n\n8\n1.631338\ntreatment\nwoman\n\n\n9\n5.865339\ncontrol\nman\n\n\n10\n3.959936\nplacebo\nwoman\n\n\n11\n4.208445\ntreatment\nman\n\n\n12\n5.238969\ncontrol\nman\n\n\n13\n3.956852\nplacebo\nwoman\n\n\n14\n4.101691\ntreatment\nman\n\n\n15\n4.584589\nplacebo\nwoman\n\n\n16\n4.654177\ncontrol\nwoman\n\n\n17\n5.096955\ncontrol\nwoman\n\n\n18\n3.757290\ntreatment\nman\n\n\n19\n5.972139\ncontrol\nwoman\n\n\n20\n3.650344\nplacebo\nman\n\n\n21\n3.960412\nplacebo\nman\n\n\n22\n2.371775\ntreatment\nwoman\n\n\n23\n4.471384\ncontrol\nwoman\n\n\n24\n4.223687\ntreatment\nman\n\n\n25\n3.720213\nplacebo\nman\n\n\n26\n2.636796\ntreatment\nwoman\n\n\n27\n2.561717\ntreatment\nwoman\n\n\n28\n3.483105\nplacebo\nwoman\n\n\n29\n4.853836\ncontrol\nman\n\n\n30\n2.742955\ntreatment\nwoman\n\n\n31\n2.998217\ntreatment\nwoman\n\n\n32\n3.874644\nplacebo\nwoman\n\n\n33\n2.410750\ntreatment\nwoman\n\n\n34\n4.280384\ncontrol\nwoman\n\n\n35\n3.993854\nplacebo\nwoman\n\n\n36\n4.176368\nplacebo\nman\n\n\n37\n5.342086\ncontrol\nman\n\n\n38\n3.502481\ntreatment\nman\n\n\n39\n2.609248\ntreatment\nwoman\n\n\n40\n4.101993\nplacebo\nwoman\n\n\n41\n3.875935\ntreatment\nman\n\n\n42\n5.168324\ncontrol\nman\n\n\n43\n3.295567\ntreatment\nman\n\n\n44\n3.545530\ntreatment\nman\n\n\n45\n5.174403\ncontrol\nman\n\n\n46\n3.158648\nplacebo\nwoman\n\n\n47\n2.672171\ntreatment\nman\n\n\n48\n5.058088\ncontrol\nman\n\n\n49\n3.227256\nplacebo\nman\n\n\n50\n5.128927\ncontrol\nwoman\n\n\n51\n3.824916\ncontrol\nwoman\n\n\n52\n3.916943\ntreatment\nman\n\n\n53\n3.208675\ntreatment\nman\n\n\n54\n3.885213\nplacebo\nman\n\n\n55\n4.717599\ncontrol\nman\n\n\n56\n2.829732\ntreatment\nman\n\n\n57\n2.984161\nplacebo\nwoman\n\n\n58\n3.834124\nplacebo\nwoman\n\n\n59\n3.786460\ntreatment\nman\n\n\n60\n1.984711\ntreatment\nwoman\n\n\n61\n3.521415\ntreatment\nman\n\n\n62\n5.283700\ncontrol\nman\n\n\n63\n3.957941\nplacebo\nwoman\n\n\n64\n2.003426\ntreatment\nwoman\n\n\n65\n3.302742\nplacebo\nman\n\n\n66\n4.519169\ncontrol\nwoman\n\n\n67\n3.315367\ntreatment\nman\n\n\n68\n3.321480\nplacebo\nman\n\n\n69\n3.092005\nplacebo\nwoman\n\n\n70\n4.486210\ncontrol\nwoman\n\n\n71\n3.889311\ncontrol\nwoman\n\n\n72\n3.239938\nplacebo\nwoman\n\n\n73\n2.494876\nplacebo\nwoman\n\n\n74\n4.865170\ncontrol\nman\n\n\n75\n3.099088\nplacebo\nman\n\n\n76\n2.703681\ntreatment\nwoman\n\n\n77\n3.294807\ntreatment\nman\n\n\n78\n3.167455\nplacebo\nwoman\n\n\n79\n4.626240\ncontrol\nman\n\n\n80\n3.111319\ntreatment\nwoman\n\n\n81\n5.244846\ncontrol\nwoman\n\n\n82\n2.789232\ntreatment\nwoman\n\n\n83\n4.391974\nplacebo\nman\n\n\n84\n2.671924\ntreatment\nwoman\n\n\n85\n4.258345\ncontrol\nman\n\n\n86\n3.574922\ntreatment\nman\n\n\n87\n4.862326\ncontrol\nman\n\n\n88\n3.094355\ntreatment\nwoman\n\n\n89\n3.424623\ntreatment\nman\n\n\n90\n3.275555\ntreatment\nwoman\n\n\n91\n2.438367\ntreatment\nwoman\n\n\n92\n3.503636\nplacebo\nman\n\n\n93\n4.049056\nplacebo\nman\n\n\n94\n4.420680\ncontrol\nwoman\n\n\n95\n3.436439\ntreatment\nman\n\n\n96\n3.354488\nplacebo\nwoman\n\n\n97\n3.983255\ncontrol\nman\n\n\n98\n5.182548\ncontrol\nman\n\n\n99\n2.613542\nplacebo\nwoman\n\n\n100\n4.055499\ncontrol\nwoman\n\n\n101\n2.437038\ntreatment\nwoman\n\n\n102\n4.679464\ncontrol\nman\n\n\n103\n4.759928\nplacebo\nman\n\n\n104\n3.029085\ntreatment\nwoman\n\n\n105\n5.313118\ncontrol\nwoman\n\n\n106\n5.176724\ncontrol\nwoman\n\n\n107\n4.085248\ncontrol\nman\n\n\n108\n5.140998\ncontrol\nman\n\n\n109\n1.968991\ntreatment\nwoman\n\n\n110\n5.186876\ncontrol\nman\n\n\n111\n2.284765\ntreatment\nwoman\n\n\n112\n3.658862\nplacebo\nman\n\n\n113\n4.350146\ntreatment\nman\n\n\n114\n4.300288\ntreatment\nman\n\n\n115\n4.686918\ncontrol\nman\n\n\n116\n3.766755\ncontrol\nman\n\n\n117\n2.883879\nplacebo\nwoman\n\n\n118\n3.178663\ntreatment\nman\n\n\n119\n2.487341\ntreatment\nwoman\n\n\n120\n3.659114\nplacebo\nman\n\n\n121\n4.557424\ncontrol\nwoman\n\n\n122\n4.907548\ncontrol\nman\n\n\n123\n4.655817\ncontrol\nwoman\n\n\n124\n3.494930\nplacebo\nwoman\n\n\n125\n2.574772\nplacebo\nman\n\n\n126\n5.417146\ncontrol\nman\n\n\n127\n3.325603\nplacebo\nwoman\n\n\n128\n2.860576\ntreatment\nwoman\n\n\n129\n3.338200\nplacebo\nman\n\n\n130\n3.473366\nplacebo\nwoman\n\n\n131\n1.553649\ntreatment\nwoman\n\n\n132\n3.361876\ntreatment\nwoman\n\n\n133\n5.294677\ncontrol\nman\n\n\n134\n3.866261\nplacebo\nwoman\n\n\n135\n3.340717\ntreatment\nwoman\n\n\n136\n5.568056\ncontrol\nwoman\n\n\n137\n4.694963\ncontrol\nman\n\n\n138\n4.268536\ncontrol\nman\n\n\n139\n2.279876\ntreatment\nwoman\n\n\n140\n4.679343\nplacebo\nman\n\n\n141\n3.470273\nplacebo\nman\n\n\n142\n4.831236\ncontrol\nwoman\n\n\n143\n3.635198\nplacebo\nwoman\n\n\n144\n3.779816\nplacebo\nman\n\n\n145\n4.048951\ntreatment\nman\n\n\n146\n3.685560\ntreatment\nman\n\n\n147\n5.342175\ncontrol\nman\n\n\n148\n4.800316\ncontrol\nman\n\n\n149\n4.426206\ncontrol\nwoman\n\n\n150\n3.059124\nplacebo\nman\n\n\n151\n4.892211\ncontrol\nman\n\n\n152\n2.261958\ntreatment\nwoman\n\n\n153\n1.477009\ntreatment\nwoman\n\n\n154\n5.552239\ncontrol\nwoman\n\n\n155\n4.072153\ntreatment\nman\n\n\n156\n3.828199\nplacebo\nwoman\n\n\n157\n2.466217\ntreatment\nwoman\n\n\n158\n2.902365\ntreatment\nwoman\n\n\n159\n4.066778\ntreatment\nman\n\n\n160\n3.591798\ntreatment\nman\n\n\n161\n4.479610\ncontrol\nwoman\n\n\n162\n3.746472\nplacebo\nman\n\n\n163\n3.409870\ntreatment\nman\n\n\n164\n3.564776\nplacebo\nman\n\n\n165\n2.127127\ntreatment\nwoman\n\n\n166\n4.304739\nplacebo\nwoman\n\n\n167\n2.373575\ntreatment\nwoman\n\n\n168\n2.880832\nplacebo\nwoman\n\n\n169\n5.201406\ncontrol\nman\n\n\n170\n3.455701\nplacebo\nwoman\n\n\n171\n2.911728\nplacebo\nwoman\n\n\n172\n3.925479\ntreatment\nwoman\n\n\n173\n3.338932\nplacebo\nwoman\n\n\n174\n3.667183\nplacebo\nwoman\n\n\n175\n1.314828\ntreatment\nwoman\n\n\n176\n2.455877\ntreatment\nwoman\n\n\n177\n2.362176\ntreatment\nwoman\n\n\n178\n5.052475\ncontrol\nman\n\n\n179\n5.529821\ncontrol\nwoman\n\n\n180\n3.213364\nplacebo\nman\n\n\n181\n4.841677\ncontrol\nman\n\n\n182\n3.651969\nplacebo\nwoman\n\n\n183\n2.557120\ntreatment\nwoman\n\n\n184\n4.788895\ncontrol\nwoman\n\n\n185\n4.023404\nplacebo\nwoman\n\n\n186\n4.282011\ncontrol\nwoman\n\n\n187\n2.442558\nplacebo\nwoman\n\n\n188\n3.762763\nplacebo\nwoman\n\n\n189\n3.966392\ntreatment\nman\n\n\n190\n1.683476\ntreatment\nwoman\n\n\n191\n5.951345\ncontrol\nman\n\n\n192\n2.663725\ntreatment\nwoman\n\n\n193\n3.530330\ntreatment\nman\n\n\n194\n4.012764\ntreatment\nman\n\n\n195\n2.412654\nplacebo\nman\n\n\n196\n1.329439\ntreatment\nwoman\n\n\n197\n4.522039\ncontrol\nwoman\n\n\n198\n2.634963\ntreatment\nwoman\n\n\n199\n5.169679\ncontrol\nwoman\n\n\n200\n3.799478\ntreatment\nman"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#fitting-the-model-in-anova-1",
    "href": "posts/moderation2022/presentation.html#fitting-the-model-in-anova-1",
    "title": "Adventuresin Moderation",
    "section": "Fitting the Model in ANOVA",
    "text": "Fitting the Model in ANOVA\n\n\nmodel2 &lt;- aov(\n  formula = symptoms ~ group * gender,\n  data = data_3x2\n)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#anova-model-parameters-1",
    "href": "posts/moderation2022/presentation.html#anova-model-parameters-1",
    "title": "Adventuresin Moderation",
    "section": "ANOVA Model Parameters",
    "text": "ANOVA Model Parameters\n\nmp2 &lt;- model_parameters(\n  model = model2, \n  contrasts = c(\"contr.sum\", \"contr.poly\"), \n  type = 3\n)\nprint(mp2)\n\n\n\n\n\nModel Summary\n\n\nParameter\nSum_Squares\ndf\nMean_Square\nF\np\n\n\n\n\n(Intercept)\n615.35\n1\n615.35\n2364.66\n&lt; .001\n\n\ngroup\n86.82\n2\n43.41\n166.81\n&lt; .001\n\n\ngender\n0.36\n1\n0.36\n1.39\n0.240\n\n\ngroup:gender\n13.09\n2\n6.54\n25.15\n&lt; .001\n\n\nResiduals\n50.48\n194\n0.26"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#estimating-marginal-means-1",
    "href": "posts/moderation2022/presentation.html#estimating-marginal-means-1",
    "title": "Adventuresin Moderation",
    "section": "Estimating Marginal Means",
    "text": "Estimating Marginal Means\n\nem2 &lt;- estimate_means(model2, at = c(\"group\", \"gender\"))\nprint(em2)\n\n\n\n\n\nEstimated Marginal Means\n\n\ngroup\ngender\nMean\nSE\n95% CI\n\n\n\n\ncontrol\nwoman\n4.77\n0.10\n(4.58, 4.97)\n\n\nplacebo\nwoman\n3.50\n0.09\n(3.33, 3.67)\n\n\ntreatment\nwoman\n2.48\n0.08\n(2.33, 2.64)\n\n\ncontrol\nman\n4.93\n0.09\n(4.75, 5.11)\n\n\nplacebo\nman\n3.63\n0.10\n(3.44, 3.82)\n\n\ntreatment\nman\n3.68\n0.09\n(3.51, 3.84)\n\n\n\nMarginal means estimated at group, gender"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#plotting-marginal-means-1",
    "href": "posts/moderation2022/presentation.html#plotting-marginal-means-1",
    "title": "Adventuresin Moderation",
    "section": "Plotting Marginal Means",
    "text": "Plotting Marginal Means\n\nplot(em2)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#estimating-and-testing-contrasts-2",
    "href": "posts/moderation2022/presentation.html#estimating-and-testing-contrasts-2",
    "title": "Adventuresin Moderation",
    "section": "Estimating and Testing Contrasts",
    "text": "Estimating and Testing Contrasts\n\nec2a &lt;- estimate_contrasts(model2, contrast = \"group\", at = \"gender\")\nprint(ec2a)\n\n\n\n\n\nMarginal Contrasts Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nLevel1\nLevel2\ngender\nDifference\n95% CI\nSE\nt(194)\np\n\n\n\n\ncontrol\nplacebo\nman\n1.30\n( 0.98, 1.62)\n0.13\n9.81\n&lt; .001\n\n\ncontrol\nplacebo\nwoman\n1.27\n( 0.96, 1.59)\n0.13\n9.73\n&lt; .001\n\n\ncontrol\ntreatment\nman\n1.25\n( 0.96, 1.55)\n0.12\n10.20\n&lt; .001\n\n\ncontrol\ntreatment\nwoman\n2.29\n( 1.99, 2.60)\n0.13\n18.23\n&lt; .001\n\n\nplacebo\ntreatment\nman\n-0.04\n(-0.36, 0.27)\n0.13\n-0.35\n0.730\n\n\nplacebo\ntreatment\nwoman\n1.02\n( 0.74, 1.30)\n0.12\n8.76\n&lt; .001\n\n\n\nMarginal contrasts estimated at group, p-value adjustment method: Holm (1979)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#estimating-and-testing-contrasts-3",
    "href": "posts/moderation2022/presentation.html#estimating-and-testing-contrasts-3",
    "title": "Adventuresin Moderation",
    "section": "Estimating and Testing Contrasts",
    "text": "Estimating and Testing Contrasts\n\nec2b &lt;- estimate_contrasts(model2, contrast = \"gender\", at = \"group\")\nprint(ec2b)\n\n\n\n\n\nMarginal Contrasts Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nLevel1\nLevel2\ngroup\nDifference\n95% CI\nSE\nt(194)\np\n\n\n\n\nwoman\nman\ncontrol\n-0.16\n(-0.42, 0.11)\n0.13\n-1.18\n0.240\n\n\nwoman\nman\nplacebo\n-0.13\n(-0.39, 0.13)\n0.13\n-0.98\n0.327\n\n\nwoman\nman\ntreatment\n-1.20\n(-1.42, -0.97)\n0.12\n-10.32\n&lt; .001\n\n\n\nMarginal contrasts estimated at gender, p-value adjustment method: Holm (1979)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#opportunities",
    "href": "posts/moderation2022/presentation.html#opportunities",
    "title": "Adventuresin Moderation",
    "section": "Opportunities",
    "text": "Opportunities\n\nRecreate the results from ANOVA\nIncorporate continuous predictors/IVs\nIncorporate any kind/number of “covariates”\nIncorporate nonlinearity (e.g., polynomials)\nExtend to GLM for non-normal outcomes/DVs\nExtend to MLM for clustered/nested data\nExtend to SEM for latent variables"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#fitting-the-model-in-regression",
    "href": "posts/moderation2022/presentation.html#fitting-the-model-in-regression",
    "title": "Adventuresin Moderation",
    "section": "Fitting the Model in Regression",
    "text": "Fitting the Model in Regression\nANOVA code (for reference)\n\nmodel1 &lt;- aov(\n  formula = score ~ condition * gender,\n  data = data_2x2\n)\n\n\nRegression code\n\nmodel1b &lt;- lm(\n  formula = score ~ condition * gender,\n  data = data_2x2\n)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#comparing-moderation-models",
    "href": "posts/moderation2022/presentation.html#comparing-moderation-models",
    "title": "Adventuresin Moderation",
    "section": "Comparing Moderation Models",
    "text": "Comparing Moderation Models\n\n\n\n\n\n\nParameter\nSum_Squares\ndf\nMean_Square\nF\np\n\n\n\n\n(Intercept)\n1443.22\n1\n1443.22\n1612.45\n&lt; .001\n\n\ncondition\n4.16\n1\n4.16\n4.65\n0.032\n\n\ngender\n13.77\n1\n13.77\n15.38\n&lt; .001\n\n\ncondition:gender\n9.14\n1\n9.14\n10.22\n0.002\n\n\nResiduals\n175.43\n196\n0.90\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(196)\np\n\n\n\n\n(Intercept)\n4.99\n0.12\n(4.74, 5.23)\n40.16\n&lt; .001\n\n\ncondition (training)\n0.40\n0.19\n(0.03, 0.77)\n2.16\n0.032\n\n\ngender (man)\n-0.74\n0.19\n(-1.11, -0.37)\n-3.92\n&lt; .001\n\n\ncondition (training) × gender (man)\n0.86\n0.27\n(0.33, 1.39)\n3.20\n0.002\n\n\n\n\n\n\n\nNote the same \\(p\\)-values and that each \\(F\\) is equal to the corresponding \\(t\\) squared."
  },
  {
    "objectID": "posts/moderation2022/presentation.html#extensions",
    "href": "posts/moderation2022/presentation.html#extensions",
    "title": "Adventuresin Moderation",
    "section": "Extensions",
    "text": "Extensions\nBy giving the lm() model to the same functions, we can…\n\nEstimate and plot (the same) marginal means\n\nem1b &lt;- estimate_means(model1b, at = c(\"condition\", \"gender\"))\nprint(em1b)\nplot(em1b)\n\n\n\nEstimate and test (the same) contrasts\n\nestimate_contrasts(model1b, contrast = \"condition\", at = \"gender\")\nestimate_contrasts(model1b, contrast = \"gender\", at = \"condition\")"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#hypotheses-and-data-2",
    "href": "posts/moderation2022/presentation.html#hypotheses-and-data-2",
    "title": "Adventuresin Moderation",
    "section": "Hypotheses and Data",
    "text": "Hypotheses and Data\n\nExercising for a longer duration will burn more calories\nLong swims will be more effective than long runs\n\n\n\n\n\n\n\n\nparticipant\ncalories\nduration\nexercise\n\n\n\n\n1\n486.9765\n2.0247982\nswim\n\n\n2\n472.8959\n1.3821725\nrun\n\n\n3\n381.9000\n1.3681240\nswim\n\n\n4\n384.2023\n1.4285675\nrun\n\n\n5\n459.0924\n2.3272059\nrun\n\n\n6\n539.3930\n1.6615903\nswim\n\n\n7\n401.4003\n1.4630448\nswim\n\n\n8\n397.0844\n1.5121709\nrun\n\n\n9\n246.7168\n1.4368627\nswim\n\n\n10\n272.5706\n1.0153436\nswim\n\n\n11\n364.9235\n1.7090055\nrun\n\n\n12\n298.7456\n1.1187464\nrun\n\n\n13\n517.4730\n1.7360870\nswim\n\n\n14\n238.2022\n1.3128362\nswim\n\n\n15\n273.5526\n1.0331437\nrun\n\n\n16\n696.8010\n2.4898915\nrun\n\n\n17\n396.4383\n1.2521552\nswim\n\n\n18\n711.3158\n2.8504048\nrun\n\n\n19\n421.4667\n1.2994872\nswim\n\n\n20\n520.9031\n1.7577685\nswim\n\n\n21\n399.1551\n1.3919107\nrun\n\n\n22\n490.5664\n2.0523853\nrun\n\n\n23\n461.3814\n1.8787314\nrun\n\n\n24\n418.8214\n1.2921095\nswim\n\n\n25\n386.2042\n1.4752701\nrun\n\n\n26\n553.7873\n2.1345285\nswim\n\n\n27\n459.1941\n1.7606419\nswim\n\n\n28\n359.6371\n1.8502758\nrun\n\n\n29\n591.2767\n2.5190743\nrun\n\n\n30\n648.0497\n2.5144626\nswim\n\n\n31\n311.0183\n1.0141624\nrun\n\n\n32\n590.6766\n1.3583350\nswim\n\n\n33\n554.7981\n2.0242906\nswim\n\n\n34\n753.9970\n2.6053155\nswim\n\n\n35\n410.9739\n1.7576647\nrun\n\n\n36\n405.6490\n1.6033452\nrun\n\n\n37\n504.7794\n2.0862072\nswim\n\n\n38\n703.8329\n2.5526066\nrun\n\n\n39\n520.8601\n2.3704378\nrun\n\n\n40\n496.4263\n1.8792767\nswim\n\n\n41\n592.9068\n2.3074787\nrun\n\n\n42\n688.9164\n2.6912732\nswim\n\n\n43\n459.9332\n1.7083987\nswim\n\n\n44\n604.1794\n1.9010350\nswim\n\n\n45\n442.8850\n1.6983853\nrun\n\n\n46\n407.9747\n1.4556758\nswim\n\n\n47\n480.6806\n2.0919576\nrun\n\n\n48\n745.8023\n2.6535681\nswim\n\n\n49\n489.1903\n1.9158584\nswim\n\n\n50\n658.7930\n2.1719204\nswim\n\n\n51\n525.0054\n2.1882373\nrun\n\n\n52\n553.2563\n1.9027302\nswim\n\n\n53\n225.7835\n0.9423320\nrun\n\n\n54\n373.1410\n1.5434663\nrun\n\n\n55\n649.7859\n2.1748546\nswim\n\n\n56\n465.0962\n2.2027886\nrun\n\n\n57\n524.8834\n2.2526631\nrun\n\n\n58\n298.0463\n1.1040664\nrun\n\n\n59\n503.7600\n1.6692555\nrun\n\n\n60\n535.0446\n1.8138636\nswim\n\n\n61\n453.8953\n2.0793305\nrun\n\n\n62\n448.7485\n1.6607938\nswim\n\n\n63\n522.9352\n2.0412445\nrun\n\n\n64\n611.5877\n2.6218701\nrun\n\n\n65\n693.7688\n2.4402478\nswim\n\n\n66\n494.5699\n2.2719794\nswim\n\n\n67\n436.5890\n1.6045474\nrun\n\n\n68\n506.5195\n2.2135723\nrun\n\n\n69\n657.3150\n2.4755572\nswim\n\n\n70\n566.0532\n2.4100585\nrun\n\n\n71\n292.0450\n1.0945270\nswim\n\n\n72\n474.1026\n1.8600432\nrun\n\n\n73\n617.6613\n2.3355341\nswim\n\n\n74\n816.7769\n2.7984863\nswim\n\n\n75\n462.7039\n1.6371341\nswim\n\n\n76\n551.6663\n2.4802876\nrun\n\n\n77\n713.1180\n2.4782198\nswim\n\n\n78\n609.7088\n2.6329238\nrun\n\n\n79\n476.8087\n1.8815219\nswim\n\n\n80\n370.2146\n1.2399430\nswim\n\n\n81\n668.3936\n2.1697767\nswim\n\n\n82\n686.7034\n2.6979870\nswim\n\n\n83\n429.6422\n1.4069689\nrun\n\n\n84\n442.3489\n1.7977642\nrun\n\n\n85\n425.3900\n1.8564469\nrun\n\n\n86\n672.3602\n2.4712929\nswim\n\n\n87\n372.3098\n1.4726307\nrun\n\n\n88\n456.4376\n1.6713612\nswim\n\n\n89\n563.7933\n1.7928556\nswim\n\n\n90\n496.6736\n2.2269833\nrun\n\n\n91\n509.8858\n1.7230230\nswim\n\n\n92\n601.9327\n2.1769343\nswim\n\n\n93\n485.9239\n2.0741429\nswim\n\n\n94\n510.3737\n2.1882560\nrun\n\n\n95\n750.1839\n2.7534936\nswim\n\n\n96\n367.3109\n1.5363525\nrun\n\n\n97\n497.9109\n1.7064294\nswim\n\n\n98\n840.7540\n3.2294629\nswim\n\n\n99\n616.4693\n2.3368709\nrun\n\n\n100\n466.5235\n2.1672122\nrun\n\n\n101\n813.8330\n2.8045210\nswim\n\n\n102\n341.9197\n1.4423004\nswim\n\n\n103\n538.2966\n2.0050650\nrun\n\n\n104\n533.4335\n1.4738210\nswim\n\n\n105\n566.0748\n2.2169616\nrun\n\n\n106\n590.7031\n2.3290448\nrun\n\n\n107\n338.1217\n1.2799633\nswim\n\n\n108\n610.8300\n2.2366806\nswim\n\n\n109\n438.1448\n1.5970739\nswim\n\n\n110\n387.2061\n1.6072670\nrun\n\n\n111\n310.1953\n1.4323247\nrun\n\n\n112\n376.5662\n1.5442874\nrun\n\n\n113\n479.5830\n2.0449831\nrun\n\n\n114\n480.1373\n1.8819526\nswim\n\n\n115\n724.9867\n2.8137896\nswim\n\n\n116\n604.7393\n2.2526003\nrun\n\n\n117\n542.7069\n2.0187187\nrun\n\n\n118\n504.2774\n2.1825826\nrun\n\n\n119\n348.8282\n1.3026996\nrun\n\n\n120\n519.7228\n2.3175673\nrun\n\n\n121\n462.2700\n1.7611237\nrun\n\n\n122\n408.7308\n1.5900129\nswim\n\n\n123\n582.0695\n2.6261921\nrun\n\n\n124\n411.2592\n1.3408250\nswim\n\n\n125\n567.4711\n2.2721306\nrun\n\n\n126\n495.5337\n1.9065515\nswim\n\n\n127\n685.5443\n2.3079818\nswim\n\n\n128\n581.1644\n2.2639938\nswim\n\n\n129\n512.6728\n2.1172276\nswim\n\n\n130\n533.8828\n1.9438510\nrun\n\n\n131\n334.7888\n1.3418515\nrun\n\n\n132\n539.4534\n1.8316162\nswim\n\n\n133\n505.0998\n2.2392051\nrun\n\n\n134\n388.8688\n1.4530442\nswim\n\n\n135\n649.0873\n2.6307028\nswim\n\n\n136\n513.1374\n2.0180076\nrun\n\n\n137\n834.2074\n2.9405336\nswim\n\n\n138\n437.2265\n1.5667725\nswim\n\n\n139\n503.4449\n1.5025761\nswim\n\n\n140\n345.7169\n1.2413832\nrun\n\n\n141\n539.9918\n1.9799114\nrun\n\n\n142\n352.5483\n1.1067311\nrun\n\n\n143\n724.8370\n2.4781231\nswim\n\n\n144\n521.8461\n1.8597521\nrun\n\n\n145\n412.4266\n1.7944063\nrun\n\n\n146\n433.7123\n1.7682848\nrun\n\n\n147\n255.0309\n0.7725315\nswim\n\n\n148\n503.2697\n2.1564432\nrun\n\n\n149\n369.8479\n1.6043309\nrun\n\n\n150\n575.9216\n2.1127325\nrun\n\n\n151\n330.4060\n1.1080961\nrun\n\n\n152\n345.6405\n1.3150856\nrun\n\n\n153\n592.7174\n2.1969883\nswim\n\n\n154\n489.6851\n2.1336110\nrun\n\n\n155\n451.7196\n2.0436179\nrun\n\n\n156\n622.8804\n2.7759325\nrun\n\n\n157\n529.4093\n2.3977643\nrun\n\n\n158\n578.7621\n2.1706701\nswim\n\n\n159\n377.5654\n1.4450951\nrun\n\n\n160\n403.5881\n1.4788189\nswim\n\n\n161\n566.3034\n1.9055026\nswim\n\n\n162\n571.0665\n2.6430154\nrun\n\n\n163\n426.0878\n1.2844053\nswim\n\n\n164\n400.3703\n1.8062697\nrun\n\n\n165\n473.1179\n1.7486985\nrun\n\n\n166\n568.8180\n2.0337752\nrun\n\n\n167\n610.9184\n2.2532732\nswim\n\n\n168\n391.3575\n1.3591922\nswim\n\n\n169\n456.8028\n1.4723230\nswim\n\n\n170\n472.4160\n1.7589106\nrun\n\n\n171\n677.9135\n2.3642659\nswim\n\n\n172\n456.1863\n1.8908098\nrun\n\n\n173\n720.5866\n2.6544566\nswim\n\n\n174\n483.1441\n1.8746492\nswim\n\n\n175\n499.1261\n1.7271725\nrun\n\n\n176\n669.3015\n2.6154537\nswim\n\n\n177\n425.2697\n1.7310856\nrun\n\n\n178\n587.5653\n2.2979109\nrun\n\n\n179\n527.2292\n2.2259815\nswim\n\n\n180\n502.5024\n1.6678979\nswim\n\n\n181\n266.1488\n0.8861173\nrun\n\n\n182\n584.7520\n1.9996355\nswim\n\n\n183\n570.9862\n2.6794293\nrun\n\n\n184\n363.5350\n1.3362204\nrun\n\n\n185\n528.4117\n2.1248815\nrun\n\n\n186\n270.5534\n1.4199320\nrun\n\n\n187\n607.8367\n2.6272930\nswim\n\n\n188\n502.5881\n1.8613267\nswim\n\n\n189\n490.5813\n1.8864095\nrun\n\n\n190\n436.3197\n1.7725541\nswim\n\n\n191\n575.4885\n1.9436487\nswim\n\n\n192\n534.6002\n1.9829328\nswim\n\n\n193\n499.4114\n1.8009545\nswim\n\n\n194\n469.5905\n1.8328453\nrun\n\n\n195\n621.7169\n2.2795903\nswim\n\n\n196\n487.5178\n1.8785222\nrun\n\n\n197\n310.7343\n1.1549574\nrun\n\n\n198\n340.1598\n1.3524512\nrun\n\n\n199\n604.2201\n2.5151987\nrun\n\n\n200\n485.3175\n1.6131578\nswim\n\n\n201\n517.7148\n2.1987332\nrun\n\n\n202\n345.6452\n1.6339589\nrun\n\n\n203\n432.5128\n2.2782919\nrun\n\n\n204\n471.3572\n1.7366560\nrun\n\n\n205\n469.3288\n1.4949134\nswim\n\n\n206\n500.2970\n1.8481248\nrun\n\n\n207\n473.2479\n1.5036224\nswim\n\n\n208\n574.3772\n2.2865004\nrun\n\n\n209\n549.5592\n2.2810127\nrun\n\n\n210\n457.4513\n1.7086970\nrun\n\n\n211\n619.9736\n2.5101894\nrun\n\n\n212\n402.9696\n1.8388787\nrun\n\n\n213\n523.0114\n1.9804562\nswim\n\n\n214\n534.0091\n2.1155032\nrun\n\n\n215\n806.5378\n2.6742191\nswim\n\n\n216\n611.5919\n2.6630524\nrun\n\n\n217\n634.6245\n1.9759511\nswim\n\n\n218\n454.8344\n1.7013932\nswim\n\n\n219\n481.7028\n2.3038684\nrun\n\n\n220\n629.6170\n2.4100758\nswim\n\n\n221\n558.3575\n1.8963233\nswim\n\n\n222\n327.4352\n1.3568557\nrun\n\n\n223\n577.7574\n2.2027033\nswim\n\n\n224\n652.6858\n2.2877986\nswim\n\n\n225\n399.3339\n1.6700979\nrun\n\n\n226\n498.4742\n1.4772452\nswim\n\n\n227\n573.9528\n2.1667294\nswim\n\n\n228\n391.2898\n1.6346929\nrun\n\n\n229\n447.3620\n1.9520290\nrun\n\n\n230\n641.0104\n2.8396592\nrun\n\n\n231\n508.5105\n1.8569021\nrun\n\n\n232\n539.4666\n2.4821575\nrun\n\n\n233\n430.4994\n1.6392192\nswim\n\n\n234\n595.6283\n2.1155764\nswim\n\n\n235\n580.6120\n2.9060750\nrun\n\n\n236\n478.5022\n1.6950776\nswim\n\n\n237\n381.8860\n1.5279678\nrun\n\n\n238\n569.9180\n2.2736918\nrun\n\n\n239\n567.5700\n2.1639122\nswim\n\n\n240\n344.5753\n1.8032291\nrun\n\n\n241\n395.8164\n1.4924014\nswim\n\n\n242\n425.6812\n1.6866865\nswim\n\n\n243\n784.5746\n2.3911675\nswim\n\n\n244\n620.0504\n3.3468893\nrun\n\n\n245\n692.6378\n2.9321096\nrun\n\n\n246\n617.8225\n2.5891923\nrun\n\n\n247\n518.3671\n1.9136030\nrun\n\n\n248\n250.5662\n1.0118327\nrun\n\n\n249\n469.9183\n1.8794926\nrun\n\n\n250\n382.4043\n1.4775046\nswim\n\n\n251\n532.7137\n2.4126705\nrun\n\n\n252\n216.6400\n0.9246425\nrun\n\n\n253\n468.9990\n2.1378735\nrun\n\n\n254\n530.0574\n2.5336028\nrun\n\n\n255\n602.5167\n2.8599362\nrun\n\n\n256\n581.5413\n2.1499824\nswim\n\n\n257\n617.4461\n2.5497054\nswim\n\n\n258\n646.6101\n2.2573090\nswim\n\n\n259\n626.1248\n2.2447256\nswim\n\n\n260\n455.8306\n1.4956228\nswim\n\n\n261\n665.0507\n2.2969297\nswim\n\n\n262\n404.4268\n1.8882466\nrun\n\n\n263\n352.5399\n1.3676008\nrun\n\n\n264\n552.6479\n1.9058879\nswim\n\n\n265\n566.5589\n2.1059959\nrun\n\n\n266\n404.2725\n1.5409786\nswim\n\n\n267\n420.8090\n1.5301990\nswim\n\n\n268\n412.8585\n1.8357175\nrun\n\n\n269\n750.7532\n2.6710935\nswim\n\n\n270\n521.4205\n1.9897437\nrun\n\n\n271\n550.4266\n1.8602216\nrun\n\n\n272\n453.5186\n1.6679728\nrun\n\n\n273\n860.0864\n3.0289471\nswim\n\n\n274\n469.8367\n1.4904957\nrun\n\n\n275\n657.8459\n2.5975101\nswim\n\n\n276\n452.2333\n2.1491049\nswim\n\n\n277\n480.4230\n2.0775148\nrun\n\n\n278\n557.5080\n2.1295221\nrun\n\n\n279\n632.9417\n2.2720281\nswim\n\n\n280\n468.6496\n1.4711412\nswim\n\n\n281\n484.7340\n2.4699356\nrun\n\n\n282\n443.4077\n1.8582948\nrun\n\n\n283\n482.2144\n1.5864967\nrun\n\n\n284\n741.5248\n2.9812446\nswim\n\n\n285\n348.4961\n1.5521005\nrun\n\n\n286\n747.9164\n3.2244027\nrun\n\n\n287\n395.2979\n1.7333078\nrun\n\n\n288\n532.4208\n2.0444902\nrun\n\n\n289\n413.3065\n1.2879297\nrun\n\n\n290\n483.3166\n1.8916819\nrun\n\n\n291\n445.5594\n1.8529907\nswim\n\n\n292\n507.4554\n1.7979370\nswim\n\n\n293\n409.4575\n1.7568796\nswim\n\n\n294\n301.9665\n1.2992459\nrun\n\n\n295\n523.3978\n2.1804282\nrun\n\n\n296\n715.6622\n2.9466068\nswim\n\n\n297\n448.0202\n1.9812995\nswim\n\n\n298\n589.6015\n1.5747256\nswim\n\n\n299\n544.2143\n2.2365115\nswim\n\n\n300\n536.1614\n2.0824728\nswim"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#fitting-the-model-in-regression-1",
    "href": "posts/moderation2022/presentation.html#fitting-the-model-in-regression-1",
    "title": "Adventuresin Moderation",
    "section": "Fitting the Model in Regression",
    "text": "Fitting the Model in Regression\n\n\nmodel3 &lt;- lm(\n  formula = calories ~ duration * exercise,\n  data = data_2xC\n)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#regression-model-parameters",
    "href": "posts/moderation2022/presentation.html#regression-model-parameters",
    "title": "Adventuresin Moderation",
    "section": "Regression Model Parameters",
    "text": "Regression Model Parameters\n\nmp3 &lt;- model_parameters(model = model3)\nprint(mp3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(296)\np\n\n\n\n\n(Intercept)\n101.44\n16.54\n(68.89, 133.99)\n6.13\n&lt; .001\n\n\nduration\n191.42\n8.28\n(175.12, 207.72)\n23.11\n&lt; .001\n\n\nexercise (swim)\n-38.85\n24.49\n(-87.05, 9.35)\n-1.59\n0.114\n\n\nduration × exercise (swim)\n52.51\n12.15\n(28.60, 76.42)\n4.32\n&lt; .001"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#plotting-the-relations",
    "href": "posts/moderation2022/presentation.html#plotting-the-relations",
    "title": "Adventuresin Moderation",
    "section": "Plotting the Relations",
    "text": "Plotting the Relations\n\nplot(estimate_relation(model3))"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#estimating-simple-slopes",
    "href": "posts/moderation2022/presentation.html#estimating-simple-slopes",
    "title": "Adventuresin Moderation",
    "section": "Estimating Simple Slopes",
    "text": "Estimating Simple Slopes\nThe slope of duration is different for running vs. swimming\nWe can use estimate_slopes() to get these “simple slopes”\n\n\nes3 &lt;- estimate_slopes(model3, trend = \"duration\", at = \"exercise\")\nprint(es3)\n\n\n\n\n\n\nEstimated Marginal Effects\n\n\nexercise\nCoefficient\nSE\n95% CI\nt(296)\np\n\n\n\n\nrun\n191.42\n8.28\n(175.12, 207.72)\n23.11\n&lt; .001\n\n\nswim\n243.93\n8.89\n(226.44, 261.42)\n27.45\n&lt; .001\n\n\n\nMarginal effects estimated for duration"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#plotting-simple-slopes",
    "href": "posts/moderation2022/presentation.html#plotting-simple-slopes",
    "title": "Adventuresin Moderation",
    "section": "Plotting Simple Slopes",
    "text": "Plotting Simple Slopes\n\nplot(es3)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#estimating-contrasts",
    "href": "posts/moderation2022/presentation.html#estimating-contrasts",
    "title": "Adventuresin Moderation",
    "section": "Estimating Contrasts",
    "text": "Estimating Contrasts\n\nec3 &lt;- estimate_contrasts(\n  model = model3, \n  contrast = \"exercise\", \n  at = \"duration = c(1, 2, 3)\"\n)\nprint(es3b)\n\n\n\n\n\nMarginal Contrasts Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nLevel1\nLevel2\nduration\nDifference\n95% CI\nSE\nt(296)\np\n\n\n\n\nrun\nswim\n1.00\n-13.66\n( -39.28, 11.96)\n13.02\n-1.05\n0.295\n\n\nrun\nswim\n2.00\n-66.16\n( -77.68, -54.65)\n5.85\n-11.31\n&lt; .001\n\n\nrun\nswim\n3.00\n-118.67\n(-146.10, -91.24)\n13.94\n-8.51\n&lt; .001\n\n\n\nMarginal contrasts estimated at exercise, p-value adjustment method: Holm (1979)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#hypotheses-and-data-3",
    "href": "posts/moderation2022/presentation.html#hypotheses-and-data-3",
    "title": "Adventuresin Moderation",
    "section": "Hypotheses and Data",
    "text": "Hypotheses and Data\n\nAttachment anxiety and avoidance will both predict reduced marital satisfaction\nThe lowest satisfaction will be from those high on both anxiety and avoidance\n\n\n\n\n\n\n\n\nparticipant\nsatisfaction\naa_anxiety\naa_avoidance\n\n\n\n\n1\n4.318792e-01\n0.9001420\n-1.297939662\n\n\n2\n7.464191e-01\n-1.1733458\n-1.682195939\n\n\n3\n3.861420e-01\n-0.8974854\n0.051376248\n\n\n4\n2.232373e-01\n-1.4445014\n-0.268264733\n\n\n5\n-7.684968e-01\n-0.3310136\n-0.022763908\n\n\n6\n1.000405e+00\n-2.9006290\n0.381256632\n\n\n7\n-3.436840e-01\n-1.0592557\n0.362953888\n\n\n8\n-4.922589e-01\n0.2779547\n0.437596629\n\n\n9\n-1.300136e-01\n0.7494859\n-0.730239562\n\n\n10\n-3.741905e-01\n0.2415825\n-0.855773523\n\n\n11\n1.709354e-01\n1.0061857\n0.216484323\n\n\n12\n-1.179075e+00\n-0.1851460\n1.140081555\n\n\n13\n1.641055e+00\n-0.9818267\n-1.668092627\n\n\n14\n-9.697921e-01\n0.0929079\n-0.305004314\n\n\n15\n8.311237e-01\n-0.0527844\n-0.465415702\n\n\n16\n1.466305e+00\n-0.0803279\n0.051780263\n\n\n17\n3.227590e-01\n-0.6541037\n0.245552680\n\n\n18\n5.613374e-01\n-0.9506835\n-1.518215283\n\n\n19\n2.867013e-01\n1.0195618\n-0.579941309\n\n\n20\n2.252759e-01\n0.8590464\n-0.109149305\n\n\n21\n4.219218e-01\n0.3644608\n0.827752583\n\n\n22\n-4.174537e-01\n0.3836510\n-0.041032775\n\n\n23\n-8.307720e-01\n1.1134057\n1.641707224\n\n\n24\n-1.465387e+00\n1.2115098\n0.235375304\n\n\n25\n1.610962e+00\n-0.3483255\n-0.614587576\n\n\n26\n-1.949855e-01\n-0.8595534\n0.839564514\n\n\n27\n-4.571056e-01\n0.6500272\n-0.237961474\n\n\n28\n3.654884e-01\n0.3280591\n0.691528406\n\n\n29\n-1.296878e+00\n-0.5179466\n0.245441084\n\n\n30\n8.860042e-01\n-0.2389821\n-0.683787989\n\n\n31\n5.425709e-01\n0.1177789\n-1.998068236\n\n\n32\n1.560483e-01\n0.8315185\n0.326002402\n\n\n33\n-4.192886e-01\n-1.5589189\n0.699155582\n\n\n34\n5.947842e-01\n-0.2205191\n-1.290554360\n\n\n35\n6.887242e-01\n-0.8171944\n-0.052312730\n\n\n36\n-2.339679e+00\n1.0766774\n-0.655731087\n\n\n37\n-2.912812e+00\n1.0796640\n1.578970839\n\n\n38\n-3.232953e-01\n0.1421281\n-0.184938564\n\n\n39\n4.294992e-01\n0.1569795\n-0.133040850\n\n\n40\n-1.011572e+00\n-0.1687203\n-0.464179272\n\n\n41\n9.539652e-01\n-0.2690377\n-0.184774263\n\n\n42\n-7.423944e-01\n0.8077684\n0.286315250\n\n\n43\n6.097210e-01\n-1.1247172\n-0.765354434\n\n\n44\n1.104435e+00\n-1.4307880\n-0.819602195\n\n\n45\n-5.290321e-02\n0.0603567\n0.558586687\n\n\n46\n9.429142e-01\n-0.7929825\n-0.499114896\n\n\n47\n9.850133e-02\n0.3402759\n-1.418395469\n\n\n48\n7.252682e-02\n-0.2594687\n-1.274938955\n\n\n49\n7.930816e-01\n-1.3048486\n0.489336786\n\n\n50\n4.904928e-01\n0.3681734\n-0.560410712\n\n\n51\n-1.562720e+00\n1.6931900\n1.013145153\n\n\n52\n-1.805093e+00\n0.9958370\n-0.289695709\n\n\n53\n-2.548434e+00\n0.1867521\n0.602950535\n\n\n54\n6.558343e-02\n1.2383374\n-0.009983538\n\n\n55\n8.203403e-01\n0.3093733\n-0.800916638\n\n\n56\n5.053028e-01\n0.6357179\n-0.026735356\n\n\n57\n1.005976e+00\n0.0231833\n-0.890694544\n\n\n58\n-4.588432e-01\n1.1778636\n0.984843824\n\n\n59\n1.994137e-01\n-0.4535466\n0.371075569\n\n\n60\n4.312962e-01\n0.4159275\n-0.365996037\n\n\n61\n1.150234e+00\n-1.3384424\n0.467640169\n\n\n62\n-3.208243e-01\n-1.2919747\n-0.757080168\n\n\n63\n-2.599497e-01\n-0.3090742\n-0.131494356\n\n\n64\n2.481826e-01\n0.1565121\n0.290605617\n\n\n65\n2.238847e+00\n-0.8339168\n0.957765007\n\n\n66\n-5.760315e-01\n-0.0245493\n1.236569926\n\n\n67\n2.454918e+00\n-1.1373516\n-0.446288457\n\n\n68\n-1.010066e+00\n1.0720542\n-0.136743305\n\n\n69\n-3.302639e+00\n2.3144986\n1.437525398\n\n\n70\n-8.252522e-01\n0.4229731\n0.934149403\n\n\n71\n7.765073e-01\n-0.1369390\n-0.223449357\n\n\n72\n-1.048895e+00\n1.3283965\n-0.679285974\n\n\n73\n-7.864643e-01\n0.4365357\n0.555531866\n\n\n74\n5.199958e-01\n0.0664285\n0.576018392\n\n\n75\n-1.099305e+00\n1.3046598\n-0.108575916\n\n\n76\n1.764975e+00\n-0.2092595\n-1.025494890\n\n\n77\n-1.079997e+00\n1.0182530\n0.704214302\n\n\n78\n-1.129907e+00\n1.3660350\n-0.151365815\n\n\n79\n-1.612603e+00\n1.4766958\n0.476644484\n\n\n80\n-1.520933e+00\n0.8873065\n1.900673007\n\n\n81\n1.573886e+00\n-1.0159089\n-0.621593661\n\n\n82\n-2.655638e+00\n1.8708691\n1.603215709\n\n\n83\n-1.166521e+00\n1.0761984\n-0.250230829\n\n\n84\n9.498327e-01\n-1.0744076\n-0.164695096\n\n\n85\n-7.852938e-01\n-2.1955760\n0.882885288\n\n\n86\n-1.718298e-01\n0.5345446\n-0.347303904\n\n\n87\n-7.909224e-01\n1.3437334\n-0.357056895\n\n\n88\n-6.439796e-01\n1.3850035\n1.037789348\n\n\n89\n-2.545346e+00\n2.7469268\n1.338605739\n\n\n90\n-1.991500e+00\n-0.0459447\n-0.362004510\n\n\n91\n-8.787975e-01\n0.7430853\n-0.645091863\n\n\n92\n-1.023981e+00\n0.2604247\n-0.464250573\n\n\n93\n2.369047e+00\n0.4282819\n0.901133644\n\n\n94\n-3.227322e+00\n-0.3682588\n2.374484976\n\n\n95\n-2.774786e+00\n2.8874233\n2.811740910\n\n\n96\n2.915870e-01\n-0.6070726\n0.884398389\n\n\n97\n1.946240e+00\n-1.8791987\n-0.836112268\n\n\n98\n-6.213365e-01\n0.7183566\n-1.543285198\n\n\n99\n-9.261351e-03\n0.2514256\n-0.110892289\n\n\n100\n-9.133249e-01\n0.4670230\n-0.774792787\n\n\n101\n-8.827214e-01\n-0.2178884\n0.701832979\n\n\n102\n-1.254565e-01\n-1.2176477\n-2.420900437\n\n\n103\n-5.933691e-01\n-0.7540785\n-0.004925148\n\n\n104\n-1.772551e+00\n0.1912302\n1.076928034\n\n\n105\n-2.521448e+00\n1.0472828\n1.997375738\n\n\n106\n-8.901792e-01\n1.0898284\n0.699715218\n\n\n107\n-2.005390e+00\n0.6954744\n1.293286295\n\n\n108\n9.362436e-01\n-0.5243688\n0.301020239\n\n\n109\n7.888413e-02\n0.2640159\n0.569440847\n\n\n110\n9.049984e-03\n1.2412004\n-0.454831592\n\n\n111\n3.197110e-01\n0.6432972\n0.806350259\n\n\n112\n-6.545846e-01\n-0.9967626\n-0.556522923\n\n\n113\n9.702854e-01\n-2.0022985\n-1.891753326\n\n\n114\n8.073802e-01\n-0.4758391\n-0.331146946\n\n\n115\n1.014645e+00\n0.0441800\n0.231454687\n\n\n116\n1.046358e-01\n-1.3811354\n-1.341179265\n\n\n117\n-3.001633e-01\n0.2475197\n-0.758524528\n\n\n118\n-9.326808e-01\n0.2241897\n-0.201952628\n\n\n119\n-1.125392e-01\n0.8614789\n1.579273202\n\n\n120\n1.203898e+00\n-0.9267623\n-0.342855097\n\n\n121\n2.023831e+00\n-0.3204430\n-0.358132760\n\n\n122\n3.891326e-01\n0.6730248\n-0.346210044\n\n\n123\n1.849161e+00\n-1.2656298\n1.454273274\n\n\n124\n2.170798e+00\n-0.5185210\n-1.115407331\n\n\n125\n-1.787623e+00\n1.1224941\n1.539719888\n\n\n126\n-2.530990e+00\n-0.3159775\n0.177948534\n\n\n127\n4.381641e-01\n-1.7582992\n-0.488145883\n\n\n128\n-9.011571e-02\n1.2372534\n0.716410828\n\n\n129\n1.251959e+00\n-1.6639524\n-0.093391606\n\n\n130\n1.031326e-02\n1.8557345\n-0.272746671\n\n\n131\n-3.033423e+00\n1.0579974\n1.279845762\n\n\n132\n-3.140016e-01\n0.0372688\n-0.229345049\n\n\n133\n1.719618e+00\n0.0046050\n-0.744252340\n\n\n134\n1.709165e-01\n-1.9438343\n1.115091174\n\n\n135\n-5.479715e-02\n-1.5696234\n-1.390343900\n\n\n136\n-1.978789e-01\n-0.2732972\n2.183022356\n\n\n137\n-9.573274e-01\n0.4688538\n-0.300883914\n\n\n138\n-4.073017e-02\n1.0325388\n0.483359595\n\n\n139\n2.311951e+00\n-1.4183961\n-1.823101630\n\n\n140\n4.058282e-01\n-0.0878775\n-0.213883281\n\n\n141\n-2.353501e+00\n1.5786307\n0.331140885\n\n\n142\n4.203606e-02\n0.2446249\n-0.264285060\n\n\n143\n-2.496434e+00\n1.4970935\n0.123168525\n\n\n144\n-1.048106e+00\n0.9159064\n-0.938483069\n\n\n145\n-3.707612e-01\n0.3208412\n0.471516941\n\n\n146\n-2.027370e+00\n0.5627243\n1.978429766\n\n\n147\n-2.288330e+00\n0.5392953\n0.183959421\n\n\n148\n3.509421e+00\n-1.0734049\n-1.164888599\n\n\n149\n-2.352608e-01\n-1.8831606\n-0.240202977\n\n\n150\n-1.310855e+00\n0.9113902\n0.508802107\n\n\n151\n-2.716284e-01\n0.0495964\n-1.234071672\n\n\n152\n-2.418530e-01\n-1.2356549\n1.531346765\n\n\n153\n1.531998e+00\n-1.2637519\n-0.869780061\n\n\n154\n-1.597984e-01\n-1.1428650\n-0.246797445\n\n\n155\n8.184913e-01\n0.6544118\n-1.521287106\n\n\n156\n2.840406e+00\n-0.6768194\n1.012289942\n\n\n157\n1.280148e+00\n-1.0739105\n-0.842671475\n\n\n158\n1.640472e-01\n-0.9756581\n-0.255944736\n\n\n159\n3.165681e+00\n-1.1262746\n-3.612259878\n\n\n160\n2.024764e+00\n-1.9693127\n-1.652343428\n\n\n161\n1.274362e+00\n-0.5819890\n-1.433540904\n\n\n162\n2.063584e+00\n-1.7625071\n-0.910669190\n\n\n163\n5.573015e-01\n-0.5278260\n0.344920199\n\n\n164\n1.320199e+00\n-1.3743276\n-3.333828981\n\n\n165\n2.052050e+00\n-1.9337126\n-1.123353679\n\n\n166\n1.501339e-01\n0.9797831\n2.394735548\n\n\n167\n1.822736e+00\n-1.4956895\n-0.193197881\n\n\n168\n-4.935315e-01\n1.7008095\n1.596147953\n\n\n169\n1.928701e+00\n-1.4010257\n0.103453288\n\n\n170\n4.563414e-01\n-0.4844630\n0.332165945\n\n\n171\n1.067778e+00\n-1.2161786\n0.138330386\n\n\n172\n1.415128e+00\n0.1047705\n-0.125563012\n\n\n173\n-8.380081e-02\n-0.2425371\n-0.151470541\n\n\n174\n3.351227e-01\n-1.4157810\n0.080448750\n\n\n175\n1.439062e+00\n-1.0494598\n-0.347999752\n\n\n176\n1.384954e+00\n0.2690570\n-0.383781456\n\n\n177\n7.254943e-01\n-0.4787162\n-0.819540099\n\n\n178\n1.692627e+00\n-0.2994484\n-1.949493368\n\n\n179\n-9.799874e-01\n1.0381485\n0.356173908\n\n\n180\n-1.098926e+00\n1.0289251\n0.024600692\n\n\n181\n9.539940e-01\n-1.9716752\n-0.374061396\n\n\n182\n8.957322e-01\n-1.2833299\n3.026959068\n\n\n183\n-8.391831e-01\n0.0485813\n0.022400990\n\n\n184\n-2.305654e+00\n1.2106311\n1.665262762\n\n\n185\n9.679084e-01\n-0.4846706\n-0.725645097\n\n\n186\n1.266266e+00\n-0.7933096\n-0.367374359\n\n\n187\n1.045740e+00\n0.1724144\n-1.120570089\n\n\n188\n-1.527779e+00\n1.1052131\n2.339188841\n\n\n189\n-1.754647e-01\n0.7408755\n-0.506560608\n\n\n190\n1.205860e+00\n-0.2414466\n-0.547748306\n\n\n191\n6.814702e-01\n0.6149574\n1.012646933\n\n\n192\n-8.497423e-01\n1.3825465\n0.159145223\n\n\n193\n1.038677e-01\n-0.5832026\n-0.622234227\n\n\n194\n7.446903e-01\n-0.1979301\n1.369185940\n\n\n195\n-3.840437e-01\n-0.6032295\n0.040170418\n\n\n196\n1.754301e+00\n-1.0886484\n-0.695144655\n\n\n197\n-1.870959e-01\n0.1839153\n-0.425581604\n\n\n198\n-8.864783e-01\n1.3071363\n1.344056288\n\n\n199\n-1.065905e-02\n-0.1682833\n-0.810195915\n\n\n200\n-2.408353e-01\n0.3438407\n1.427373965\n\n\n201\n-4.686279e-02\n0.3764746\n0.109433820\n\n\n202\n-8.420080e-01\n-0.1945396\n0.421158522\n\n\n203\n2.524474e-02\n-2.1153360\n-1.738273728\n\n\n204\n5.904463e-01\n-0.9130674\n-0.791526227\n\n\n205\n-5.462648e-01\n0.3497093\n1.250430775\n\n\n206\n2.145896e-02\n0.4055771\n-1.041946599\n\n\n207\n-6.293104e-01\n0.5053261\n-0.083521278\n\n\n208\n-8.643903e-01\n-1.7918671\n-0.880153872\n\n\n209\n2.072623e-01\n-0.6614889\n1.252569584\n\n\n210\n1.115597e+00\n-0.3722728\n0.396607911\n\n\n211\n3.081642e-01\n0.1586610\n-0.883729633\n\n\n212\n1.387054e+00\n-0.6784124\n-0.661819273\n\n\n213\n-1.499474e+00\n0.0824890\n0.506231130\n\n\n214\n-7.283932e-01\n1.2437401\n0.427650660\n\n\n215\n6.187606e-01\n0.8804955\n1.131259604\n\n\n216\n1.760526e+00\n0.5439587\n-1.962428737\n\n\n217\n-3.031339e-01\n-0.7909051\n0.201452923\n\n\n218\n-5.059877e-01\n0.4271447\n-0.307543251\n\n\n219\n-1.897550e+00\n0.9511143\n0.332758351\n\n\n220\n-6.631145e-01\n0.8201171\n0.212226294\n\n\n221\n4.173146e-01\n-1.8109460\n-1.570398181\n\n\n222\n9.377251e-01\n-0.2799136\n0.139186237\n\n\n223\n-8.251228e-01\n0.6710683\n0.091349053\n\n\n224\n-2.082842e+00\n1.5969725\n2.147690913\n\n\n225\n4.424863e-05\n-0.7257318\n-0.320482948\n\n\n226\n2.557414e-01\n0.9605752\n-0.261804095\n\n\n227\n-1.730604e+00\n0.9564396\n1.355745109\n\n\n228\n-1.308812e+00\n1.2658477\n0.360172153\n\n\n229\n-6.199446e-01\n-0.2369562\n-0.918562026\n\n\n230\n7.559797e-01\n-1.5201141\n-0.635406198\n\n\n231\n-1.356204e+00\n0.3395534\n1.612518539\n\n\n232\n1.178817e+00\n1.3959739\n0.094606351\n\n\n233\n-2.228757e-01\n-1.1860622\n0.657765029\n\n\n234\n9.236243e-01\n-0.4044715\n-0.263906212\n\n\n235\n7.669923e-01\n-0.2871061\n-0.751342339\n\n\n236\n-9.580882e-01\n0.9425857\n0.626081962\n\n\n237\n2.386048e+00\n-1.0547386\n-0.597231828\n\n\n238\n1.806184e+00\n-0.6572775\n-0.556716700\n\n\n239\n-4.133801e-01\n-0.4142889\n1.002296691\n\n\n240\n1.485323e-01\n0.4539667\n-0.529385436\n\n\n241\n1.342898e+00\n-0.5539540\n0.250476450\n\n\n242\n-1.245033e+00\n0.3538685\n0.357846053\n\n\n243\n-5.440004e-01\n0.1482858\n-1.426988693\n\n\n244\n6.432294e-01\n0.3765119\n-0.161305591\n\n\n245\n-1.715760e+00\n1.5069871\n1.073880436\n\n\n246\n1.715339e+00\n-0.9272950\n-0.878325479\n\n\n247\n-3.540130e-01\n-0.5871412\n0.087268970\n\n\n248\n-1.955983e+00\n2.4589257\n1.076394683\n\n\n249\n-2.174571e+00\n0.6737419\n1.361548494\n\n\n250\n1.616574e+00\n0.3344244\n-0.910236975\n\n\n251\n-1.996195e+00\n1.6090419\n2.072018297\n\n\n252\n-1.272891e-01\n-1.1153993\n-1.870136593\n\n\n253\n-9.983739e-01\n0.0101301\n0.897503379\n\n\n254\n5.857948e-01\n-1.0523579\n1.562042433\n\n\n255\n-8.429583e-01\n0.4339232\n0.784188541\n\n\n256\n1.513390e+00\n0.6580896\n0.908044879\n\n\n257\n1.820886e+00\n-1.4400733\n-1.369788958\n\n\n258\n-6.707887e-01\n0.4733612\n0.312437860\n\n\n259\n8.764716e-02\n-0.8058521\n-0.634012494\n\n\n260\n2.168514e+00\n-0.7854659\n-0.720172779\n\n\n261\n9.082819e-01\n-1.1353506\n-1.627034548\n\n\n262\n7.440821e-01\n-0.9114252\n-0.730590892\n\n\n263\n-1.744318e+00\n0.0899663\n-0.306843489\n\n\n264\n-7.041372e-01\n-0.2360947\n-0.858497938\n\n\n265\n-2.013845e+00\n1.6275791\n0.395797732\n\n\n266\n-2.387147e+00\n0.5052006\n1.393984220\n\n\n267\n8.645580e-02\n0.0374373\n0.938679861\n\n\n268\n3.739279e-01\n0.3651652\n-0.257291893\n\n\n269\n-3.584666e-01\n-1.3946008\n-0.528645733\n\n\n270\n1.222918e+00\n0.6351347\n-0.371106469\n\n\n271\n9.035632e-01\n-0.4777527\n0.213085530\n\n\n272\n3.550738e-01\n-0.8199743\n-1.153347150\n\n\n273\n-6.342262e-01\n1.2523842\n-0.131223092\n\n\n274\n8.573957e-01\n-1.3183499\n-0.230684098\n\n\n275\n-9.164994e-01\n0.5442612\n0.646720307\n\n\n276\n-5.874836e-01\n-0.1868970\n-0.660131655\n\n\n277\n-1.286417e+00\n0.6159636\n1.444018424\n\n\n278\n-1.332752e+00\n0.5279876\n-0.332375304\n\n\n279\n-3.327827e-01\n0.2344553\n-1.083580061\n\n\n280\n-6.173638e-01\n-0.1122981\n0.997041516\n\n\n281\n-3.529321e-02\n-1.3162969\n-0.904262164\n\n\n282\n6.669514e-01\n-0.3367675\n0.415770773\n\n\n283\n-4.088405e-01\n0.4784102\n-0.409679416\n\n\n284\n2.503699e-01\n-1.0939116\n-1.039349540\n\n\n285\n-2.128611e-01\n1.2614056\n-0.364784026\n\n\n286\n1.483441e-01\n0.0360152\n0.393754508\n\n\n287\n-3.066208e+00\n1.8810672\n1.970859825\n\n\n288\n-1.026338e+00\n-0.8664550\n-0.544492323\n\n\n289\n1.430970e+00\n-0.9948478\n0.906184403\n\n\n290\n1.856977e+00\n-1.5172336\n-0.404709651\n\n\n291\n-4.428215e-01\n-0.0401771\n1.003318640\n\n\n292\n1.900399e+00\n-1.7865378\n0.120237109\n\n\n293\n-2.392911e+00\n0.9562462\n1.572884059\n\n\n294\n1.478994e+00\n-0.2804958\n1.023295295\n\n\n295\n5.727454e-02\n-0.4111873\n-0.807525193\n\n\n296\n-2.058447e-01\n-0.4634304\n-0.336423680\n\n\n297\n9.112563e-01\n-2.4549370\n-1.123347370\n\n\n298\n-1.181803e+00\n0.3128863\n-0.198562219\n\n\n299\n4.375730e-01\n-0.7913382\n-1.032605668\n\n\n300\n2.357914e-01\n0.2254650\n1.274868080"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#fitting-the-model-in-regression-2",
    "href": "posts/moderation2022/presentation.html#fitting-the-model-in-regression-2",
    "title": "Adventuresin Moderation",
    "section": "Fitting the Model in Regression",
    "text": "Fitting the Model in Regression\n\nmodel4 &lt;- lm(\n  formula = satisfaction ~ aa_anxiety * aa_avoidance,\n  data = data_CxC\n)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#regression-model-parameters-1",
    "href": "posts/moderation2022/presentation.html#regression-model-parameters-1",
    "title": "Adventuresin Moderation",
    "section": "Regression Model Parameters",
    "text": "Regression Model Parameters\n\nmp4 &lt;- model_parameters(model = model4)\nprint(mp4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(296)\np\n\n\n\n\n(Intercept)\n-9.64e-03\n0.06\n(-0.12, 0.11)\n-0.17\n0.869\n\n\naa anxiety\n-0.58\n0.06\n(-0.70, -0.46)\n-9.61\n&lt; .001\n\n\naa avoidance\n-0.37\n0.06\n(-0.49, -0.25)\n-6.09\n&lt; .001\n\n\naa anxiety × aa avoidance\n-0.12\n0.05\n(-0.21, -0.02)\n-2.41\n0.017"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#plotting-the-relations-1",
    "href": "posts/moderation2022/presentation.html#plotting-the-relations-1",
    "title": "Adventuresin Moderation",
    "section": "Plotting the Relations",
    "text": "Plotting the Relations\n\nplot(estimate_relation(model4))"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#estimating-simple-slopes-1",
    "href": "posts/moderation2022/presentation.html#estimating-simple-slopes-1",
    "title": "Adventuresin Moderation",
    "section": "Estimating Simple Slopes",
    "text": "Estimating Simple Slopes\n\nestimate_slopes(\n  model = model4, \n  trend = \"aa_anxiety\",\n  at = \"aa_avoidance = c(-3, -1.5, 0, 1.5, 3)\",\n)\n\n\n\n\n\nEstimated Marginal Effects\n\n\naa_avoidance\nCoefficient\nSE\n95% CI\nt(296)\np\n\n\n\n\n-3.00\n-0.23\n0.16\n(-0.55, 0.09)\n-1.39\n0.165\n\n\n-1.50\n-0.40\n0.10\n(-0.60, -0.21)\n-4.03\n&lt; .001\n\n\n0.00\n-0.58\n0.06\n(-0.70, -0.46)\n-9.61\n&lt; .001\n\n\n1.50\n-0.75\n0.09\n(-0.93, -0.58)\n-8.55\n&lt; .001\n\n\n3.00\n-0.93\n0.15\n(-1.22, -0.63)\n-6.20\n&lt; .001\n\n\n\nMarginal effects estimated for aa_anxiety"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#plotting-simple-slopes-1",
    "href": "posts/moderation2022/presentation.html#plotting-simple-slopes-1",
    "title": "Adventuresin Moderation",
    "section": "Plotting Simple Slopes",
    "text": "Plotting Simple Slopes\n\nes4a &lt;- estimate_slopes(model4, trend = \"aa_anxiety\", \n                        at = \"aa_avoidance\", length = 1000)\nplot(es4a)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#estimating-simple-slopes-2",
    "href": "posts/moderation2022/presentation.html#estimating-simple-slopes-2",
    "title": "Adventuresin Moderation",
    "section": "Estimating Simple Slopes",
    "text": "Estimating Simple Slopes\n\nestimate_slopes(\n  model = model4, \n  trend = \"aa_avoidance\",\n  at = \"aa_anxiety = c(-3, -1.5, 0, 1.5, 3)\"\n)\n\n\n\n\n\nEstimated Marginal Effects\n\n\naa_anxiety\nCoefficient\nSE\n95% CI\nt(296)\np\n\n\n\n\n-3.00\n-0.02\n0.15\n(-0.32, 0.28)\n-0.11\n0.912\n\n\n-1.50\n-0.19\n0.09\n(-0.37, -0.01)\n-2.12\n0.035\n\n\n0.00\n-0.37\n0.06\n(-0.49, -0.25)\n-6.09\n&lt; .001\n\n\n1.50\n-0.54\n0.10\n(-0.73, -0.35)\n-5.52\n&lt; .001\n\n\n3.00\n-0.72\n0.16\n(-1.03, -0.40)\n-4.43\n&lt; .001\n\n\n\nMarginal effects estimated for aa_avoidance"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#plotting-simple-slopes-2",
    "href": "posts/moderation2022/presentation.html#plotting-simple-slopes-2",
    "title": "Adventuresin Moderation",
    "section": "Plotting Simple Slopes",
    "text": "Plotting Simple Slopes\n\nes4b &lt;- estimate_slopes(model4, trend = \"aa_avoidance\", \n                        at = \"aa_anxiety\", length = 1000)\nplot(es4b)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#challenges-and-opportunities",
    "href": "posts/moderation2022/presentation.html#challenges-and-opportunities",
    "title": "Adventuresin Moderation",
    "section": "Challenges and Opportunities",
    "text": "Challenges and Opportunities\n\nProducts are only one type of interaction (bilinear)\n\nOthers include nonlinear, threshold, etc.\n\n\n\n\nMeasurement error gets compounded in moderation\n\nTesting in the SEM framework may be necessary\nMultivariate outliers can have a large influence\n\n\n\n\n\nPower analysis is complicated for interactions\n\nInteractions tend to be very power hungry\nSimulation-based power analysis may be needed\n\n\n\n\n\nInterpretations may differ between scales in GLM\n\nCaution and justification are warranted"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#references",
    "href": "posts/moderation2022/presentation.html#references",
    "title": "Adventuresin Moderation",
    "section": "References",
    "text": "References\n\n\nBauer, D. J., & Curran, P. J. (2005). Probing interactions in fixed and multilevel regression: Inferential and graphical techniques. Multivariate Behavioral Research, 40(3), 373–400. https://doi.org/10/d5wzg5\nEsarey, J., & Sumner, J. L. (2018). Marginal effects in interaction models: Determining and controlling the false positive rate. Comparative Political Studies, 51(9), 1144–1176. https://doi.org/10/gdw8xw\nFinsaas, M. C., & Goldstein, B. L. (2021). Do simple slopes follow-up tests lead us astray? Advancements in the visualization and reporting of interactions. Psychological Methods, 26(1), 38–60. https://doi.org/10/ggsng9\nMcCabe, C. J., Kim, D. S., & King, K. M. (2018). Improving present practices in the visual display of interactions. Advances in Methods and Practices in Psychological Science, 1(2), 147–165. https://doi.org/10/gf5sqb\nMcClelland, G. H., & Judd, C. M. (1993). Statistical difficulties of detecting interactions and moderator effects. Psychological Bulletin, 114(2), 376–390. https://doi.org/10/cj3kvv\nMiller, J. W., Stromeyer, W. R., & Schwieterman, M. A. (2013). Extensions of the Johnson-Neyman technique to linear models with curvilinear effects: Derivations and analytical tools. Multivariate Behavioral Research, 48(2), 267–300. https://doi.org/10/ggwpvb\nRohrer, J. M., & Arslan, R. C. (2021). Precise answers to vague questions: Issues with interactions. Advances in Methods and Practices in Psychological Science, 4(2), 1–19. https://doi.org/10/gk9zkd"
  },
  {
    "objectID": "publications/articles/hansen2018.html",
    "href": "publications/articles/hansen2018.html",
    "title": "How Mobile Health Technology and Electronic Health Records Will Change Care of Patients with Parkinson’s Disease",
    "section": "",
    "text": "Hansen, C., Sanchez-Ferro, A., & Maetzler, W. (2018). How Mobile Health Technology and Electronic Health Records Will Change Care of Patients with Parkinson’s Disease. In P. Brundin, J. W. Langston, & B. R. Bloem (Eds.), Journal of Parkinson’s Disease (Vol. 8, Issue s1, pp. S41–S45). IOS Press. https://doi.org/10.3233/jpd-181498."
  },
  {
    "objectID": "publications/articles/hansen2018.html#citation-apa-7",
    "href": "publications/articles/hansen2018.html#citation-apa-7",
    "title": "How Mobile Health Technology and Electronic Health Records Will Change Care of Patients with Parkinson’s Disease",
    "section": "",
    "text": "Hansen, C., Sanchez-Ferro, A., & Maetzler, W. (2018). How Mobile Health Technology and Electronic Health Records Will Change Care of Patients with Parkinson’s Disease. In P. Brundin, J. W. Langston, & B. R. Bloem (Eds.), Journal of Parkinson’s Disease (Vol. 8, Issue s1, pp. S41–S45). IOS Press. https://doi.org/10.3233/jpd-181498."
  },
  {
    "objectID": "publications/articles/hansen2018.html#abstract",
    "href": "publications/articles/hansen2018.html#abstract",
    "title": "How Mobile Health Technology and Electronic Health Records Will Change Care of Patients with Parkinson’s Disease",
    "section": "Abstract",
    "text": "Abstract\nCare of patients with Parkinson’s disease (PD) will dramatically change in the upcoming years. The nationwide implementations of the patient-controlled electronic health record (EHR) and the technology-based home monitoring system will most probably be the cornerstones of this revolution. We speculate that, within the course of the next decade, EHRs will lead to a substantial empowerment of patients, and monitoring of motor and non-motor manifestations of PD will shift from the clinic to the home. As far as this can be foreseen, small, partly clothing-embedded and implanted sensor systems allowing passive (i.e., non-obtrusive) data collection will dominate the market. They will interoperate with the personal EHR and other potentially health-related electronic databases such as clinical warehouses and population health analytics platforms. Analysis software will be mainly built on artificial intelligence, and presentation of data will be intuitive. This scenario will eventually help both the patient and the medical professional by providing higher amounts of quality information about daily-relevant effects of disease and treatment, eventually allowing for a better and more personalized care."
  },
  {
    "objectID": "publications/articles/romijnders2021.html",
    "href": "publications/articles/romijnders2021.html",
    "title": "Validation of IMU-based gait event detection during curved walking and turning in older adults and Parkinson’s Disease patients",
    "section": "",
    "text": "Romijnders, R., Warmerdam, E., Hansen, C. et al. Validation of IMU-based gait event detection during curved walking and turning in older adults and Parkinson’s Disease patients. J NeuroEngineering Rehabil 18, 28 (2021). https://doi.org/10.1186/s12984-021-00828-0"
  },
  {
    "objectID": "publications/articles/romijnders2021.html#citation-apa-7",
    "href": "publications/articles/romijnders2021.html#citation-apa-7",
    "title": "Validation of IMU-based gait event detection during curved walking and turning in older adults and Parkinson’s Disease patients",
    "section": "",
    "text": "Romijnders, R., Warmerdam, E., Hansen, C. et al. Validation of IMU-based gait event detection during curved walking and turning in older adults and Parkinson’s Disease patients. J NeuroEngineering Rehabil 18, 28 (2021). https://doi.org/10.1186/s12984-021-00828-0"
  },
  {
    "objectID": "publications/articles/romijnders2021.html#abstract",
    "href": "publications/articles/romijnders2021.html#abstract",
    "title": "Validation of IMU-based gait event detection during curved walking and turning in older adults and Parkinson’s Disease patients",
    "section": "Abstract",
    "text": "Abstract\nIdentification of individual gait events is essential for clinical gait analysis, because it can be used for diagnostic purposes or tracking disease progression in neurological diseases such as Parkinson’s disease. Previous research has shown that gait events can be detected from a shank-mounted inertial measurement unit (IMU), however detection performance was often evaluated only from straight-line walking. For use in daily life, the detection performance needs to be evaluated in curved walking and turning as well as in single-task and dual-task conditions.Participants (older adults, people with Parkinson’s disease, or people who had suffered from a stroke) performed three different walking trials: (1) straight-line walking, (2) slalom walking, (3) Stroop-and-walk trial. An optical motion capture system was used a reference system. Markers were attached to the heel and toe regions of the shoe, and participants wore IMUs on the lateral sides of both shanks. The angular velocity of the shank IMUs was used to detect instances of initial foot contact (IC) and final foot contact (FC), which were compared to reference values obtained from the marker trajectories.The detection method showed high recall, precision and F1 scores in different populations for both initial contacts and final contacts during straight-line walking. Shank-mounted IMUs can be used to detect gait events during straight-line walking, slalom walking and turning. However, more false events were observed during turning and more events were missed during turning. For use in daily life we recommend identifying turning before extracting temporal gait parameters from identified gait events."
  },
  {
    "objectID": "publications/articles/warmerdam2020.html",
    "href": "publications/articles/warmerdam2020.html",
    "title": "Long-term unsupervised mobility assessment in movement disorders",
    "section": "",
    "text": "Warmerdam, E., Hausdorff, J. M., Atrsaei, A., Zhou, Y., Mirelman, A., Aminian, K., Espay, A. J., Hansen, C., Evers, L. J. W., Keller, A., Lamoth, C., Pilotto, A., Rochester, L., Schmidt, G., Bloem, B. R., & Maetzler, W. (2020). Long-term unsupervised mobility assessment in movement disorders. In The Lancet Neurology (Vol. 19, Issue 5, pp. 462–470). Elsevier BV. https://doi.org/10.1016/s1474-4422(19)30397-7."
  },
  {
    "objectID": "publications/articles/warmerdam2020.html#citation-apa-7",
    "href": "publications/articles/warmerdam2020.html#citation-apa-7",
    "title": "Long-term unsupervised mobility assessment in movement disorders",
    "section": "",
    "text": "Warmerdam, E., Hausdorff, J. M., Atrsaei, A., Zhou, Y., Mirelman, A., Aminian, K., Espay, A. J., Hansen, C., Evers, L. J. W., Keller, A., Lamoth, C., Pilotto, A., Rochester, L., Schmidt, G., Bloem, B. R., & Maetzler, W. (2020). Long-term unsupervised mobility assessment in movement disorders. In The Lancet Neurology (Vol. 19, Issue 5, pp. 462–470). Elsevier BV. https://doi.org/10.1016/s1474-4422(19)30397-7."
  },
  {
    "objectID": "publications/articles/warmerdam2020.html#abstract",
    "href": "publications/articles/warmerdam2020.html#abstract",
    "title": "Long-term unsupervised mobility assessment in movement disorders",
    "section": "Abstract",
    "text": "Abstract\nMobile health technologies (wearable, portable, body-fixed sensors, or domestic-integrated devices) that quantify mobility in unsupervised, daily living environments are emerging as complementary clinical assessments. Data collected in these ecologically valid, patient-relevant settings can overcome limitations of conventional clinical assessments, as they capture fluctuating and rare events. These data could support clinical decision making and could also serve as outcomes in clinical trials. However, studies that directly compared assessments made in unsupervised and supervised (eg, in the laboratory or hospital) settings point to large disparities, even in the same parameters of mobility. These differences appear to be affected by psychological, physiological, cognitive, environmental, and technical factors, and by the types of mobilities and diagnoses assessed. To facilitate the successful adaptation of the unsupervised assessment of mobility into clinical practice and clinical trials, clinicians and researchers should consider these disparities and the multiple factors that contribute to them."
  }
]